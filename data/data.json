{
    "https://docs.truefoundry.com/docs/introduction": "About TrueFoundry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account About TrueFoundry All Pages Start typing to search\u2026 About TrueFoundry TrueFoundry is a platform on Kubernetes that makes it really easy to build, track, and deploy models without having a detailed understanding of Kubernetes. TrueFoundry is deployed on a cluster on your own cloud, so the data never leaves your environment, and you don't incur any data egress costs. Key design principles Cloud Native: TrueFoundry runs on Kubernetes, so it can work on almost all cloud providers or even on-premises. ML Inherits the same SRE principles as the rest of the infrastructure within your organization: TrueFoundry seamlessly integrates with the rest of your software stack to provide the same SRE, security, and cost optimization features to the ML team. No Vendor Lockin: We are developers, and we understand that nobody wants to be vendor-locked in to a specific provider. We have put in a lot of effort to make sure that it is really easy to migrate off of TrueFoundry if you need to do so. The TrueFoundry APIs do not interfere with your main code, and we also expose to you all the Kubernetes manifests we generate, so if you ever want to migrate, you can take those manifests and apply them yourself. Key features Jupyter Notebooks: Start Notebooks or VSCode Server on the cloud with auto shutdown. Deploy a training or batch inference job: Write your Python script, log your metrics, models, and artifacts, and trigger jobs either manually or on a schedule. Deploy your models as APIs: Deploy the model artifact directly to get APIs or wrap it in FastAPI, Flask, or another framework to host the APIs. The deployments support autoscaling and canary deployments out of the box. Easy Debugging: View logs, metrics, and cost optimization insights for all services. Model Registry: Track all the models and their versions in your organization along with their current deployment status and metadata. Deploy and Fine-tune LLMs: Deploy open source LLMs in one click and fine-tune them on your own data. Deploy common ML software: Deploy the most commonly used ML software like LabelStudio, Helm Charts, etc. Manage multiple environments and promotion: Manage multiple Kubernetes clusters from different environments and move workloads across them in a single click. Integration with other infrastructure TrueFoundry integrates with the rest of your infrastructure in the following way: TrueFoundry architecture TrueFoundry follows a split-plane architecture that enables on-premises deployment as well as makes sure that the reliability of your services does not depend on TrueFoundry. To read more about the architecture, you can refer to this blog: TrueFoundry: A ML Platform on Kubernetes: https://blog.truefoundry.com/truefoundry-ml-platform-on-kubernetes/ . You can connect your own cluster to the TrueFoundry hosted control plane. Our enterprise plan also allows you to host the control plane in your own account. Updated about 2 months ago",
    "https://docs.truefoundry.com/": "TrueFoundry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Home Blogs Create Account All Pages Start typing to search\u2026 TrueFoundry Docs Find comprehensive code and documentation to start deploying with TrueFoundry. For Data Scientists/ ML Engineers Experiment in your Editor of Choice Jupyter Notebook Remote SSH in VSCode Train & Deploy ML Models Training Job Model as API Batch Inference Deploy ML Applications Streamlit ML Pipelines Async Inference Deploy on Different Hardware GPU(Fractional) TPU For LLMOps AI Gateway Playground 20+ LLM Providers Fallback Finetune & Deploy LLMs, Vision Models Deploy LLM as API Finetune Job For Infra/DevOps Engineers Architecture Split-plane Architecture Deploy on your own Cloud AWS GCP Azure On Prem Role Based Access Control (RBAC) Key Entities User Management Stay in the loop Newsletter Subscribe to our weekly Newsletter to get latest updates in your inbox. Slack Meet other developers and connect with our team. TrueFoundry \u00a9 2024 All rights reserved",
    "https://docs.truefoundry.com/docs": "About TrueFoundry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account About TrueFoundry All Pages Start typing to search\u2026 About TrueFoundry TrueFoundry is a platform on Kubernetes that makes it really easy to build, track, and deploy models without having a detailed understanding of Kubernetes. TrueFoundry is deployed on a cluster on your own cloud, so the data never leaves your environment, and you don't incur any data egress costs. Key design principles Cloud Native: TrueFoundry runs on Kubernetes, so it can work on almost all cloud providers or even on-premises. ML Inherits the same SRE principles as the rest of the infrastructure within your organization: TrueFoundry seamlessly integrates with the rest of your software stack to provide the same SRE, security, and cost optimization features to the ML team. No Vendor Lockin: We are developers, and we understand that nobody wants to be vendor-locked in to a specific provider. We have put in a lot of effort to make sure that it is really easy to migrate off of TrueFoundry if you need to do so. The TrueFoundry APIs do not interfere with your main code, and we also expose to you all the Kubernetes manifests we generate, so if you ever want to migrate, you can take those manifests and apply them yourself. Key features Jupyter Notebooks: Start Notebooks or VSCode Server on the cloud with auto shutdown. Deploy a training or batch inference job: Write your Python script, log your metrics, models, and artifacts, and trigger jobs either manually or on a schedule. Deploy your models as APIs: Deploy the model artifact directly to get APIs or wrap it in FastAPI, Flask, or another framework to host the APIs. The deployments support autoscaling and canary deployments out of the box. Easy Debugging: View logs, metrics, and cost optimization insights for all services. Model Registry: Track all the models and their versions in your organization along with their current deployment status and metadata. Deploy and Fine-tune LLMs: Deploy open source LLMs in one click and fine-tune them on your own data. Deploy common ML software: Deploy the most commonly used ML software like LabelStudio, Helm Charts, etc. Manage multiple environments and promotion: Manage multiple Kubernetes clusters from different environments and move workloads across them in a single click. Integration with other infrastructure TrueFoundry integrates with the rest of your infrastructure in the following way: TrueFoundry architecture TrueFoundry follows a split-plane architecture that enables on-premises deployment as well as makes sure that the reliability of your services does not depend on TrueFoundry. To read more about the architecture, you can refer to this blog: TrueFoundry: A ML Platform on Kubernetes: https://blog.truefoundry.com/truefoundry-ml-platform-on-kubernetes/ . You can connect your own cluster to the TrueFoundry hosted control plane. Our enterprise plan also allows you to host the control plane in your own account. Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/create-and-setup-your-account": "Create and Setup your account Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Create and Setup your account All Pages Start typing to search\u2026 Create and Setup your account Register on TrueFoundry Step 1. Fill out the registration form To register for a TrueFoundry account please navigate to the \"Create your account\" page. In the \u201cCreate your account\u201d form, specify your company name, work email, username, and password, and click the \u201cCreate Account\u201d button. Please note: The value you enter in the company name field will be used to create a unique URL that you will use to access your TrueFoundry account. For example, if you enter \u201cchat-io\u201d then your access URL will be \u201cchat-io.truefoundry.cloud\u201d Step 2: Activate your account & login Once you fill out the form and submit it. You will receive an email to activate your account. Please open that email and click on the link. This is a mandatory step, only after activation you will be able to log in. After successful activation of your account, you will be redirected to the login page. Please enter your credentials and log in to the app. Create new or connect an existing Kubernetes cluster TrueFoundry offers support for the three major cloud providers: AWS EKS, GCP GKE, and Azure AKS, as well as Generic Kubernetes clusters. For the complete range of platform features, it is recommended to use one of the three major cloud providers. Please note that Generic clusters may not support all the platform features, such as service exposure, autoscaling, GPUs, and more. You can refer to these guides to connect your cluster based on the cloud you are using: Connect an AWS EKS cluster to TrueFoundry Connect an Azure AKS cluster to TrueFoundry Connect a GCP GKE cluster to TrueFoundry Additional setup (Optional) Environments: You can use the environments feature to tag your cluster with different environments like dev, staging, or prod. You can set up environments under the \u201cEnvironment tab in Settings\u201d. Once an environment is created you can click on edit cluster, select the environments, and hit the save button. Setup Environments Docker Registry Integration : TrueFoundry needs to access your Docker registry to save images built for deploying code or repositories and to deploy images from the registry to one of your connected workspaces. Integrate Docker Registry Git Integration: TrueFoundry needs to be able to access your Git repository to deploy source code from private git repositories to TrueFoundry. Integrate Git Blob Storage Integration : TrueFoundry needs to be able to access your blob storage to store the generated artifacts, logs, and metrics. Integrate Blob Storage Install applications: You can install additional applications on a cluster to enable new functionalities like Notebooks, Volumes, Autoscaling, Metrics, Logs, etc. On the cluster card, click on \"Manage\" next to \"Installed Applications\" to install new applications on your cluster. Add collaborators: You can invite users and create teams on TrueFoundry. These users and teams can be assigned different roles to access and perform different operations on the cluster. On the cluster card, click on the menu and then click on \"Manage collaborators\" to add/remove access of users and teams from your cluster. Secret Store Integration : TrueFoundry needs to be able to access your secret store to Store secrets securely. Integrate Secret Store Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/key-concepts": "TrueFoundry Key Concepts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account TrueFoundry Key Concepts All Pages Start typing to search\u2026 TrueFoundry Key Concepts Key Concepts and Information Architecture to get started Before we get started with deploying on TrueFoundry, it's important to understand a few key concepts. These are primarily Cluster, Workspace, and ML Repo. Cluster A cluster (Kubernetes Cluster) is a group of machines that can autoscale up and down. One cluster belongs to only one region, however, there can be multiple clusters in a region. A cluster represents a physical separation between resources. So we can have 2 clusters in Europe West, 1 cluster in us-east and 1 cluster in Asia. Workspace A cluster can have multiple workspaces. Each workspace is a logical separation within a cluster so that different teams, applications, and environments can sit within a cluster. Each workspace is essentially a Kubernetes namespace. For e.g, let's say there are three teams within a company. Team1 manages application1, application2, team2 manages application3, and team3 manages application4 and application5. Each of the applications further has three environments - dev, staging, and production. One way to organize the workspaces can be: We can also have the dev and prod workspaces in one cluster if you want to structure them like that. Creating a Workspace You can create a workspace from the Workspace tab in the platform. Once you create, you can get the FQN of the workspace from the FQN button. Getting Workspace FQN In the Workspace section, locate your workspace and then click the FQN button on the right side to copy the FQN to your clipboard ML Repo ML Repos concepts in TrueFoundry are like Git repos for code versioning, except that ML Repos are meant for versioning of ML models, artifacts, and metadata. You can provide access to certain ML Repos to users, teams, or workspaces. Once a workspace has access to an ML repo, all applications inside the workspace can then read or write to the MLRepo depending on whether the workspace has viewer or editor access. This way, we don't need to inject any keys and this makes the entire system much more secure. In the case above, workspace1 has access to MLRepo1 and hence team1 can access the assets in MLrepo1. This way, you can also divide datasets and model access across teams and workspaces. Creating a ML Repo \ud83d\udcd8 Prerequiste - Blob Storage Integration Before you can create an ML Repo, you'd need to connect one or more Blob Storages (S3, GCS, Azure Blob, MinIO, etc) to store artifacts and models associated with a ML Repo. If this one time setup is already done, you can skip to next section You can refer to one of the following pages to connect your blob storage to TrueFoundry AWS S3 Google Cloud Storage Azure Blob Storage Any S3 API Compatible Storage You can then create an ML Repo from the ML Repo's tab in the Platform. Grant access of ML Repo to Workspace Providing access to a certain ML Repos to a workspace ensures that every application in the Workspace gains access to that ML Repo. You can Grant access to an ML Repo to a Workspace while creating or editing a Workspace. Environment An environment is a tag applied to workspaces to categorize them based on factors like development stage, team ownership, etc. For example: dev, staging, production, frontend, backend, etc. We create three environments initially dev, staging and prod - which you can change at any point of time. Its essential to assign an environment to the cluster and workspace. A cluster can be associated to multiple environments but a workspace can only be associated to one environment. For each environment, you can also mark the following two metadata: IsProduction - This helps the TrueFoundry platform to understand its a production environment and it can later give you insights related to access control, availability and security. OptimizeFor (Cost / Availability) : This helps put more optimization in the cluster to either minimize cost or increase availability. Creating Environments To create an environment in TrueFoundry, follow these steps: How to tag a workspace with an environment To tag a workspace with an environment, first, the Cluster where the workspace resides needs to have those environments added. For this, you will have to add all environments relevant to your cluster (one cluster can have multiple environments) in the cluster, using the instructions provided below. Now all your applications deployed within that specific workspace will have the environment of the workspace show up beside them. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/create-your-first-deployment": "Create Your First Deployment Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Create Your First Deployment All Pages Start typing to search\u2026 Create Your First Deployment TrueFoundry helps you to seamlessly manage the entire machine learning lifecycle, from experimentation to deployment and beyond. You can: Kickstart your machine learning journey by launching a Jupyter Notebook to explore and experiment with your ideas. Launch a Notebook Launch SSH Server and Connect with VS Code Once your model is ready for training, execute a model training job from within the Notebook using the Python SDK. Or you can push your training code to a Github Repository and deploy directly from a public Github repository Deploy a Job from Github Repo Deploy a Job using Python SDK Seamlessly log your trained model to the TrueFoundry Model Registry, which is backed by a secure blob storage service like S3, GCS, or Azure Container. Log your trained model Deploy the logged model as a: Real-time API Service: Deploy your model as a real-time API Service to serve predictions in real-time, either from a public Github repository or from a local-machine / notebook Deploy a Service from Github Repo Deploy a Service using Python SDK Batch Inference Service: Deploy your model for batch inference to process large datasets efficiently by deploying it as a Job Async Service: Handle requests asynchronously using a queue to store intermediate requests by deploying an Async Service Deploy an Async Service LLM Testing and Deployment: Evaluate and compare the performance of various LLMs using TrueFoundry's LLM Gateway capabilities. Once you've selected the desired LLM, deploy it with ease using pre-configured settings Compare various LLM's using LLM Gateway Deploy a LLM LLM Finetuning: Leverage TrueFoundry's LLM finetuning capabilities to tailor LLMs to your specific needs and data. Finetune a LLM on your own data Updated 5 months ago",
    "https://docs.truefoundry.com/docs/setup": "Setup for CLI Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Setup for CLI All Pages Start typing to search\u2026 Setup for CLI TrueFoundry CLI Library TrueFoundry CLI library to help you interact with the platform programmatically by Interacting with the deployments side of TrueFoundry, enabling you to manage workspaces, deployments, applications, and view logs. Providing experiment tracking capabilities, allowing you to track ML experiments and interact with ML repositories within TrueFoundry. Setup for CLI Installation \ud83d\udcd8 Prerequisites Python >= 3.8 To install the Truefoundry CLI to enable both deployment and models functionality, run the following command: shell pip install -U \"truefoundry\" \ud83d\udcd8 Extras By default just installing truefoundry does not include support to work with Workflows . To include support for Workflows install the workflow extra like so Shell pip install -U \"truefoundry[workflow]\" Login Login by entering a Device Code You can log in to TrueFoundry using Truefoundry CLI. To log in to TrueFoundry , run the following command: Shell tfy login --host <your-truefoundry-host> # e.g. host: https://myorg.truefoundry.cloud This will prompt you to open a URL and enter the displayed code \ud83d\udcd8 Tip The value for --host is the first part of the url when you open the platform. This domain might be subdomain of your organization. The below example shows https://myorg.truefoundry.cloud Login using API Key (Non-Interactive Mode) For scenarios where code interacts with Truefoundry APIs or where opening a browser is not feasible, you can log in using the Truefoundry API Key. To begin, Generate a TrueFoundry API Key . Once you have the API key, you can authenticate in these ways: Option 1: Setting environment variables Set the TFY_API_KEY and TFY_HOST environment variables. Set TFY_HOST to your Truefoundry host URL. e.g. https://your-domain.truefoundry.com Set TFY_API_KEY to your API key. Option 2: Using the login command on CLI When using the CLI, you can provide the API key directly within the command: Shell tfy login --host <your-truefoundry-host> --api-key <your-api-key> Generate TrueFoundry API Key TrueFoundry API comes in handy when you want to interact with the Platform programmatically. You can generate the API key by following the instructions below: \ud83d\udcd8 For production environments, prefer Virtual Account API Keys \ud83d\udcd8 Note: Your API Key is a sensitive piece of information, so handle it with care. Never share it with unauthorized individuals. Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/generating-truefoundry-api-keys": "Generating TrueFoundry API Keys Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Generating TrueFoundry API Keys All Pages Start typing to search\u2026 Generating TrueFoundry API Keys Personal Access Tokens and Virtual Accounts TrueFoundry API Keys allow you to programmatically interact and perform actions on the TrueFoundry platform either using the CLI / Python SDK or HTTP API . Additionally they also allow you to access models via the AI Gateway . This API Key can be provided to CLI / Python SDK via the TFY_API_KEY environment variable There are two types of API Keys Personal Access Tokens Virtual Accounts Personal Access Tokens Personal Access Tokens have the same exact set of permissions that your current logged in user has. \ud83d\udcd8 For production settings, prefer Virtual Accounts Personal Access Tokens mirror the exact same set of permissions of your current user. Thus it might be too permissive in a production environment Virtual Accounts Virtual Accounts allow creating API Keys with limited permissions and expiration date, thus making them ideal for production settings or using them in C/I Actions \ud83d\udcd8 Only Tenant Admins can create Virtual Accounts Updated 5 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-a-service": "Introduction to a Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to a Service All Pages Start typing to search\u2026 Introduction to a Service A Truefoundry Service represents a continuously running application that typically provides a set of APIs for interaction. Services can be dynamically scaled based on incoming traffic or resource demands. Services are perfect for scenarios where real-time responses are essential, such as: Hosting Real-time Model Inference (e.g., Flask, FastAPI) Fueling Dynamic Website Backends Creating Model Demos (e.g., Streamlit, Gradio) Important Considerations for Service Deployment When deploying a service, you'll typically expose APIs on specific ports. These considerations are essential when deploying a service: Dockerize the code to be deployed Specify the ports for service exposure - Define Ports and Domains . [Optional] Map domains to the ports to enable communication with other applications - Define Ports and Domains . Define the resources requirements for your service - Define Resources [Optional] Define environment variables and secrets to inject into the code - Environment Variables and Secrets . [Optional] Set up an autoscaling policy for production workloads - Autoscaling . [Best Practice] Define Liveness/Readiness Probes - Liveness/Readiness Probe . [Best Practice] Establish a rollout strategy - Rollout Strategy . [Optional] Mount files or volumes to your service - Mounting Volumes and Files . Configure Monitoring Dashboards Set up Alerting Update, Rollback, and Promote your Service Set up CI/CD for your Service Access data from S3 or other clouds Deploy Your First Service To deploy your first service, choose one of the following guides based on the location of your service code: Deploy Service from a public Github repository . Deploy Service from your local machine . Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-service-from-a-public-github-repository": "Deploy Service from a public Github repository Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Service from a public Github repository All Pages Start typing to search\u2026 Deploy Service from a public Github repository In this guide, we'll deploy a FastAPI service for solving the Iris classification problem. This problem involves predicting the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already created a FastAPI service for the Iris classification problem, and you can find the code in our GitHub Repository . Please visit the repository to familiarize yourself with the code you'll be deploying. Project Structure The project files are organized as follows: Text . \u251c\u2500\u2500 app.py - Contains FastAPI code for inference. \u251c\u2500\u2500 iris_classifier.joblib - The model file. \u2514\u2500\u2500 requirements.txt - Lists dependencies. All these files are located in the same directory. Prerequisties Before you proceed with the guide, please ensure that you have setup a Workspace . To deploy your service, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Initiating Deployment via UI Use these configs for the deployment form: Repo URL : https://github.com/truefoundry/getting-started-examples Path to build context : ./deploy-ml-model/ Command : uvicorn app:app --host 0.0.0.0 --port 8000 Port : 8000 \ud83d\udcd8 What we did above: In the example above, we only had Python code and a requirements.txt. We didn't have a prewritten docker file - so we chose the Python Code option - to let Truefoundry templatize a Dockerfile from the details provided about the application and build the Docker image for us. We give these details in the Build context field, where we specify the directory in the GitHub repository where our service code resides ( ./deploy-ml-model/ ). We also specify the command that we need to use to run our service ( uvicorn app:app --host 0.0.0.0 --port 8000 ). Finally, we specify the port that we want our service to listen on ( 8000 ). View your deployed service Once you click Submit , your deployment will be successful in a few seconds, and your service will be displayed as active (green), indicating that it's up and running. Congratulations! You've successfully deployed your FastAPI service. To learn how to use your service-specific dashboard and send requests to your service, check out this guide: Interacting with your Service Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-service-using-python-sdk": "Deploy Service using Python SDK Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Service using Python SDK All Pages Start typing to search\u2026 Deploy Service using Python SDK In this guide, we'll deploy a FastAPI service for solving the Iris classification problem. This problem involves predicting the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already created a FastAPI service for the Iris classification problem, and you can find the code in our GitHub repository: GitHub Repository . Clone the GitHub repository with the following command: Shell git clone https://github.com/truefoundry/getting-started-examples.git Navigate to the project directory: Shell cd deploy-ml-model Please review the service code to become familiar with the code you'll deploy. Project Structure The project files are organized as follows: . \u251c\u2500\u2500 app.py - Contains FastAPI code for inference. \u251c\u2500\u2500 iris_classifier.joblib - The model file. \u2514\u2500\u2500 requirements.txt - Lists dependencies. All these files are located in the same directory. Prerequisties Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your service, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Also, copy the Workspace FQN as this will be required later. Deploying the Service Create a deploy.py file in the same directory as your Service code ( app.py ). This file will contain the necessary configuration for your Service. Your directory structure will then appear as follows: File Structure Text . \u251c\u2500\u2500 iris_classifier.joblib \u251c\u2500\u2500 app.py \u251c\u2500\u2500 deploy.py \u2514\u2500\u2500 requirements.txt deploy.py \ud83d\udcd8 Picking a value for host Providing a host value depends on the base domain urls configured in the cluster settings, you can learn how to find the base domain urls available to you here For e.g. If your base domain url is *.truefoundry.your-org.com then a valid value can be fastapi-your-workspace-8000.truefoundry.your-org.com . Alternatively if you have a non wildcard based domain url e.g. truefoundry.your-org.com , then a valid value can be truefoundry.your-org.com/fastapi-your-workspace-8000 deploy.py import logging from truefoundry.deploy import Build, PythonBuild, Service, Resources, Port # Set up logging to display informational messages logging.basicConfig(level=logging.INFO) # Create a TrueFoundry **Service** object to configure your service service = Service( # Specify the name of the service name=\"your-service\", # Define how to build your code into a Docker image image=Build( # `PythonBuild` helps specify the details of your Python Code. # These details will be used to templatize a DockerFile to build your Docker Image build_spec=PythonBuild( # Define the command to run the application command=\"uvicorn app:app --port 8000 --host 0.0.0.0\", # Specify the path to requirements file requirements_path=\"requirements.txt\", ) ), # Set the ports your server will listen on ports=[ Port( port=8000, # Define the host for the service, to learn how to set this view the above callout host=\"your-service-your-workspace-8000.your-organization.ai\", ) ], # Define the resource constraints. # # Requests are the minimum amount of resources that a container needs to run. # Limits are the maximum amount of resources that a container can use. # # If a container tries to use more resources than its limits, it will be throttled or killed. resources=Resources( # CPU is specified as a number. 1 CPU unit is equivalent to 1 physical CPU core, or 1 virtual core. cpu_request=0.2, cpu_limit=0.5, # Memory is defined as an integer and the unit is Megabytes. memory_request=200, memory_limit=500, # Ephemeral storage is defined as an integer and the unit is Megabytes. ephemeral_storage_request=1000, ephemeral_storage_limit=2000, ), # Define environment variables that your Service will have access to env={ \"UVICORN_WEB_CONCURRENCY\": \"1\", \"ENVIRONMENT\": \"dev\" } ) # Deploy the service to the specified workspace, copy workspace FQN using the following guide # https://docs.truefoundry.com/docs/key-concepts#creating-a-workspace service.deploy(workspace_fqn=\"your-workspace-fqn\") To understand the code, you can click the following recipe: To deploy using Python SDK use: Shell python deploy.py Run the above command from the same directory containing the app.py and requirements.txt files. \ud83d\udcd8 Exclude files when building and deploying your source code: To exclude specific files from being built and deployed, create a .tfyignore file in the directory containing your deployment script ( deploy.py ). The .tfyignore file follows the same rules as the .gitignore file. If your repository already has a .gitignore file, you don't need to create a .tfyignore file. Service Foundry will automatically detect the files to ignore. Place the .tfyignore file in the project's root directory, alongside deploy.py. After running the command mentioned above, wait for the deployment process to complete. Monitor the status until it shows DEPLOY_SUCCESS: , indicating a successful deployment. Once deployed, you'll receive a dashboard access link in the output, typically mentioned as You can find the application on the dashboard: . Click this link to access the deployment dashboard. View your deployed service Your deployment will be successful in a few seconds, and your service will be displayed as active (green), indicating that it's up and running. Congratulations! You've successfully deployed your FastAPI service. To learn how to use your service-specific dashboard and send requests to your service, check out this guide: Interacting with your Service Updated 4 months ago",
    "https://docs.truefoundry.com/docs/interacting-with-your-service": "Interacting with your Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Interacting with your Service All Pages Start typing to search\u2026 Interacting with your Service Service-Specific Dashboard To access the dedicated dashboard for your service, click its name. Pause / Resume your Service Pausing a service means that all replicas of your Service will be shut down, and your service will no longer consume any resources or serve requests - hence saving costs. This can be useful for dev environments when you don't want to run the service but also don't want to delete the configuration since it might be used later. Pause a Service \ud83d\udcd8 Note: If you have Autoscaling enabled for your service the option to pause a service will not be available to you. Resuming a Service Copy Endpoint URL To make a request to the Service, you will need the Endpoint URL. The endpoint URL is the same one you provided while deploying the service in the Ports Section. You can also copy it from the Service UI. The endpoint will be an internal cluster URL or an external URL based on whether you chose Expose in the Ports configuration. The two cases are as follows: The service port is exposed and mapped to a domain In this case, this becomes a public endpoint and you can call this service from anywhere - your own laptop or code running anywhere. You can add username-password-based authentication to the API in case you don't want everyone to be able to call this API. For this, you can refer to the following section Add Authentication to Endpoint . The service port is not exposed If you have not exposed the port, the endpoint is internal to the cluster and can only be called by other services running in the same cluster (including Jupyter Notebooks running in the same cluster). No one externally can access this service and this is the recommended mode for APIs that don't need to be exposed for external usage. These APIs will be of the format servicename-workspacename.svc.cluster.local:port Sending Requests to your Service Once you deploy the service, you will want to interact with the API in your code or manually using curl or Postman or Python code as shown below: import json from urllib.parse import urljoin import requests # Replace this with the value of your endpoint URL ENDPOINT_URL = \"\\<YOUR_ENDPOINT_URL>\" # e.g., https://your-service-endpoint.com response = requests.post( urljoin(ENDPOINT_URL, 'predict'), params={ \"sepal_length\": 7.0, \"sepal_width\": 3.2, \"petal_length\": 4.7, \"petal_width\": 1.4, } ) result = response.json() print(\"Predicted Classes:\", result[\"prediction\"]) Updated 1 day ago",
    "https://docs.truefoundry.com/docs/define-code": "Dockerize your code Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Dockerize your code All Pages Start typing to search\u2026 Dockerize your code Its a good practice to always dockerize your code while deploying so that it is guaranteed to run anywhere. Truefoundry helps you deploy your code on Kubernetes for which you will first need to create a docker image for your codebase. Truefoundry can help you deploy in all the three use cases: You already have a docker image built that you want to deploy - You can integrate the docker registry with Truefoundry and deploy the image. You don't have an image, but you have written a Dockerfile to build the code - Truefoundry can help build the image and deploy the image. You just have the code and the requirements.txt and don't have a Dockerfile - Truefoundry can automatically generate a Dockerfile and build and deploy the image. Regardless of any of the above scenarios, we will finally have a docker image which Truefoundry will then deploy on Kubernetes. \ud83d\udcd8 Private Docker Registry If you are using a Private Docker Registry, you will need to also integrate it with TrueFoundry. Check the Integrations page to connect your docker registry. Deploying your application when you have a Pre-built Image Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Image, Port service = Service( name=\"your-service\", + image = Image( + image_uri=\"your_image_uri\", # You can get this following this guide: https://docs.truefoundry.com/docs/integrations-docker-registry + docker_registry=\"your_docker_registry_fqn\", + command=\"command override\", + ) ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Deploying your application when you have written a Dockerfile and haven't built the image Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Build, DockerFileBuild, Port service = Service( name=\"my-service\", + image=Build( + build_spec=DockerFileBuild( + dockerfile_path=\"Dockerfile\" + build_context_path=\"./\", + build_args={ + \"param\": \"value\", + \"param1\": \"value1\", + }, + ), + ) ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") \ud83d\udcd8 Docker Build Args In DockerFileBuild , use build_args to set arguments for docker build --build-arg . It involves key-value pairs of string datatype. For instance: Python from truefoundry.deploy import DockerFileBuild build = DockerFileBuild( build_args={\"FOO\": \"Hello\", \"BAR\": \"World!\"} ) The ARG instruction in Dockerfiles defines variables set at build-time with --build-arg during docker build . Here's a concise Dockerfile example: Dockerfile FROM ubuntu ARG FOO ARG BAR ENV FOO_ENV=$FOO ENV BAR_ENV=$BAR RUN echo $FOO_ENV && echo $BAR_ENV Deploying your application when you haven't written a Dockerfile Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Build, PythonBuild, Port service = Service( name=\"your-service\", + image=Build( + build_spec=PythonBuild( + python_version=\"3.9\", + build_context_path=\"./\", + requirements_path=\"my-requirements.txt\", + pip_packages=[\"fastapi==0.82.0\", \"uvicorn\"], + command=\"uvicorn main:app --port 8000 --host 0.0.0.0\" + ), + ), ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") If you run the deploy.py , Truefoundry will start building the image on your local machine if docker is installed. Building locally can be quite fast specially if you are iterating rapidly. Truefoundry can also build the image remotely using a remote build server. It will fallback to the remote build server if you don't have docker installed locally. If you always want the build to fallback to the remote server, you can set local_build as False. as described below. This can be the preferred option if its taking a long time to upload the docker image to the registry from your local machine because of slow internet. Python from truefoundry.deploy import LocalSource, Build ... service = Service( ... image=Build( build_source=LocalSource( ... local_build=False ... ), ) ... ) ... Updated 1 day ago",
    "https://docs.truefoundry.com/docs/define-ports-and-domains": "Define Ports and Domains Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Define Ports and Domains All Pages Start typing to search\u2026 Define Ports and Domains When you create a Service, you need to specify the ports that the Service will listen on. You can decide to expose a port in which case you will have to attach a domain to the service. Not exposing the port If you do not expose a service, you get a URL of the form servicename-workspacename.svc.cluster.local:port which can only be called from services within the cluster. Exposing the port This will enable the service to be called from anywhere using the endpoint - other users or services living outside your cluster will also be able to access it. In this case, you will need to map a domain URL to your service. TrueFoundry supports both host-based routing and path-based routing. If you use host-based routing, two services s1 and s2 will have URLs of the form s1.yourdomain.com and s2.yourdomain.com . In the case of path-based routing, the URLs will be of the form yourdomain.com/s1 and yourdomain.com/s2 . If you decide to enable path-based routing, please ensure the application that you have deployed supports path-based routing. Domains are configured in the cluster. You can view which domains are accessible to you from here . Host-based routing will be preferred especially if you are deploying some UI-based applications. Configuring the Port for your Service Via the User Interface (UI) Via the Python SDK The Service class has a ports argument where you can pass a list of Port objects. Each Port object represents a port that the service will listen on. In the Port field you can specify the Port Number , host for your Endpoint and Path (Optional) \ud83d\udcd8 Picking a value for host Providing a host value depends on the base domain urls configured in the cluster settings, you can learn how to find the base domain urls available to you here For e.g. If your base domain url is *.truefoundry.your-org.com then a valid value can be fastapi-your-workspace-8000.truefoundry.your-org.com . Alternatively if you have a non wildcard based domain url e.g. truefoundry.your-org.com , then a valid value can be truefoundry.your-org.com/fastapi-your-workspace-8000 Diff import logging from truefoundry.deploy import Build, Service, DockerFileBuild, Port logging.basicConfig(level=logging.INFO) service = Service( name=\"your-service\", image=Build(build_spec=DockerFileBuild()), + ports=[ + Port( + port=8080, + # Uses subdomain based routing + host=\"fastapi-your-workspace-8080.your-organization.com\", + # Alternatively we can use path based routing - E.g. + # host=\"your-site.your-organization.com\", + path=\"/fastapi/v1/\" + ) + ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") What if you don't know which port to use? Usually, most of the commonly used applications will come with a way to define the port. We have listed it for some of the most commonly used frameworks in ML. FastAPI (Uvicorn) - Default port is 8000. This can be overridden using the command uvicorn main:app --port 8001 Gradio App - Launch your Gradio app as follows and enter the port number 8080. Python gradio.app.launch(server_name=\"0.0.0.0\", port=8080) Now you need to specify the same port number in the service configuration. In this case 8080 Streamlit - Default port is 8501. Adding Basic authentication to Service Ideally when deploying a Service it is a good idea to protect publicly exposed ports using some form of authentication. You can add a username password based authentication to your endpoints or JWT-based authentication to your services. Via the User Interface (UI) Via the Python SDK We can add basic (username and password) authentication to a Port of the Service in the Python definition using truefoundry.deploy.BasicAuthCreds ) deploy.py from truefoundry.deploy import Build, Service, Port, BasicAuthCreds, DockerFileBuild service = Service( name=\"your-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( port=8080, host=\"fastapi-my-workspace-8080.truefoundry.my-org.com\", + auth=BasicAuthCreds( + username=\"hello\", + password=\"pass\" + ) ) ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Sending request to your Authenticated Endpoint Upon deploying the service, the endpoint will now display a padlock \ud83d\udd12 icon, indicating that authentication is required for access. Browser Access When attempting to access the endpoint from a web browser, you will be prompted to enter your credentials. Python Access When using Python, you can pass the credentials in the following manner: import requests endpoint = 'https://fastapi-my-workspace-8080.truefoundry.my-org.com/' response = requests.get(endpoint, auth=('hello', 'pass')) Curl Access When using cURL, you'll need to include the credentials as part of the command: cURL curl -v -X GET --user hello:pass https://fastapi-my-workspace-8080.truefoundry.my-org.com/ Adding JWT Authentication to Your Service You can configure JWT-based authentication for your services hosted on TrueFoundry to ensure that only authenticated traffic can access your endpoints. The JWT token is sent as part of HTTP requests, and your service verifies it using a public key from the Identity Provider\u2019s (IDP) JSON Web Key Set (JWKS). This ensures the token is valid and untampered, allowing authentication based on its claims. Integrating Third-Party Identity Providers Identity providers like Amazon Cognito or Azure AD manage user authentication and issue JWTs. To validate these tokens, you need to configure a Custom JWT Auth integration that specifies: Issuer URL \u2013 The trusted identity provider. JWKS URI \u2013 The endpoint for fetching public keys used in token validation. Creating a Custom JWT Auth Integration Go to the Integrations Page and add a new Custom JWT Auth integration. Provide the following details: Issuer JWKS URI (Optional) If you want to enable OAuth2-based login for services using this integration: Select OAuth2 Client Configuration . Fill in the following fields from your OAuth2 provider: Client ID Client Secret Authorization URL Token URL Scopes (e.g., openid , email , etc.) Once configured, your service will authenticate incoming requests based on JWT claims, ensuring secure and verified access. Setting Up Authentication Providers To integrate JWT authentication, you need to configure an identity provider such as Amazon Cognito, Okta, Google OAuth2, or Microsoft Entra ID . Each provider has a slightly different setup, but the key steps remain the same: Create or use an existing application within the provider. Retrieve the Client ID, Client Secret , and OpenID configuration . Configure the Issuer, JWKS URI, Authorization URL, and Token URL in TrueFoundry. Set up scopes required for authentication. Below are detailed steps for specific providers. Amazon Cognito Setup Follow the AWS Cognito guide to create a Cognito application or use an existing one. Steps Create a Cognito User Pool (if not already available). Create an Application: Choose \"Traditional Web Application\" as the Application Type . Skip adding the return URL for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Open the OpenID Configuration: Example: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_GOoTGBS6e/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include appropriate scopes as required for your integration (e.g., openid , email , etc.). Retain JWT Source as Access Token (no changes needed). Example configuration Sample Custom JWT Auth integration for Amazon Cognito Google OAuth 2.0 Setup Follow the Google OAuth 2.0 guide to create a Google application or use an existing one. Steps Create a Google Cloud Project (if not already available) at Google Cloud Console . Enable the OAuth 2.0 API: Go to APIs & Services > Credentials . Click \"Create Credentials\" and select OAuth Client ID . Create an OAuth 2.0 Application: Choose \"Web Application\" as the Application Type . Skip adding the Authorized Redirect URI for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Open the OpenID Configuration: Example: https://accounts.google.com/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include the mandatory openid scope , along with other required scopes (e.g., email etc.). Set JWT Source to ID Token instead of Access Token. Example configuration Custom JWT Auth integration for Google OAuth2 Microsoft Entra ID (Azure AD) Setup Follow the Microsoft Entra ID guide to create an application or use an existing one. Steps to Configure the Application Save the following details: Client ID Client Secret Tenant ID Open the OpenID Configuration: OpenID Configuration URL: https://login.microsoftonline.com/{tenant_id}/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Redirect URI: Skip adding the Redirect URI initially . This can be added later once the TrueFoundry service deployment is created. Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include the mandatory openid scope , along with other required scopes (e.g., email , profile , etc.). Set JWT Source to ID Token . Example configuration Custom JWT Auth integration for Azure AD/Microsoft Entra ID Okta Setup Follow the Okta developer guide to create an Okta application or use an existing one. Steps Create an Okta Developer Account (if not already available) at https://developer.okta.com/signup/ . Create an Application: Choose \"Web Application\" as the Application Type . Skip adding the Sign-in redirect URI for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Okta Domain (e.g., https://dev-123456.okta.com ) Open the OpenID Configuration: Example: https://dev-123456.okta.com/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include appropriate scopes as required for your integration (e.g., offline_access , openid , email , etc.). Retain JWT Source as Access Token (no changes needed). Example configuration Custom JWT Auth integration for Okta Enabling JWT Authentication in Your Service Once you have set up the Custom JWT Auth integration, you can enable authentication in your service deployment. Steps: Enable Custom Security Rule: Navigate to the Security settings of your service. Select Custom Security Rule and choose the JWT authentication integration from the dropdown. Enable Login (if required): If your service requires OAuth2-based login (typically for frontend applications), enable the \"Enable Login\" option. This allows users to log in using the OAuth2 provider configured in the integration. Verify Token Claims (Optional): You can enforce additional security by verifying claims in the token. Define accepted values per key to ensure only valid tokens are allowed. Example: Restrict access to users with emails from truefoundry.com : \"email\": \"*@truefoundry.com\" Sample JWT Auth Config while deploying a service If OAuth2-based login is enabled, you must configure the Redirect URL in your OAuth2 provider settings. After deploying the service, the Redirect URL will be displayed on the deployment page. Copy this URL and add it to your OAuth2 provider\u2019s redirect URIs section. redirect URL for OAuth2 Sending request to your Authenticated Endpoint While sending request to the authenticated, you need to provide the token in the Authorization header. This header carries the token, allowing your service to verify and authenticate the request based on the credentials and claims present in the JWT. Here is an example of how to send a request using curl, a common command-line tool used for making HTTP requests: cURL curl -X GET \"https://your-service-endpoint.truefoundry.cloud/api/data\" \\ -H \"Authorization: Bearer YOUR_JWT_HERE\" Unauthenticated requests would return 403 . Specifying Host When we deploy services, we often want to put them behind a well-defined recognizable URL instead of load balancer URLs or IP addresses. E.g. https://app.your-organization.com or https://your-site.your-organization.com/app/v1/ . There are two steps involved in this: If your organization owns domains, integrate them before proceeding. For instructions, please refer to the following guide Construct endpoint URLs on top of your domains. Identifying Available Domains Before specifying the host for your service URL, determine which domains are available to you. This helps in choosing the appropriate endpoint structure. You will see that there are two possible domain types: Wildcard Domain: *.your-organization.com Standard Domain: your-organization.com The approach to constructing the URL will differ based on what type of domain we are using. Constructing the URL a) Wildcard Domain : For wildcard domains, specify the unique identifier before the .your-organization.com part. For example, your-site-name.your-organization.com . b) Standard Domain : For standard domains, specify the unique identifier after the your-organization.com part. For example, your-organization.com/your-site-name . Updated about 1 month ago",
    "https://docs.truefoundry.com/docs/add-jwt-authentication": "Add JWT Authentication Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Add JWT Authentication All Pages Start typing to search\u2026 Add JWT Authentication You can configure JWT-based authentication for your services hosted on TrueFoundry to ensure that only authenticated traffic can access your endpoints. The JWT token is sent as part of HTTP requests, and your service verifies it using a public key from the Identity Provider\u2019s (IDP) JSON Web Key Set (JWKS). This ensures the token is valid and untampered, allowing authentication based on its claims. Integrating Third-Party Identity Providers Identity providers like Amazon Cognito or Azure AD manage user authentication and issue JWTs. To validate these tokens, you need to configure a Custom JWT Auth integration that specifies: Issuer URL \u2013 The trusted identity provider. JWKS URI \u2013 The endpoint for fetching public keys used in token validation. Creating a Custom JWT Auth Integration Go to the Integrations Page and add a new Custom JWT Auth integration. Provide the following details: Issuer JWKS URI (Optional) If you want to enable OAuth2-based login for services using this integration: Select OAuth2 Client Configuration . Fill in the following fields from your OAuth2 provider: Client ID Client Secret Authorization URL Token URL Scopes (e.g., openid , email , etc.) Once configured, your service will authenticate incoming requests based on JWT claims, ensuring secure and verified access. Setting Up Authentication Providers To integrate JWT authentication, you need to configure an identity provider such as Amazon Cognito, Okta, Google OAuth2, or Microsoft Entra ID . Each provider has a slightly different setup, but the key steps remain the same: Create or use an existing application within the provider. Retrieve the Client ID, Client Secret , and OpenID configuration . Configure the Issuer, JWKS URI, Authorization URL, and Token URL in TrueFoundry. Set up scopes required for authentication. Below are detailed steps for specific providers. Amazon Cognito Setup Enabling JWT Authentication in Your Service Once you have set up the Custom JWT Auth integration, you can enable authentication in your service deployment. Steps: Enable Custom Security Rule: Navigate to the Security settings of your service. Select Custom Security Rule and choose the JWT authentication integration from the dropdown. Enable Login (if required): If your service requires OAuth2-based login (typically for frontend applications), enable the \"Enable Login\" option. This allows users to log in using the OAuth2 provider configured in the integration. Verify Token Claims (Optional): You can enforce additional security by verifying claims in the token. Define accepted values per key to ensure only valid tokens are allowed. Example: Restrict access to users with emails from truefoundry.com : \"email\": \"*@truefoundry.com\" Sample JWT Auth Config while deploying a service Updated 16 days ago",
    "https://docs.truefoundry.com/docs/amazon-cognito-setup": "Amazon Cognito Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Amazon Cognito All Pages Start typing to search\u2026 Amazon Cognito Follow the AWS Cognito guide to create a Cognito application or use an existing one. Steps Create a Cognito User Pool (if not already available). Create an Application: Choose \"Traditional Web Application\" as the Application Type . Skip adding the return URL for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Open the OpenID Configuration: Example: https://cognito-idp.us-east-1.amazonaws.com/us-east-1_GOoTGBS6e/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include appropriate scopes as required for your integration (e.g., openid , email , etc.). Retain JWT Source as Access Token (no changes needed). Example configuration Sample Custom JWT Auth integration for Amazon Cognito Updated 16 days ago",
    "https://docs.truefoundry.com/docs/google": "Google Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Google All Pages Start typing to search\u2026 Google Google OAuth 2.0 Setup Follow the Google OAuth 2.0 guide to create a Google application or use an existing one. Steps Create a Google Cloud Project (if not already available) at Google Cloud Console . Enable the OAuth 2.0 API: Go to APIs & Services > Credentials . Click \"Create Credentials\" and select OAuth Client ID . Create an OAuth 2.0 Application: Choose \"Web Application\" as the Application Type . Skip adding the Authorized Redirect URI for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Open the OpenID Configuration: Example: https://accounts.google.com/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include the mandatory openid scope , along with other required scopes (e.g., email etc.). Set JWT Source to ID Token instead of Access Token. Example configuration Custom JWT Auth integration for Google OAuth2 Updated 16 days ago",
    "https://docs.truefoundry.com/docs/microsoft-entra-id-azure-ad": "Microsoft Entra ID (Azure AD) Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Microsoft Entra ID (Azure AD) All Pages Start typing to search\u2026 Microsoft Entra ID (Azure AD) Microsoft Entra ID (Azure AD) Setup Follow the Microsoft Entra ID guide to create an application or use an existing one. Steps to Configure the Application Save the following details: Client ID Client Secret Tenant ID Open the OpenID Configuration: OpenID Configuration URL: https://login.microsoftonline.com/{tenant_id}/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Redirect URI: Skip adding the Redirect URI initially . This can be added later once the TrueFoundry service deployment is created. Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include the mandatory openid scope , along with other required scopes (e.g., email , profile , etc.). Set JWT Source to ID Token . Example configuration Custom JWT Auth integration for Azure AD/Microsoft Entra ID Updated 16 days ago",
    "https://docs.truefoundry.com/docs/okta": "Okta Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Okta All Pages Start typing to search\u2026 Okta Okta Setup Follow the Okta developer guide to create an Okta application or use an existing one. Steps Create an Okta Developer Account (if not already available) at https://developer.okta.com/signup/ . Create an Application: Choose \"Web Application\" as the Application Type . Skip adding the Sign-in redirect URI for now; it can be added later after deploying the TrueFoundry service. Save the following details: Client ID Client Secret Okta Domain (e.g., https://dev-123456.okta.com ) Open the OpenID Configuration: Example: https://dev-123456.okta.com/.well-known/openid-configuration This contains required fields like: Issuer JWKS URI Authorization URL Token URL Supported Scopes Integration with TrueFoundry Use the values from the OpenID configuration in the integration. For Client Secret , you can: Create a TrueFoundry Secret , or Directly add the value. Include appropriate scopes as required for your integration (e.g., offline_access , openid , email , etc.). Retain JWT Source as Access Token (no changes needed). Example configuration Custom JWT Auth integration for Okta Updated 16 days ago",
    "https://docs.truefoundry.com/docs/resources": "Define Resources Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Define Resources All Pages Start typing to search\u2026 Define Resources When we deploy a service or job or any other application, we will need to define the resources that the application will. This will ensure that Kubernetes can allot the correct set of resources and the application continues to run smoothly. There are a few key resources you will need to configure: Resource Unit Description CPU CPU cores Processing power required by the application. Defined in terms of CPU cores. 1 CPU unit is equivalent to 1 virtual core. It's possible to ask for fractional CPU resources like 0.1 or even 0.01. You will need to specify CPU requests and limits. The number you define in the CPU request will always be reserved for the application. Your application can occasionaly take more CPU than what is requestes till the CPU limit, but not beyond that. For e.g, if CPU request is 0.5 and limit is 1, it means that the application has 0.5 CPU reserved for itself. CPU usage can go upto 1 if there is CPU available on the machine - else it will be throttled. Memory Megabytes (MB) Defined as an integer and the unit is Megabytes. So a value of 1 means 1 MB of memory and 1000 means 1GB of memory. You need to specify memory requests and limits. The number you define in the memory request will always be reserved for the application. Your application can occasionaly take more memory than what is requested till the memory limit. If the application takes up more memory than the limit, then the application will be killed and restarted. For e.g, if memory request is 500 MB and limit is 1000 MB, it means that you application will always have 500MB of RAM. You can have spikes of memory usage till 1 GB, beyond which the application will be killed and restarted. Ephemeral Storage Megabytes (MB) Temporary disk space to keep code, artifacts, etc which is lost when the pod is terminated. Defined as an integer and the unit is Megabytes. A value of 1 means 1 MB of disk space and 1000 means 1GB of disk space. You need to specify ephemeral Storage requests and limits. If you specify 1 GB as request and 5 GB as limit, you will have guaranteed access to 1GB of disk space. You can go upto 5GB in case there is disk left on the machine, but we shouldn't rely on this. If the application tries to take up more than 5GB, the application will be killed. GPU GPU-Type and Count Graphics processing power required by the application. You need firstly specify which GPU you want to provision for your Application (GPUs can be of the following types: K80, P4, P100, V100, T4, A10G, A100_40GB, A100_80GB, etc.). You can find more details about these GPUs here . In the case of AWS or GCP , you can do this by selecting the GPU-Type (read more here . In case you are using Azure or some other cloud you will just have to specify the Nodepool that contains the GPU (read more here . Secondly, you need to specify the GPU-Count . Please note that if you ask for GPU count as 2 and type as A100, you will get a machine with atleast 2 A100 GPU cards. Its possible in some cloud providers that one machine has 4 A100 GPU cards. In this case, your application will use 2 of the 4 GPU cards and another application can use the rest 2 cards. Shared Memory Megabytes (MB) Shared memory size is needed for data sharing between processes. This is useful in certain scenarios, for example, Torch Dataloaders rely on shared memory to efficiently share data between multiple workers during training. Defined as an integer and the unit is Megabytes. A value of 1 means 1 MB of shared memory and 1000 means 1GB of shared memory. In case your use-case requires Shared memory, and the usage exceeds the Shared memory size, your applications replica will get killed Setting Resources and Node Constraints Setting resources for your application's pods ensures they have the necessary capabilities to run smoothly and efficiently. However, you can further enhance performance and control by defining constraints on the nodes that are brought up to run your pods. AWS and GCP: Node Selectors If you are using AWS and GCP, we further enable you to define which nodes or instance families can be provisioned to run your pod. This is done through Node-Selector Node-Selector enables you to specify which nodes your application's pods should run on. This can be useful in scenarios such as when you know that your application demands a substantial amount of CPU resources. Here you can utilize a Node-Selector to guarantee that your pods only run on nodes with the required number of CPU cores. Setting resources on AWS and GCP through User Interface Setting resources on AWS and GCP via Python SDK Setting GPU using GPU Type Setting Instance Families from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Resources, NodeSelector, NvidiaGPU service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + resources=Resources( # You can use this in `Job` and `Notebooks` too. + cpu_request=0.2, + cpu_limit=0.5, + memory_request=128, + memory_limit=512, +. # Supported GPUs are -> \"T4\", \"A100_40GB\", \"A100_80GB\", \"V100\" + devices=[NvidiaGPU(name=\"T4\", count=1)]], + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Resources, NodeSelector service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + resources=Resources( # You can use this argument in `Job` too. + cpu_request=0.2, + cpu_limit=0.5, + memory_request=128, + memory_limit=512, + node=NodeSelector( + instance_families=[\"e2\",\"n1\"], + ) + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Azure, on-prem-clusters, and other cloud providers: Nodepool Selector Azure and other Generic Clusters, don't support Node-Selector . To set constraints on the nodes, you can instead set up Nodepool. A node pool is a collection of nodes that are all configured identically. You will first have to set up these Nodepools following this guide . Once you have set the Nodepool, you can specify from which node pools can the nodes be brought to bring up the applications pods. Setting resources on Azure, on-prem-clusters, and other cloud providers through User Interface Setting resources on Azure, on-prem-clusters, and other cloud providers via Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Resources, NodepoolSelector, NvidiaGPU service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + resources=Resources( # You can use this argument in `Job` too. + cpu_request=0.2, + cpu_limit=0.5, + memory_request=128, + memory_limit=512, + ephemeral_storage_request=512, + ephemeral_storage_limit=1024, +. devices=[NvidiaGPU(count=1),] + node=NodepoolSelector( + nodepools=[\"a100\",\"g2\"], + ) + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Choosing between Spot and On-demand Instances Spot and on-demand instances are two types of instances offered by cloud providers like AWS, GCP, and Azure. On-demand instances guarantee availability and once obtained will not be moved away unless the machine has a failure and the cloud provider decides to recycle it. Spot instance will be around 50-70% cheaper than on-demand instances but they do not guarantee availability. This means that a spot machine can be taken away at any point the cloud provider faces a demand for that machine type. Truefoundry automatically brings in another machine in case a spot machine is taken away, but this does mean a unavailability of a few seconds to minutes in case a spot instance is taken away. This time is needed to bring up the new machine, download the docker image and then start the service. In case of AWS, this process is a bit faster since we start bringing another machine the moment AWS tells us a spot instance is going to be preempted - which is usually 2 minutes before the actual preemption. When to use Spot Instances? Spot instances are a good option for nodes when: You are running multiple replicas of a service - hence the chances of all the spot machines going down at the same time is extremely low. You are running a long running training job and saving checkpoints which allows you to resume the training from the last checkpoint in case the machine is taken away. Your service can handle going down and restarting on a different node. Using spot instances is not a good idea in the following scenarios: You are running a training job and you cannot afford to restart the job from scratch in case it gets preempted in the middle. You are running only one replica of a service and cannot tolerate 1-2 minutes of downtime. You are hosting services on GPU and using less than 3 replicas and cannot tolerate downtime. This is because GPU machines take longer to get replaced. Capacity-Type modes offered in TrueFoundry TrueFoundry offers three different capacity-type modes for instance: On-demand : Provides consistent performance and guaranteed availability. Spot : Cost-effective option with the risk of interruptions. In this case, if one spot goes away, we will try to replace it with another spot instance. If spot instances are not available, then service will not get an instance to run on. This mode is recommended only for dev workloads. Spot (with fallback on On-demand) : In this case, the application is preferably put on a spot instance. However, if a spot machine is preempted, we try to find another spot machine to replace it. If we cannot find a spot machine, we bring up an on-demand machine to serve the application then. How do you use Spot on AWS and GCP? In AWS and GCP you can specify that you only want Spot Instances to be used for a particular Service by utilizing Node Selectors. Using Spot on AWS and GCP through User Interface Using Spot on AWS and GCP via Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Resources, CapacityType service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + resources=Resources( # You can use this argument in `Job` too. + cpu_request=0.2, + cpu_limit=0.5, + memory_request=128, + memory_limit=512, + ephemeral_storage_request=512, + ephemeral_storage_limit=1024, + node=NodeSelector( + instance_families=[\"e2\",\"n1\"], + capacity_type=CapacityType.spot_fallback_on_demand + ) + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") How do you use Spot on Azure, on-prem clusters, and other cloud providers? In Azure, on-prem clusters, and other cloud providers, before utilizing Spot Instances, you must first create a Nodepool specifically for them. You can refer to the following documentation for creating those node pools. Once your Spot Nodepool is created, you can select it during service deployment. Using Spot on Azure, on-prem-clusters, and other cloud providers through User Interface Using Spot on Azure, on-prem-clusters, and other cloud providers via Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Resources, NodePoolSelector service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + resources=Resources( # You can use this argument in `Job` too. + cpu_request=0.2, + cpu_limit=0.5, + memory_request=128, + memory_limit=512, + ephemeral_storage_request=512, + ephemeral_storage_limit=1024, + node=NodepoolSelector( + nodepools=[\"a100-spot\",\"g2-spot\"], + ) + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/environment-variables-and-secrets-services": "Environment Variables and Secrets Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Environment Variables and Secrets All Pages Start typing to search\u2026 Environment Variables and Secrets Environment variables are key-value pairs stored outside of the code itself, accessible to the application during runtime. This approach effectively separates configuration from the code, offering several benefits: Secure Storage of Sensitive Data : Sensitive information, such as database credentials or API keys, is not embedded directly into the code, reducing the risk of exposure. Ease of Configuration Management : Configuration can be easily managed and adapted based on the environment (development, staging, production, etc.), without modifying the code. Example: Hardcoding Information vs. Using Environment Variables For instance, consider accessing database from a Service in a traditional approach. You might hardcode the connection details directly into the code: Python import psycopg2 # Connect to the database conn = psycopg2.connect( database=\"your-database-name\", user=\"your-username\", password=\"your-password\" ) # Retrieve the secret value from the database cursor = conn.cursor() cursor.execute(\"SELECT some_value FROM table\") value = cursor.fetchone()[0] However, if we hardcode the values, it will be difficult to manage across different environments since the database credentials will be different across dev, staging and prod. To solve this issue, you can utilize environment variables to store and access these values securely. Configure environment variables like YOUR-DATABASE-URL , YOUR-USERNAME , and YOUR-PASSWORD during deployment configuration, and retrieve them within your code using os.environ(ENV_VAR_NAME) Python import os # Retrieve the secret value from the environment variable database_url = os.environ['YOUR-DATABASE-URL'] database_url = os.environ['YOUR-USERNAME'] database_url = os.environ['YOUR-PASSWORD'] # Use the secret value to connect to the database conn = psycopg2.connect(database_url) # Retrieve the value from the database cursor = conn.cursor() cursor.execute(\"SELECT some_value FROM table\") value = cursor.fetchone()[0] By separating configuration data from the code, you can seamlessly switch between different environments (production, development, etc.) without altering the code itself. Simply modify the environment variables during deployment configuration, allowing you to maintain a consistent codebase across different environments. Secrets: Enhanced Protection for Highly Sensitive Data For sensitive keys like database passwords, api-keys, its not advisable to provide them directly as environment variables in the deployment configuration since everyone can see the deployment spec and get access to these sensitive passwords. Hence, we usually store such sensitive values in secret managers (AWS SSM, AWS Secret Manager, GCP Secret Manager, Azure Vault or Hashicorp Vault) and then only provide the key to the secret in the environment configuration. Translating the key to the actual value is done at a different step. Truefoundry provides an easy way to put the sensitive values in Secret Managers and then just provide the FQN of the secret (of the form tfy-secret://user:my-secret-group:my-secret ) in the deployment spec. You can read about how to create the secrets here . Truefoundry will automatically fetch the value and inject it into the environment at runtime. How to inject environment variables and secrets in TrueFoundry Via the User Interface (UI) Via the Python SDK Both Service and Job classes have an argument env where you can pass a dictionary. The dictionary keys will be assumed as environment variable names and the values will be the environment variable values. In case of Secrets enter the Secret FQN in the value deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, Port service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + env={ + \"MY_ENV_VAR_NAME\": \"MY_ENV_VAR_VALUE\", + \"MY_SECRET\": \"tfy-secret://user:my-secret-group:my-secret\", + }, ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/scaling-and-autoscaling": "Autoscaling Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Autoscaling All Pages Start typing to search\u2026 Autoscaling While deploying a service, you can provision multiple instances of your container (pod in the kubernetes world) so that the incoming traffic is load-balanced between the multiple instances. We refer to each such instance as a replica in Truefoundry. While deploying at Truefoundry, the default value of a replica is 1. While this is fine for development purposes, a single replica is not the best choice for production workloads because of the following reasons: If one replica goes down because of any reason, the entire service goes down. Having 2-3 replicas at minimum helps provide more fault-tolerance. A single replica might not be able to take all of the incoming traffic and hence cause client side errors. To increase the number of replicas, we can either set it to a fixed higher value or enable autoscaling so that the number of replicas is automatically adjusted based on some configuration of incoming traffic, or cpu usage. Setting Fixed Number of Replicas This can be a good choice if the incoming traffic remains constant and we have a good idea of the number of replicas needed. To set the replicas using the UI, you can follow the steps below: If you are using Python SDK to manage deployments then you need to add the following in your deploy.py file: Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + replica=3, ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Enable Autoscaling When the traffic or resource usage of the service is not constant, we ideally want the number of replicas to go up and down based on the incoming traffic. We need to define the minimum and maximum number of replicas in this case and the autoscaling strategy will decide what should be the number of replicas between the min and max replicas. We can enable autoscaling based on the following parameters: CPU Usage: If the application's cpu usage goes up when traffic increases, this can be a good parameter to autoscale on. For e.g., let's say your application (one replica) consumes 0.3 vCPU on steady state - however, as traffic goes up, the CPU usage starts increasing to a max of 1 CPU. In this case, setting autoscaling to trigger when CPU usage is greater than 0.6 can be a good idea. Requests Per Second (RPS) : This is the easiest to reason about and calculate. We can benchmark our service to decide how many requests per second can be served by one replica using benchmarking tools like Locust . Let's say one replica can serve 10 requests per second without an degradation in quality (increase in latency or errors). If we expect the traffic to vary from 50 requests per second to 200 requests per second, we can set the minimum replicas to 5, maximum to 20 and set the rps autoscaling metric to 10. Time Based Autoscaling : This can be useful if the traffic patterns shift based on timezones, or you want to shut down dev workloads during non-office hours. For e.g, if you want to scale up replicas only between Monday to Friday 9AM to 9PM, you can set time based scheduling with cron start schedule as 0 9 * * 1-5 and end schedule as 0 21 * * 1-5 . In most cases, RPS turns out as the most effective and simple to understand metric for autoscaling. Configure Autoscaling for an Application Via UI Configure Autoscaling for an Application Via Python SDK In your Service deployment code ( deploy.py ), include the following for autoscaling on RPS: deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Autoscaling, RPSMetric service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + replicas = Autoscaling( + min_replicas=1, + max_replicas=3, + metrics=RPSMetric( + value=2 + ), + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") In your Service deployment code ( deploy.py ), include the following for autoscaling on CPU utilization: deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Autoscaling, CPUUtilizationMetric service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + replicas = Autoscaling( + min_replicas=1, + max_replicas=3, + metrics=CPUUtilizationMetric( + value=30 + ), + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") In your Service deployment code ( deploy.py ), include the following for time based autoscaling: deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, Port, Autoscaling, CronMetric service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + replicas = Autoscaling( + min_replicas=1, + max_replicas=3, + metrics=CronMetric( + desired_replicas: Any | None, + start: \"0 9 * * 1-5\", # Check https://crontab.guru/ for cron expressions + end: \"0 21 * * 1-5\", # You can see the list of supported timezones at https://docs.truefoundry.com/docs/list-of-supported-timezones + timezone: \"Europe/London\" + ), + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Advanced configuration polling_interval : Time interval in seconds to check the metrics on. By default, autoscaler will check metrics every 30 seconds. cooldown_period : Time in seconds for which autoscaler waits before scaling to absolute zero replicas. It is only application in case the min_replicas is set to 0 . (default: 300) Auto Shutdown This feature allows your workloads to automatically scale down to zero when they are idle, effectively eliminating unnecessary costs without sacrificing performance. You can specify the time during which, if no requests are received, the replicas should be scaled down to zero. Installation You need to install an infra component called Elasti before using it. You can go to cluster page and install the component from modules. Usage This feature can be enabled from the advanced configuration in a Service's deployment configuration page. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/benchmarking-your-deployed-service-using-locust": "Benchmarking your deployed service using Locust Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Benchmarking your deployed service using Locust All Pages Start typing to search\u2026 Benchmarking your deployed service using Locust This guide will help you learn how to benchmark your deployed service. When deploying a service into production, it becomes essential to thoroughly understand its performance metrics. Key considerations include: Determining the service's capacity in terms of handling requests per second. Assessing the threshold of concurrent requests, indicating the number of users the service can accommodate simultaneously. Analyzing how latency fluctuates with increasing traffic volume. These inquiries are important as they inform crucial decisions regarding service configurations and operational strategies: Determining the requisite number of replicas necessary. Evaluating the necessity of implementing auto-scaling mechanisms. If auto-scaling is necessary: Selecting appropriate metrics for triggering auto-scaling mechanisms (e.g., request per second, CPU utilization, or custom metrics). Establishing thresholds for these auto-scaling strategies. This evaluation ensures adherence to service level agreements (SLAs) while simultaneously optimizing costs. In this guide, we will use Locust which is an open-sourced tool for benchmarking services. Setup You can setup locust in any environment with python installed with the following command: pip install locust Writing the Locust File In order to benchmark your service with locust, you need to write a locust file. You need to define what API Endpoint (path) you need to benchmark and write a sample request for the same. Here is a small example to benchmark a deployed LLM. You can write this script to benchmark to any service and not just LLMs. Note: You will need to replace your model name with the name of deployed service. locust_benchmark.py from locust import FastHttpUser, task class HelloWorldUser(FastHttpUser): @task def hello_world(self): self.client.post( \"/v1/completions\", json={ \"model\": \"<Add your deployed service name>\", \"prompt\": [ \"This is a test prompt\" ], \"max_tokens\":80, \"ignore_eos\":True } ) Running the benchmarks You can start the launcher for locust with the following command: locust -f locust_benchmark.py Once you run this, you will find the find a service running on port 8089 Now, open http://localhost:8089 in a browser window. You will find the UI like this: In the section of host, paste the endpoint of your \"Service\" by copying the deployed endpoint as shown below: Once you paste the link, you can click on \"Start Swarming\" after setting the following parameters: Number of Users: Number of concurrent users that will bombard your service Spawn Rate: If multiple users are selected, the rate at which it will create new users (this can be 1 by default) Once you start swarming, you can see the results on the dashboard: Once this is setup, you can edit and increase the number of users from top by clicking on Edit. You can view the detailed charts by clicking on \"Charts\" tab as shown in the picture. The results look something like this. Deploying this Locust Script as a Service: While you can run this script locally, your internet speed and difference in local setup of different users might affect the results. For this, you can deploy this as a service on Truefoundry. You will need two files: locust_benchmark.py (shown above) requirements.txt The contents of requirements.txt are: requirements.txt locust # add other dependencies if used in your locust script Once you have this ready, please go to Truefoundry UI and follow the following steps: Click on New Deployment button on top right of your screen. Select your workspace Select Code from Laptop Click on Next Follow the guide from the UI Step 2 to 4 are illustrated in the image below: Once you complete all the steps and deploy your Service, you can access the deployed locust benchmarking script from the UI by clicking on the \"Endpoint\" as shown below: Updated 5 months ago",
    "https://docs.truefoundry.com/docs/liveness-readiness-probe": "Liveness/Readiness Probe Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Liveness/Readiness Probe All Pages Start typing to search\u2026 Liveness/Readiness Probe Truefoundry helps you deploy on Kubernetes and one of the key advantages here is that Kubernetes will automatically restart your process if something goes wrong. It can also stop routing requests to a replica of a service which is not yet ready or healthy to serve the traffic. The way this works is by telling Kubernetes to keep checking at some regular intervals some API endpoints on your service to detect if the service is ready and healthy. There are two types of such health checks: Liveness probe : Checks whether the service's replica is currently healthy. If the service is not healthy, the replica will be terminated and another one will be started. Readiness probe : Checks whether the service's replica is ready to receive traffic. Until the readiness probe succeeds, no incoming traffic will be routed to this replica. The main difference between liveness and readiness probes is that liveness probes check if the service is running, while readiness probes check if the service is ready to serve requests. Readiness probes are also used to prevent traffic from being routed to replicas that are still starting up or that are in the process of being restarted. For example, a machine learning service may take some time to load its model before it is ready to serve requests. During this time, the service would still be considered to be live (i.e., the liveness probe would return a success response), but it would not be ready to serve requests (i.e., the readiness probe would not return a success response). This would prevent traffic from being routed to that replica until the model has been loaded and initialized. How to add Liveness and Readiness Probes to your deployed services To add liveness and readiness probes to your deployed service, you will need to: Create two endpoints in your service, one for the liveness probe and one for the readiness probe. The endpoints should return a successful response if the service is healthy and a failed response if the service is unhealthy. Configure your deployment to use the health check endpoints. You can do this via the user interface (UI) or via the Python SDK. Example FastAPI health check endpoints The following example shows two simple FastAPI health check endpoints /livez and /readyz that will be used by the Health Checks: app.py from contextlib import asynccontextmanager from fastapi import FastAPI app = FastAPI(root_path=os.getenv(\"TFY_SERVICE_ROOT_PATH\")) # Flag to indicate whether the model is loaded and ready model_loaded = False @app.get(\"/livez\") def liveness(): # Check if the service is running return True @app.get(\"/readyz\") def readyness(): # Check if the service is ready to receive traffic # For example, check if the model is loaded and if the initialization task is complete. # Once loaded, set the model_loaded flag to True return model_loaded @app.get(\"/\") async def root(): return {\"message\": \"Hello World\"} Configuring Health Check for your Service Here are the parameters for liveness and readiness probes: Parameter Description path The path to the health check endpoint. port The port on which the health check endpoint is listening. initial_delay_seconds Number of seconds to wait after the container has started before checking the endpoint. period_seconds How often to check the endpoint. timeout_seconds Number of seconds to wait for a response from the endpoint before considering it to be down. success_threshold Number of times in a row the endpoint must respond successfully before the container is considered to be healthy. failure_threshold Number of times in a row the endpoint can fail to respond before the container is considered to be down. For example, let's say we have the following configuration: Parameter Value path /health port 8080 initial_delay_seconds 10 period_seconds 5 timeout_seconds 2 success_threshold 3 failure_threshold 2 This configuration will cause the probe to check the /health endpoint on port 8080 every 5 seconds. The probe will wait for 2 seconds for a response before failing the probe. The probe will require 3 consecutive successful probes before marking the container as healthy, and it will allow 2 consecutive failed probes before marking the container as unhealthy. Via the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, Port, HealthProve, HttpProbe service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + liveness_probe=HealthProbe( + config=HttpProbe(path=\"/livez\", port=8000), #Replace /livez with application specific path + initial_delay_seconds=0, + period_seconds=10, + timeout_seconds=1, + success_threshold=1, + failure_threshold=3, + ), + readiness_probe=HealthProbe( + config=HttpProbe(path=\"/readyz\", port=8000), #Replace /readyz with application specific path + period_seconds=5, + ), ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Viewing health check logs You can go to the Pods tab on the dashboard, and then click on the logs. You will be able to see the logs of the replica, including the logs of the health check probes. The GET /livez and GET /readyz giving 200 OK messages indicate that the health checks are passing and that the service is healthy. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/rollout-strategy": "Rollout Strategy Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Rollout Strategy All Pages Start typing to search\u2026 Rollout Strategy Configure how to rollout a new version of your application Rolling out a new version of your application shouldn't incur downtime for your application - specially for production applications. In some cases, you might also want to test the new version of your application on a small segment of traffic before rolling out to everyone to catch unexpected issues. This is where the rollout strategy comes in. Let's imagine you have 5 replicas running for v1 of your service. We want to roll out our new shiny version v2. There are a few ways we can roll out v2: Stop all 5 replicas of v1 and then bring up 5 replicas of v2. - This will cause a downtime of the application while the 5 replicas of v2 are starting up and hence is not a great idea. Start 1 replica of v2 - when 1 replica of v2 is up, stop 1 replica of v1 . Continue the process again for the remaining 4 replicas till all 5 replicas are swapped - This makes sure there are always 5 replicas of v2 that are running and there is no disruption in quality of the service. However, there is a small duration when 6 replicas will be live since we only stop v1 replica after v2 replica is live. This strategy is called Rolling Update Strategy. Stop 1 replica of v1, then start 1 replica of v2 . Continue the process till all 5 replicas of v1 are swapped. If we do this, there is a small period of time when 4 replicas are serving the traffic and hence this can cause a slight disruption in traffic, specially if all 5 replicas are fully required to serve the traffic. This strategy is called Rolling Update Strategy. Bring up 1 replica of v2 - once it is up, divert 20% traffic to v2. Then stop one replica of v1, bring up another replica of v2 and divert additional 20% traffic - hence making 40% traffic served by v2 and 60% traffic served by v1. You can continue this process till 100% traffic is served by v2 and 0% is served by v1 - This is what is called Canary deployment. . This might seem similar to the Rolling Update strategy defined above - however, in case of Canary, you can shift traffic smaller than the traffic handled by one pod, pause in between the rollout and control the traffic shift increments in a more finegrained way. Bring up 5 replicas completely of v2, shift 100% traffic to v2 and then stop the 5 replicas of v1. - This is what is called BlueGreen Deployment. In the approaches #2 and #3, we might want to swap more then 1 replica at a time to increase the speed of rollout. Since otherwise, if there are a 1000 replicas, rolling out one replica at a time can cause the deployment to take multiple hours. Using Truefoundry, you can configure the rollout strategy according to your needs. The different rollout strategies and their configuration are described below. Rolling Update Strategy Rolling Update is a deployment strategy that seamlessly replaces the old version of your application with the new one, ensuring a gradual transition without disruptions. In this case, we want to be able to configure how many replicas of the old version get replaced with the new version and whether we create new replicas of the new version before shutting off the replicas of the older version. This can be configured using the following parameters: Max Surge(%) : This defines the max number of replicas of the older and newer version of the application that can be live at any point during the rollout. This is expressed as a percentage of the current number of the replicas. If the current number of replicas is 10 and we set max surge to 20%, this means that 12 replicas can at max be live any point. What this implies is the system will bring up 2 replicas of v2 before shutting down 2 v1 replicas. Max Unavailable(%) : This defines how many minimum replicas (old + new version combined) should be available during the rollout. It is expressed as a percentage of the current number of the replicas.. For example, if replica count is set to 12 for the deployment and max unavailable is set to 25%, then minimum of 9 replicas need to be live at any point during the rollout. What this means is the system can bring down 3 replicas of v1 before starting the 3 replicas of v2. Let's assume we are upgrading the deployment from v1 to v2 assuming both Max surge and max unavailable are set to 25% and initial replicas are 12. Step v1 Replicas v2 Replicas Total Replicas Before rollout 12 0 12 It will bring down 3 v1 replicas because minimum available replica(max unavailable) should be 9. 9 0 9 It will bring up 6 v2 replicas because maximum available replicas(max surge) can be 15 9 6 15 Once all v2 replicas are available then it will again bring down 6 v1 replicas 3 6 9 It will bring up 3 v2 replicas because the total should be 12 at rollout completion 0 12 12 Rollout Complete 0 12 12 A good default startegy is to keep max_unavailable at 10% and max_surge at 10%. If you are working with GPU machines and don't want additional machines to come up during rollout, you can set max_surge to 0% and max_unavailable to 20% Configuring Rolling Update for an Application Via the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: deploy.py truefoundry.yaml from truefoundry.deploy import Build, Service, DockerFileBuild, Rolling service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + rollout_strategy=Rolling( + max_surge_percentage=10, + max_unavailable_percentage=25, + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") name: my-service type: service image: type: build build_source: type: local build_spec: type: dockerfile ports: - port: 8501 host: your_host +rollout_strategy: + type: rolling_update + max_surge_percentage: 10 + max_unavailable_percentage: 25 One limitation that we have in rolling update is that we cannot control how much traffic goes to v1 and v2. This is where Canary rollout comes into play. Canary Canary deployment is a cautious approach to rolling out updates where a small subset of users gets to try the new version first before it's made available to everyone. It is ideal for minimizing risks associated with major updates to test the new deployment before a full rollout. Some key terms to get comfortable with before we can understand this strategy. Canary Steps: You can have multiple stages/steps in your canary rollout. You get absolute control control at each step regarding how much traffic should be redirected to the new version and should it automatically move to the next step after a certain time interval or a manual trigger is needed to move it to the next step. Set traffic weight to: This is the percentage of traffic that should be redirected to the new version at each stage. Pause for: This helps to define if the rollout should move to next step after a certain time or a manual trigger is needed. Let's assume we are upgrading the deployment from v1 to v2, assuming we want to: Deploy total 10% traffic to new version and wait for 60 seconds. Deploy total 30% traffic to new version and wait until we manually trigger to next step. Deploy total 80% traffic to new version and wait for 30 seconds. Steps v1 Traffic(%) v2 Traffic(%) Pause For(seconds) Before rollout 100% NA 1 - Automatic 90% 10% 60 2 - Automatic 70% 30% Pause Indefintely 3- Manual trigger next step 20% 80% 30 Rollout complete 0% 100% NA Configuring Canary for an Application Via the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: deploy.py truefoundry.yaml from truefoundry.deploy import Build, Service, DockerFileBuild, Canary, CanaryStep service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + rollout_strategy=Canary( + steps=[ + CanaryStep(weight_percentage=25, pause_duration=30), + CanaryStep(weight_percentage=50, pause_duration=30), + CanaryStep(weight_percentage=75, pause_duration=30) + ] + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") name: my-service type: service image: type: build build_source: type: local build_spec: type: dockerfile ports: - port: 8501 host: your_host +rollout_strategy: + type: canary + steps: + - weight_percentage: 25 + pause_duration: 30 + - weight_percentage: 50 + pause_duration: 30 + - weight_percentage: 75 + pause_duration: 30 Blue-Green: Blue-Green is a strategy that ensures a seamless transition between different versions by maintaining two identical environments: one for the current version (blue) and one for the new version (green). It minimises downtime and provides an instant fallback mechanism in case issues arise in the new version. Let's assume we are upgrading the deployment from v1(blue) to v2(green): We will create v2 as a full clone of v1. If there were 12 replicas in v1 then we create exact 12 replicas in v2. Once all the pods are ready in v2, we move full traffic to v2. This moving traffic from v1 to v2 can be automatic or it can be manual depending on what we configure. Blue Green Configuration For configuring Blue Green, you can adjust the following settings: Auto-promotion: Toggle this setting to automatically promote the new version to active once it passes all tests and checks. Auto-promotion seconds: Specify the duration, in seconds, to wait before automatically promoting the new version to active. Configuring Rolling Update for an Application Via the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: deploy.py truefoundry.yaml from truefoundry.deploy import Build, Service, DockerFileBuild, BlueGreen service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + rollout_strategy=BlueGreen( + enable_auto_promotion=True, + auto_promotion_seconds=30 + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") name: my-service type: service image: type: build build_source: type: local build_spec: type: dockerfile ports: - port: 8501 host: your_host +rollout_strategy: + type: blue_green + enable_auto_promotion: true + auto_promotion_seconds: 30 One drawback of using this strategy is heavy cost. If you had 12 replicas in v1 then 12 v2 replicas are brought up so you have to pay cost for a total of 24 replicas until the rollout is complete. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/mounting-volumes-service": "Mounting Files Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Mounting Files All Pages Start typing to search\u2026 Mounting Files In some applications, you might need a file to be present at a certain path for the application to work. Or you have data in directories that need to be mounted for the application to work. TrueFoundry allows you to mount files in the following ways: Mount Volume If your application needs access to multiple files of data or multiple directories, you can use a volume to store the data and then mount the volume at the desired path. You can learn more about Volumes here . To use a persistent volume, we will first need to create one and then attach it to your deployment. You can learn how to create volumes using the Creating a Volume guide Mount Volume through the User Interface (UI) Mount Volume using the Python SDK You can add the following in your deploy.py file. deploy.py from truefoundry.deploy import Build, Service, DockerFileBuild, VolumeMount service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + mounts=[ + VolumeMount( + mount_path=\"/model\", #or your desired path + volume_fqn=\"your-volume-fqn\" + ) + ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Mount Secret This is similar to string mount, except, in this case, you will directly provide a TrueFoundry Secret FQN. You can read more about Secrets and how to create them here . The content in the secret will be dumped into a file and mounted in the provided location. A good usecase of this is for mounting Google credentials file which you might need to access Google services. Mount secret through the User Interface (UI) Mount secret using the Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, SecretMount service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + mount = [ + SecretMount( + mount_path=\"/etc/google-credentials.json\", + data=\"tfy-secret://user:my-secret-group:my-secret\" + ) + ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Mount String This can be useful if you need a small configuration file to be present at a certain file path. To configure this, you need to provide the path where it needs to be mounted and the string data that should be in that file. A good example can be a Nginx configuration file that you can mount along with the Nginx docker image. Mount string through the User Interface (UI) Mount string using the Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, StringDataMount service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + mount = [ + StringDataMount( + mount_path=\"/etc/nginx/conf.d/my_config.conf\", + data=\"server { listen 80; }\" + ) + ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Using mounted files in your deployment Once you have attached a file to your deployment, you can use it in your deployment like any other file. For example, if you mounted the file to /etc/config.json , you can access the file in the /etc/config.json path Python import json with open(\"/etc/config.json\", \"r\") as f: config_data = json.load(f) # Access specific values from the config data dictionary api_key = config_data[\"api_key\"] database_url = config_data[\"database_url\"] # Use these values in your application logic print(f\"API key: {api_key}\") print(f\"Database URL: {database_url}\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/access-data-from-s3-or-other-clouds-services": "Access Cloud Services like S3 Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Access Cloud Services like S3 All Pages Start typing to search\u2026 Access Cloud Services like S3 In some instances, your Service may need to access data stored in S3 or other cloud storage platforms. To facilitate this access, you can employ one of two approaches: Credential-Based Access through environment variables This approach involves defining specific environment variables that contain the necessary credentials for accessing the cloud storage platform. For instance, to access S3, you would set environment variables for the AWS access key ID and secret access key, the environment variables being: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY IAM Role-Based Access through Service Account The second approach is to provide your Service with a Role with the necessary permission through Service Accounts. Service Accounts provide a streamlined approach to managing access without the need for tokens or complex authentication methods. This approach involves creating a Principal IAM Role within your cloud platform, granting it the necessary permissions for your project's requirements. Here are detailed guides for creating Principal IAM Roles in your respective cloud platforms and integrating them as Service Accounts within the workspace: AWS: Authenticate to AWS services using IAM service account GCP: Authenticate to GCP using IAM serviceaccount Once you've configured the Service Account within the workspace, you can simply toggle the Show Advanced Fields option at the bottom of the form. This will reveal an expanded set of options, from which you can select the desired Service Account using the provided dropdown menu. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/download-and-cache-models": "Download Models and Artifacts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Download Models and Artifacts All Pages Start typing to search\u2026 Download Models and Artifacts If you are deploying models as APIs, you will need the model file to be present along with the service so that you can load and serve it. There are a few ways people usually end up doing this: Bake the model file into the docker image This works, however, it has the following disadvantages: The docker image size can become huge if the model file is big. We will need to rebuild the docker image if the model file changes. Every time the pod needs to come up on a new machine, it will end up downloading the model file again. Download the model file from S3 at the start of the server In this case, every time a pod comes up, we need to download the model first which leads to high startup time. It also means more networking costs every time the models are downloaded. Create a volume, download the model file on the volume, and mount the volume to your service This approach has a lot of benefits: Smaller docker image size leads to faster startup Faster loading of models from volume leads the faster startup times Cost savings because of not having to download the model repeatedly. However, this can be a bit cumbersome to set up since the steps need to be done manually. Also, one more thing to be careful of here is the upgrade process from one model version to another. The key issues here are: Upgrading requires manual intervention of having to download the new version of the model on the volume. If we swap the older model on the volume with the newer model, and a pod restarts just after that, it will lead to one pod using the newer version of the model, even though we haven't started the deployment process. To overcome the above issues, you can use the Artifacts Download feature in Truefoundry which automatically provisions a volume, downloads the model onto the volume, mounts the volume and also handles upgrades safely. The way this works during the first deployment is: As you can see in the diagram above, the user will need to define the link to the model to be downloaded and an environment variable in which Truefoundry will provide the path where the model is present. The reason this is done this way is to provide a smooth and failsafe upgrade process. When you are deploying again, the following flow will be followed: In this scenario, the model is not downloaded if there is no change in the artifact version. However, if there is a change, Truefoundry downloads the new model at a different path and then restarts all the servers with the new path in the environment variable. It also cleans up the older model once the rollout is fully complete. This makes it easy to switch back to the older version during a rollout if anything goes wrong. Download and Cache Models through the User Interface Download and Cache Models using the Python SDK If you are using Python SDK to manage deployments then you need to add the following in your deploy.py file: Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, ArtifactsDownload, ArtifactsCacheVolume, HuggingfaceArtifactSource, TruefoundryArtifactSource service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], + artifacts_download=ArtifactsDownload( + artifacts=[ + HuggingfaceArtifactSource( + model_id=\"NousResearch/Llama-2-7b-hf\", + revision=\"dacdfcde31297e34b19ee0e7532f29586d2c17bc\", + download_path_env_variable=\"MODEL_ID_1\", + ), + TruefoundryArtifactSource( + artifact_version_fqn = \"artifact:truefoundry/your-ml-repo/your-artifact:1\", + download_path_env_variable=\"MODEL_ID_2\" + ) + + ], + cache_volume=ArtifactsCacheVolume( + storage_class=\"azureblob-nfs-premium\", + cache_size=200 + ) + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Using the Cached Model In your Service code, to use the cached model, you can utilize the Download Path Environment Variable you specified while configuring the Download and Cache Artifact . You can do one of these two things, Get it directly in your code using os.environ.get() or Pass it as a command-line argument if your service needs it that way. Direct Reference For example, if you specify MODEL_ID_1 as the Download Path Environment Variable , to download model.joblib file, you can access the cached model like this: Python import os import joblib # Get the model_path using the environment variable model_path_1 = os.environ.get(\"MODEL_ID_1\") # Load the model using the specified path model_1 = joblib.load(os.path.join(model_path_1, \"model.joblib\")) Command Line Argument For example, in case you are deploying an LLM on top of the vLLM model server using the vllm/vllm-openai:v0.2.6 docker image. The following takes the model and tokenizer as a command-line argument. You can pass the Download Path Environment Variable in the arguments. Download Private Hugging Face Hub Models To download a non-public Hugging Face Hub model, such as https://huggingface.co/meta-llama/Llama-2-7b-chat-hf , make sure to include your Hugging Face Hub token in the environment section. You can obtain your token by visiting https://huggingface.co/settings/tokens . Through User Interface Locate the Environment Variables section in the deployment form, and add the token like the following. Through Python SDK Diff from truefoundry.deploy import Build, Service, DockerFileBuild, Port, ArtifactsDownload, ArtifactsCacheVolume, HuggingfaceArtifactSource, TruefoundryArtifactSource service = Service( name=\"my-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ], artifacts_download=ArtifactsDownload( artifacts=[ HuggingfaceArtifactSource( model_id=\"meta-llama/Llama-2-7b-chat-hf\", revision=\"<revision id>\", download_path_env_variable=\"MODEL_ID\", ), ], ), + env={ + \"HF_TOKEN\": \"<paste your token here>\", + \"HUGGING_FACE_HUB_TOKEN\": \"<paste your token here>\" + } ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/using-fractional-gpus": "Using Fractional GPUs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using Fractional GPUs All Pages Start typing to search\u2026 Using Fractional GPUs Nvidia MIG and Nvidia TimeSlicing Fractional GPUs enable us to allocate multiple workloads to a single GPU which can be useful in the following scenarios: The workloads take around 2-3 GB of VRAM - so you can allocate multiple replicas of this workload on a single GPU which has around 16GB of VRAM or more. Each workload has sparse traffic and its not able to max out the GPU usage. There are two ways to use fractional GPUs: TimeSlicing : In this approach, you can slice a GPU into a few fixed number of fractional parts and then choose a fraction of GPU for this workload. For e.g, we can decide to divide a GPU into 10 slices and then request 3 slices for one workload, 5 slices for workload2 and 2 slices for the workload3. This means that worload1 will use 0.3 GPU (compute + 30% of VRAM), workload2 will use 0.5 GPU and workload3 will use 0.2 GPU. However, timeslicing is only used for scheduling the workloads on the same machine - it doesn't mean any actual isolation on the machine. For example, if the GPU machine has 16GB VRAM, its on the user to actually make sure that workload1 takes less than 4.8GB VRAM, workload2 takes less than 8 GB of VRAM and workload3 takes less than 3.2 GB of VRAM. If one workload starts taking more memory suddenly, it can lead to crashing of the other processes. The compute is also shared but one workload can go upto using the complete GPU if the other workloads are idle - its basically context-switching among the three workloads. You can read about this more here. MIG (Multi-Instance GPUs) : This is a feature provided by Nvidia only on the A100 and H100 GPUs - it doesn't work on other GPUs. We can divide the GPUs into a fixed number of configurable parts as mentioned in the table below. The workloads can choose one of the slices and they will get compute and memory isolation. The instances are not exactly the complete fractions of the GPU, but more discrete units as mentioned in the table below. For e.g. let's say we divide one A100 GPU of 40GB into 7 parts - then we can place 7 workloads each using around 1/7 GPU and 5GB VRAM. Please note that we cannot simply provide 2 slices to one workload in this case and expect it to get 2/7 GPU and 10 GB VRAM. Each workload can only get one slice in this case. GPU GPU Compute Fraction / Instance Number of instances per GPU GPU Memory / Instance Configuration Name GPU Instance Profile (for Azure) A100 (40GB) 1/7 7 5GB 1g.5gb MIG1g A100 (40GB) 2/7 3 10GB 2g.10gb MIG2g A100 (40GB) 3/7 2 20GB 3g.20gb MIG3g A100 (80GB) 1/7 7 10GB 1g.10gb MIG1g A100 (80GB) 2/7 3 20GB 2g.20gb MIG2g A100 (80GB) 3/7 2 40GB 3g.40gb MIG3g Prerequisites for Fractional GPU Add Cloud Integration To enable fractional GPUs, we will need to create a separate nodepool of the GPUs and it will not work via standard dynamic node provisioning in AWS / GCP. For Truefoundry to be able to read those nodepools, we have to make sure that we have the cloud integration already done with Truefoundry. To verify if you have the cloud APIs already integrated, you can check on this screen. If its not enabled yet, please follow this guide to enable Cloud Integration. Once cloud integration is added, you need to \"create nodepools\" for MIG or TimeSlicing enabled GPUs. This configuration is different for different cloud providers. Please follow the guide below to enable fractional GPUs on your cluster. Install Latest Version of tfy-gpu-operator Go to Deployments -> Helm -> tfy-gpu-operator . Click on edit (three dots on the right) Choose the latest version of chart (top most) from the dropdown and click on Submit. Enable MIG Azure Create a Nodepool with MIG enabled using the argument --gpu-instance-profile of Azure CLI. Here is a sample command to do the same: Shell az aks nodepool add \\ --cluster-name <your cluster name> \\ --resource-group <your resource group> \\ --no-wait \\ --enable-cluster-autoscaler \\ --eviction-policy Delete \\ --node-count 0 \\ --max-count 20 \\ --min-count 1 \\ --node-osdisk-size 200 \\ --scale-down-mode Delete \\ --os-type Linux \\ --node-taints \"nvidia.com/gpu=Present:NoSchedule\" \\ --name a100mig7 \\ --node-vm-size Standard_NC24ads_A100_v4 \\ --priority Spot \\ --os-sku Ubuntu \\ --gpu-instance-profile MIG1g Refresh the nodepools in the Truefoundry cluster. Deploy your workload by selecting GPU (with count 1) and selecting the correct nodepool. GCP Create a nodepool and pass the mig_profile in accelerator by passing gpu_partition_size=1g.5gb [OR one of the allowed values for MIG profile you can find on top of this page] Shell gcloud container node-pools create a100-40-mig-1g5gb \\ \ue0b2 INT \u2718 \ue0b2 \uf7b7 --project=<enter your project name> \\ --region=<enter your region> \\ --cluster=<enter your cluster name here> \\ --machine-type=a2-highgpu-1g \\ --accelerator type=nvidia-tesla-a100,count=1,gpu-partition-size=1g.5gb \\ --enable-autoscaling \\ --total-min-nodes 0 \\ --total-max-nodes 4 \\ --min-provision-nodes 0 \\ --num-nodes 0 AWS It is not trivial to currently support MIG GPUs on AWS in a managed way, although if you want to try the feature out -> Please refer to these docs Enable Timeslicing Azure Ensure that nvidia-device-plugin config is correctly set in tfy-gpu-operator chart. Go to Helm -> tfy-gpu-operator , click on edit and ensure following lines are present in the values azure-aks-gpu-operator: devicePlugin: config: data: all: \"\" time-sliced-10: |- version: v1 sharing: timeSlicing: renameByDefault: true resources: - name: nvidia.com/gpu replicas: 10 name: time-slicing-config create: true default: all Create a Nodepool with device-plugin.config pointing to the correct time-slicing config with Azure CLI. Here is a sample command to do the same. Shell az aks nodepool add \\ --cluster-name <your cluster name> \\ --resource-group <your resource group> \\ --no-wait \\ --enable-cluster-autoscaler \\ --eviction-policy Delete \\ --node-count 0 \\ --max-count 20 \\ --min-count 0 \\ --node-osdisk-size 200 \\ --scale-down-mode Delete \\ --os-type Linux \\ --node-taints \"nvidia.com/gpu=Present:NoSchedule\" \\ --name a100mig7 \\ --node-vm-size Standard_NC24ads_A100_v4 \\ --priority Spot \\ --os-sku Ubuntu \\ --labels nvidia.com/device-plugin.config=time-sliced-10 Refresh the nodepools on truefoundry cluster. Deploy your workload by selecting GPU (with count 1) and selecting the correct nodepool. GCP gcloud container node-pools create a100-40-frac-10 \\ \ue0b2 \u2714 \ue0b2 \uf7b7 --project=tfy-devtest \\ --region=us-central1 \\ --cluster=tfy-gtl-b-us-central-1 \\ --machine-type=a2-highgpu-1g \\ --accelerator type=nvidia-tesla-a100,count=1,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=10 \\ --enable-autoscaling \\ --total-min-nodes 0 \\ --total-max-nodes 4 \\ --min-provision-nodes 0 \\ --num-nodes 0 AWS Ensure that nvidia-device-plugin config is correctly set in tfy-gpu-operator chart. Go to Helm -> tfy-gpu-operator , click on edit and ensure following lines are present in the values aws-eks-gpu-operator: devicePlugin: config: data: all: \"\" time-sliced-10: |- version: v1 sharing: timeSlicing: renameByDefault: true resources: - name: nvidia.com/gpu replicas: 10 name: time-slicing-config create: true default: all Create nodegroup on AWS EKS with the following label: labels: \"nvidia.com/device-plugin.config\": \"time-sliced-10\" Using fractional GPUs in your Service To use fractional GPUs in your service: Ensure that you have added the desired nodepools. Please sync the cluster nodepools from your cloud account by going to Integrations -> Clusters -> Sync as shown below: You can deploy using either Truefoundry's UI or using Python SDK. Note: Autoscaling of Nodepools will work only in GCP clusters. You will need to manually scale up / scale down nodepools in Azure/AWS. Deploying with UI To deploy a workload that utilizes fractional GPU, start deploying your service/job on truefoundry and in the \"Resources\" section, select nodepool selector Once you select the Nodepool Selector on top right of Resources section, you can now see the Fractional GPUs on the UI which you can select (as shown below) Using MIG GPU Using Timeslicing GPU Deploying with Python SDK You can use fractional GPUs using python SDK with the following changes in the resources change: Using MIG GPUs Python from truefoundry.deploy import ( ... Service, NvidiaMIGGPU, NodepoolSelector, ) service = Service( ... resources=Resources( ... node=NodepoolSelector( nodepools=[\"<add your nodepool name>\"], ), devices=[ NvidiaMIGGPU(profile=\"1g.5gb\") ], ), ) Using Timeslicing GPU Python from truefoundry.deploy import ( Service, NvidiaTimeslicingGPU, NodepoolSelector, ) service = Service( ... resources=Resources( ... node=NodepoolSelector( nodepools=[\"<add your nodepool name>\"], ), devices=[ NvidiaTimeslicingGPU(gpu_memory=4000), ], ), ) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/using-tpus": "Using TPUs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using TPUs All Pages Start typing to search\u2026 Using TPUs Deploy apps on Single Host TPU v4, v5e, v5p slices \ud83d\udea7 Compatibility Notes Currently we only support Single Host TPU Topologies. If you have a use case that can benefit from Multi Host Topologies please reach out to us Currently we only support provisioning/scheduling TPU workloads using Node Auto Provisioning (NAP). We'll add support for nodepools soon. Prerequistes Kubernetes Version While different TPU types require different mininum GKE version, considering all TPU types, Node Auto Provisioning Support and ability to run without privileded mode, we recommend using 1.28.7-gke.1020000 or later and 1.29.2-gke.1035000 or later Please refer to following links for up to date requirements https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#availability https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning#tpus https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning#configuring_tpus https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#plan-tpu-configuration Regional Availability TPUs are available in limited zones. Following table lists the availability for Single Host TPUs. Please refer to following links for up to date availability: https://cloud.google.com/kubernetes-engine/docs/concepts/tpus#availability TPU Type us-west1-c us-west4-a us-central1-a us-central2-b us-east1-c us-east1-d us-east5-a us-east5-c europe-west4-b v4 \u2705 v5e Device \u2705 \u2705 v5e PodSlice \u2705 \u2705 \u2705 v5p \u2705 \u2705 \u2705 Quota Please make sure you have enough quota available in the respective region. You can apply for Quotas on your GCP Console: https://console.cloud.google.com/iam-admin/quotas Node Auto Provisioning Once you have Quota, add the TPU accelerators to NAP's limits. Please follow the guide at https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning#config_file to update these limits. \ud83d\udcd8 CPU and Memory Limits TPU VMs often have large amounts of CPU and Memory. Please make sure to also adjust CPU and Memory resource limits in NAP config. Inadequate limits on CPU and Memory might block TPU nodes from scaling Here is what a sample config looks like for adding TPU accelerators yaml resourceLimits: - resourceType: 'cpu' minimum: 0 maximum: 1000 - resourceType: 'memory' minimum: 0 maximum: 10000 # ... - resourceType: 'tpu-v4-podslice' minimum: 0 maximum: 32 - resourceType: 'tpu-v5-lite-podslice' minimum: 0 maximum: 32 - resourceType: 'tpu-v5-lite-device' minimum: 0 maximum: 32 - resourceType: 'tpu-v5p-slice' minimum: 0 maximum: 32 autoprovisioningLocations: # Change these zones according to your cluster - us-central2-b management: autoRepair: true autoUpgrade: true shieldedInstanceConfig: enableSecureBoot: true enableIntegrityMonitoring: true diskSizeGb: 100 diskType: pd-balanced Adding TPU Devices Using Spec or Python SDK \ud83d\udcd8 Tip You can use the UI to generate equivalent YAML or Python Spec You can add TPU by simply adding it under the resources.devices section like folllows: YAML name: my-service type: service ... resources: node: type: node_selector devices: + - name: tpu-v5-lite-device + type: gcp_tpu + topology: 1x1 cpu_request: 21 cpu_limit: 23 memory_request: 39100 memory_limit: 46000 ephemeral_storage_request: 20000 ephemeral_storage_limit: 100000 ... Python from truefoundry.deploy import Service, Resources, NodeSelector + from truefoundry.deploy import GcpTPU, TPUType ... service = Service( name=\" my-service\", ... resources=Resources( + devices=[ + GcpTPU(name=TPUType.V5_LITE_DEVICE, topology=\"1x1\") + ] cpu_request=21, cpu_limit=23, memory_request=39100, memory_limit=46000, ephemeral_storage_request=20000, ephemeral_storage_limit=100000, node=NodeSelector() ) ) Supported values for name and topologies: TPU Type Name Supported Topologies Equivalent Devices Count v4 PodSlice tpu-v4-podslice 2x2x1 4 v5 Lite Device tpu-v5-lite-device 1x1 , 2x2 , 2x4 1, 4, 8 v5 Lite PodSlice tpu-v5-lite-podslice 1x1 , 2x2 , 2x4 1, 4, 8 v5p Slice tpu-v5p-slice 2x2x1 4 Using UI You can add TPUs to your Service / Job / Notebook / SSH Server by simply selecting a TPU Type and a topology from the Resources section Examples \ud83d\udcd8 Before you begin We recommend reading through following pages first to understand basics of deployment with TrueFoundry Truefoundry Key Concepts Deploy Service using Python SDK Deploy and Run Job using Python SDK Secrets Enumerate TPU chips This simple example Job installs Jax and enumerates the assigned TPU devices truefoundry.yaml name: enumerate-tpu-devices type: job image: type: image image_uri: python:3.11 command: bash /run.sh mounts: - type: string mount_path: /run.sh data: | #!/bin/bash pip install --quiet --upgrade 'jax[tpu]>0.3.0' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html python -c 'import jax; print(jax.devices())' retries: 0 trigger: type: manual resources: node: type: node_selector capacity_type: spot_fallback_on_demand devices: - name: tpu-v5-lite-device type: gcp_tpu topology: 1x1 cpu_request: 21 cpu_limit: 23 memory_request: 39100 memory_limit: 46000 ephemeral_storage_request: 20000 ephemeral_storage_limit: 100000 trigger_on_deploy: true On submitting, it would bring up a TPU node and produce the following output [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] Running Gemma 2B with Jax This Sample Notebook demonstrates running Gemma 2B using Jax in an interactive environment. The notebook can be uploaded directly to Notebook instance running with TPUs on TrueFoundry Deploying Gemma 2B LLM with Jestream & MaxText on TPU v5e This section is adapted from official GKE Guide: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-tpu-jetstream Step 1: Get Access to the model on Kaggle Request Access to the Model on Kaggle: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-tpu-jetstream#sign-consent Create Kaggle Access Token: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-tpu-jetstream#generate-token . You will download a kaggle.json which we will use in the next step Step 2: Add kaggle.json to Secrets Create a Secret add the contents of kaggle.json like follows. Once you have saved, keep the secret fqn handy, we'll use it later Step 3: Create Workspace, GCS Bucket, GCP Service Account and Link Them Create a Workspace to deploy the model to Create a GCS Bucket from GCP Console Run the following script to create GCP Service Account with correct permissions. Make sure to edit your project id, bucket name, regions, workspace name Shell #!/bin/bash gcloud config set project your-project PROJECT_ID=$(gcloud config get project) BUCKET_NAME=tpu-models-bucket REGION=us-central1 LOCATION=us-central1-a WORKSPACE_NAME=tpu-models-workspace SA_NAME=tpu-models-sa gcloud iam service-accounts create \"${SA_NAME}\" gcloud storage buckets add-iam-policy-binding \\ \"gs://${BUCKET_NAME}\" \\ --member \"serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\ --role roles/storage.objectUser \\ --project \"${PROJECT_ID}\" gcloud storage buckets add-iam-policy-binding \\ \"gs://${BUCKET_NAME}\" \\ --member \"serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\ --role roles/storage.insightsCollectorService \\ --project \"${PROJECT_ID}\" gcloud iam service-accounts add-iam-policy-binding \\ \"${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\ --role roles/iam.workloadIdentityUser \\ --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[${WORKSPACE_NAME}/${SA_NAME}]\" Finally edit the workspace you created and add the service account principal Step 4: Deploy and Run the Job to convert the model to MaxText Engine format This Job pulls in the model from Kaggle using the kaggle.json secret we created earlier and puts them in your GCS bucket using the serviceaccount we created earlier Python import logging from truefoundry.deploy import Job, Image, Resources, GcpTPU, TPUType, SecretMount logging.basicConfig(level=logging.INFO) job = Job( name=\"maxengine-ckpt-conv\", # Mention your bucket in command below image=Image( image_uri=\"us-docker.pkg.dev/cloud-tpu-images/inference/inference-checkpoint:v0.2.0\", command=\"/usr/bin/checkpoint_converter.sh -b=tpu-models-bucket -m=google/gemma/maxtext/2b-it/3\", ), # Service account to use to write to the bucket service_account=\"tpu-models-sa\", # Secret to use for fetching the model checkpoint from Kaggle mounts=[ SecretMount( mount_path=\"/kaggle/kaggle.json\", secret_fqn=\"tfy-secret://your-org:kaggle-secrets:kaggle-json\" ) ], resources=Resources( cpu_request=20, cpu_limit=20, memory_request=150000, memory_limit=190000, ephemeral_storage_request=30000, ephemeral_storage_limit=40000, devices=[ GcpTPU( name=TPUType.V5_LITE_DEVICE, topology=\"2x2\" ) ] ), trigger_on_deploy=True, ) # Add the fqn of the Workspace we created earlier job.deploy(workspace_fqn=\"<your-workspace-fqn>\", wait=False) Once this runs successfully, you will get the base and converted checkpoints in your bucket Step 5: Deploy the model via MaxText Engine Python import logging from servicefoundry import Service, Image, Port, Resources, GcpTPU, TPUType logging.basicConfig(level=logging.INFO) service = Service( name=\"maxengine-server\", # Model name and bucket are mentioned in the command image=Image( image_uri=\"us-docker.pkg.dev/cloud-tpu-images/inference/maxengine-server:v0.2.0\", command=\"/usr/bin/maxengine_server_entrypoint.sh model_name=gemma-2b tokenizer_path=assets/tokenizer.gemma per_device_batch_size=4 max_prefill_predict_length=1024 max_target_length=2048 async_checkpointing=false ici_fsdp_parallelism=1 ici_autoregressive_parallelism=-1 ici_tensor_parallelism=1 scan_layers=false weight_dtype=bfloat16 load_parameters_path=gs://tpu-models-bucket/final/unscanned/gemma_2b-it/0/checkpoints/0/items\", ), ports=[ Port(port=9000, expose=False), ], # Service account to use to read from the bucket service_account=\"tpu-models-sa\", resources=Resources( cpu_request=20, cpu_limit=24, memory_request=40000, memory_limit=48000, ephemeral_storage_request=30000, ephemeral_storage_limit=40000, devices=[ GcpTPU( name=TPUType.V5_LITE_DEVICE, topology=\"1x1\" ) ] ), ) # Add the fqn of the Workspace we created earlier service.deploy(workspace_fqn=\"<your-workspace-fqn>\", wait=False) This runs the maxengine server in gRPC mode on port 9000 Step 5: Deploy a HTTP Head Server Finally we can deploy a http head server that is just a gRPC client wrapped in FastAPI app Python import logging from servicefoundry import Service, Image, LocalSource, DockerFileBuild, Port, Resources logging.basicConfig(level=logging.INFO) service = Service( name=\"jetstream-http\", image=Image(image_uri=\"truefoundrycloud/jetstream-http-patched:v0.2.0\"), ports=[ Port( port=8000, host=\"<Your Host>\" ), ], resources=Resources( cpu_request=0.5, cpu_limit=1, memory_request=500, memory_limit=1000, ephemeral_storage_request=1000, ephemeral_storage_limit=4000, ), ) # Add the fqn of the Workspace we created earlier service.deploy(workspace_fqn=\"<your-workspace-fqn>\", wait=False) Once deployed, you can make request with the following body on the /generate endpoint { \"server\": \"maxengine-server.tpu-models-workspace.svc.cluster.local\", \"port\": \"9000\", \"prompt\": \"This is a sample prompt \", \"priority\": 0, \"max_tokens\": 500 } Updated 8 days ago",
    "https://docs.truefoundry.com/docs/intercepts": "Redirect and Mirror Traffic Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Redirect and Mirror Traffic All Pages Start typing to search\u2026 Redirect and Mirror Traffic Intercepts provide a way to attach ad-hoc routing rules for a service deployed on the TrueFoundry platform. We can use it to redirect traffic arriving at a service to another one on the basis of a header. It can be useful to test a new release for a service by intercepting the traffic on the main service and redirecting it to the the copied service. This is useful in development purposes for rapid debugging specially if a lot of microservices are involved. Let's consider the scenario below: Let's imagine we are working on the model microservice and currently the v1 version is deployed. While we can test the model microservice separately by issuing API calls, sometimes its more convenient to check the end to end flow with a new version of the model. To test this, we can swap the model-v1 with model-v2, but this will affect the experience of everyone in case model-v2 doesn't work well. Or we need to duplicate frontend, backend and db to be able to test the entire stack separately. This can become very cost intensive and also a lot of manual work - specially if a lot of developers are testing things independently. Intercepts provide an easy solution to the problem by redirect a small percentage of the traffic to the model-v2 or redirect based on headers in the incoming request. In this case, we can just make a request from the existing frontend with some header like MODEL_VERSION: v2 and configure the interceptor to redirect all requests going to model-v1 with that header to redirect to model-v2. This way the other requests to frontend continue to get routed to model-v1 and only the request that we are testing end up going to model-v2. This can be useful to test end to end scenarios without redeploying the entire stack for every single developer. Using UI Before intercepts can be enabled for a service, we need to allow intercepts for that service. We now can start adding intercept rules To test the intercept you can send a request to the service with the header set to the value in intercept rule and verify that the request indeed lands on the service set as destination. Eg - curl -H \"<header name>: <header value>\" https://<service endpoint> . If the intercept rules don't match, the request will continue to go to the original service. \ud83d\udcd8 Note: A service if associated with an intercept cannot be deleted until the intercept is deleted first Updated 5 months ago",
    "https://docs.truefoundry.com/docs/kustomize": "Patch your Kubernetes Deployment (Advanced) Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Patch your Kubernetes Deployment (Advanced) All Pages Start typing to search\u2026 Patch your Kubernetes Deployment (Advanced) Patch or Add resources using Kustomize TrueFoundry allows you to tweak the most common parameters of the deployment through the service spec. However, there might be situations in which you might want to override some fields that are not exposed in the TrueFoundry Service spec. You can then use Kustomize to add, patch or delete the Kubernetes resources that TrueFoundry deploys on the cluster. Kustomize enables you to Patch the rendered Kubernetes resources generated by the TrueFoundry Application. E.g. Adding extra annotations for Prometheus / Datadog Add extra Kubernetes resources along with your TrueFoundry Application. E.g. Adding extra ConfigMap, Secret, Istio VirtualService, etc \ud83d\udcd8 Supported Application Types Service Job Helm Using Kustomize You can add patches and resources using the kustomize field of the spec \ud83d\udcd8 Resources are patched/added in the same namespace Patches and new resources are applied to the same namespace as the application Here is an example that Adds Prometheus scrape annotations to the pod spec patches in Kubernetes Deployment resource that will be generated for the Service . Here we use the patch section of the kustomize because we only want to add extra annotations to an existing resource. Adds a new complete ConfigMap resource. Here we use additions as we define the complete spec of the new resource. \ud83d\udcd8 Viewing Resources Generated by TrueFoundry You can view all the resources rendered by the Application in the Application Spec Tab and then selecting Applied K8s Manifest truefoundry.yaml Python type: service name: my-service image: type: image image_uri: nginx:latest ... ports: - port: 8000 ... kustomize: patch: patchesStrategicMerge: - | kind: Deployment apiVersion: apps/v1 metadata: name: my-service namespace: my-workspace-name # Note that this is workspace name. Not FQN. spec: template: metadata: annotations: prometheus.io/port: \"8000\" prometheus.io/scrape: \"true\" additions: - apiVersion: v1 data: test: data kind: ConfigMap metadata: name: configmap-1 # pip install PyYAML==6.0.1 from truefoundry.deploy import Service, Image, Port, Kustomize import yaml SERVICE_NAME = \"my-service\" WORKSPACE_NAME = \"my-workspace-name\" # Note that this is workspace name. Not FQN. SERVICE_PORT = 8000 # Add Prometheus annotation to the Deployment ADD_PROMETHEUS_ANNOTATIONS_TO_DEPLOYMENT = f\"\"\"\\ kind: Deployment apiVersion: apps/v1 metadata: name: {SERVICE_NAME} namespace: {WORKSPACE_NAME} spec: template: metadata: annotations: prometheus.io/port: \"{SERVICE_PORT}\" prometheus.io/scrape: \"true\" \"\"\" ADD_CONFIG_MAP = \"\"\"\\ apiVersion: v1 kind: ConfigMap metadata: name: configmap-1 data: test: data \"\"\" service = Service( name=SERVICE_NAME, image=Image(image_uri=\"nginx:latest\", command=...), ports=[Port(port=SERVICE_PORT, ...)], ..., kustomize=Kustomize( patch={ \"patchesStrategicMerge\": [ ADD_PROMETHEUS_ANNOTATIONS_TO_DEPLOYMENT, ] }, additions=[ yaml.safe_load(ADD_CONFIG_MAP), ] ), ) \ud83d\udcd8 Data Types kustomize.patch is an object . The most commonly used key under it is patchesStrategicMerge which is a list of strings . Each string member is a patch in YAML format kustomize.additions is a list of objects. Each object is a Kubernetes resource definition. You can configure the same using the UI by enabling the Advanced Fields and then enabling Kustomize Updated 4 months ago",
    "https://docs.truefoundry.com/docs/sticky-routing": "Sticky Routing Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Sticky Routing All Pages Start typing to search\u2026 Sticky Routing Pin user requests using consistent hash based routing Imagine a situation where a user is performing some expensive calculation in a request and wants to do so repeatedly over a bunch of API calls. Some of these expensive calculations might have some common computation that you might want to cache between requests. In such a case, you would like to route all the requests from a specific user to a specific replica. One such example is Prefix Caching with model servers like vLLM and SGLang where generating the next reply in a converstation can benefit from the cached computation of the conversation history so far and result in significantly lower latencies. User 1's requests are always routed to Pod 1 while User 2's requests are always routed to Pod 2 \ud83d\udea7 Pods can be transitory and sticky routing is best-effort basis It is important to remember that Kubernetes pods and nodes can be short lived and can be re-scheduled due to external events. It is not advised to put any critical stateful logic in Services. Considering this, sticky routing is best effort basis. In the event of a pinned pod getting deleted / evicted, all sessions pinned to that pod will be re-assigned randomly to other pods. Similarly, in the event of a scale up or scale down, the sessions might be re-assigned to evenly distribute the load. Enabling Sticky Routing To enable sticky routing, first in the Service labels section add a special label tfy_sticky_session_header_name and add a header name against it. For e.g. x-truefoundry-sticky-session-id Passing the header when making requests Now, your clients can send this header with a unique value for a \"session\". For e.g. this value can be a conversation id or user id, anything that identifies a unique session that can benefit from stickyness. For e.g. cURL Python OpenAI SDK curl -X POST https://my-service.example.com/v1/chat/completions \\ -H 'Content-Type: application/json' \\ -H 'x-truefoundry-sticky-session-id: session-id-qnfjk' \\ -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]}' import requests response = requests.post( \"https://my-service.example.com/v1/chat/completions\", headers={\"x-truefoundry-sticky-session-id\": \"session-id-qnfjk\"}, json={\"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}]} } response.raise_for_status() print(response.json()) from openai import OpenAI client = OpenAI( base_url=\"https://my-service.example.com/v1\", api_key=\"<YOUR API KEY>\", default_headers={\"x-truefoundry-sticky-session-id\": \"session-id-qnfjk\"}, ) After the first request, any request that has header x-truefoundry-sticky-session-id: session-id-qnfjk will be routed to the same pod the first request was routed to. \ud83d\udcd8 What happens if header is not added? The request will be routed according to default load balancing policy (random / round robin / least load) Updated 4 months ago",
    "https://docs.truefoundry.com/docs/setting-up-cicd-for-your-service": "Set up CI/CD Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Set up CI/CD All Pages Start typing to search\u2026 Set up CI/CD When you are rapidly iterating on an application, deploying it manually repeatedly can become tedious. Also, everybody working on the codebase then needs to know how to deploy. To streamline this, you can setup CI/CD pipeline which can automate application deployments on TrueFoundry based on specific triggers, such as image creation or commit merging to the main branch. The specific CI/CD configuration approach depends on the deployment scenario, you can configure the CI/CD options available by clicking on Setup CI/CD button for you application This will show the steps to setup CI/CD for your application. You can choose between different CI/CD modes according to your use case: Updated 2 months ago",
    "https://docs.truefoundry.com/docs/update-rollback-promote-your-service": "Update, Rollback, Promote Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Update, Rollback, Promote All Pages Start typing to search\u2026 Update, Rollback, Promote During the lifecycle of an application, we will usually encounter the scenarios of upgrading the version of the application, rolling back the active version or promoting the application from dev to staging and then to production. Truefoundry faciliates these flows and categorizes them into 3 workflows: Updating, Rolling back and Promoting. Updating : Modifying the existing code or configuration of a service to introduce new features, fix bugs, or enhance performance. Rolling Back : Reverting a service to a previous, known-good state in case of unexpected issues or performance degradation. Promoting : Moving a service from a lower environment (e.g., development or staging) to a higher environment (e.g., production) for being consumed by end users. Updating Services To update a service in TrueFoundry, simply deploy the service with the desired changes, maintaining the same service name and workspace. TrueFoundry will automatically detect the update and create a new version. Alternatively, you can update your Machine Learning Service using the Edit button to modify your Service's configuration, following the instructions below. This will also create and deploy a new version with your changes. Rolling Back Services TrueFoundry retains a comprehensive history of your service's deployed versions. You can easily redeploy a previous stable state, following the steps outlined in the demo below: This action will create a new version with the configuration of the redeployed version, effectively rolling back the service to its previous state. For e.g. if the current version is v10, and you rollback to v5, a new version v11 will be created with the same spec as v5. Promoting Services Usually, there will be multiple environments for an application for different stages of testing. While most companies maintain 3 environments: dev, staging, and production, your company might have fewer or more environments depending on the requirements. Independent of the number of environments, the problem statement of promoting an application from one environment to the other remains the same. Let's say we are promoting a service from staging to production. A few things to keep in mind are: The docker image URI of current production should be overwritten by the image URI currently running in staging. Settings like autoscaling and replicas in production shouldn't be overwritten with what is there in staging. Settings like readiness probe, and liveness probe should be changed with what is running actively in staging. We need to make sure environment variables are consistent among staging and production. If a new environment variable is added to staging, and we forget to add it to production while promoting, it can be disastrous. TrueFoundry promote flow takes these factors into account and shows the correct diff to the user while promoting to avoid manual errors. Let's say you've developed an initial version of a ML service for an ongoing project. This service has been deployed to your team's development workspace. After making some refinements, your development service is ready to move to production. The production environment can be another workspace on the same cluster or on a different cluster. To promote this to a production environment, make sure you have the production workspace ready and then follow the steps below: \ud83d\udcd8 Please remember to make modifications in the prod service after promoting Environment Variables : You might need to modify the environment variables to access different DBs, etc. Resource Scaling : To handle production traffic, you might need to change the number of replicas or configure autoscaling. The journey doesn't end once the service is deployed in production. You will constantly make changes to the service in development environment and then promote again when you want to upgrade production again. When you promote again, TrueFoundry will analyze the configuration differences between your enhanced development service and the existing production service. It will highlight the things that are going to be changed in the current production environment like docker image uri, environment variables, liveness/readiness probes and ignore the parts like autoscaling that are going to be retained in the production service. This helps avoid manual errors during the promote flow. By leveraging Promote Flow, you can confidently introduce enhancements to your production service without compromising its stability and security. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/monitor-your-service": "View logs, metrics and events Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account View logs, metrics and events All Pages Start typing to search\u2026 View logs, metrics and events TrueFoundry provides you with Logs, Metrics and Events to monitor your deployments to identify and debug issues. Browse Logs using UI Logs are records of events that occur in your deployment, such as requests to your services, errors that occur, and system messages. TrueFoundry provides you with logs at 2 levels: Deployment Level Logs : This will show the combined logs from all the pods in the deployment. Pod Level Logs: This allows you to view logs for an individual pod in the deployment. Metrics Dashboard Metrics dashboard provides a visual representation of the metrics collected from your application like CPU Usage, GPU Usage, Network Usage etc. This allows you quick and easy way to identify fluctuations and potential performance bottlenecks. TrueFoundry provides you with metrics dashboards at two levels: Deployment Level Metrics : This will show the combined metrics from all the pods in the deployment. Pod Level Metrics: This allows you to view metrics for an individual pod in the deployment. View Events List Events are the occurrences that happen in your TrueFoundry deployment. They can be triggered by a variety of things, a few examples of which are: Deploying a new service version Starting or stopping a pod A pod crashing Events can be used to track the progress of your deployment and to troubleshoot any problems that may occur. These are the standard Kubernetes events - you probably don't need to look at them often unless you are debugging your pod not starting. TrueFoundry provides you with events at 2 levels: Deployment Level Events : This will show the combined events from all the pods in the deployment. Pod Level Events: This allows you to view events for an individual pod in the deployment. Set up Monitoring through Grafana Grafana is a popular open-source dashboarding tool that can plot the different metrics in Prometheus. Since Truefoundry services metrics are present in Prometheus, they can be viewed in Grafana also. Exporting the metrics dashboard to Grafana will enable the following scenarios: Customizable dashboards : Grafana allows you to create custom dashboards that display the metrics that are most important to you. Alerting : You can set up alerts in Grafana to notify you when certain metrics reach specific thresholds. Integration with other tools : Grafana integrates with a variety of other tools, such as Prometheus and Elasticsearch, so you can consolidate your monitoring data into a single platform. To get the Grafana dashboard, you can use Export Metrics Dashboard to Grafana feature as described below. Prerequisites Before you can start exporting your Service's Metrics Dashboard to Grafana, you need to complete the following prerequisites: Install Grafana on your Cluster : Follow the instructions provided below to install Grafana on your cluster. Obtain Grafana Credentials : Once Grafana is installed, follow the instructions below and obtain the necessary credentials. These credentials will be used later in the process of accessing your Grafana Dashboard Exporting Metrics Dashboard To export the Metrics Dashboard to Grafana you can follow the instruction given below: Integrate with your own monitoring and logging solution You can integrate any of your existing monitoring and logging solutions like Datadog, Newrelic, Prometheus, Cloudwatch, Splunk, etc for the applications deployed on Truefoundry. Since Truefoundry applications run on a Kubernetes cluster deployed on your account, you can install the respective agents of the monitoring solutions on the cluster and get all the logs and metrics show up on your monitoring dashboard. Updated 2 months ago",
    "https://docs.truefoundry.com/docs/set-up-alerts": "Set up Alerts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Set up Alerts All Pages Start typing to search\u2026 Set up Alerts Truefoundry integrates with different monitoring and alerting solutions to enable setting alerts on the deployed services or jobs. This article describes the process of setting up alerts using Prometheus/Grafana, but you can set alerts using any other monitoring and alerting provider. To setup alerts on Grafana, you will first need to export your service's metrics dashboard to Grafana. Follow this guide for instructions. Setup Alerting for specific Metrics Before you can start setting up alerts for your metrics, you need to establish a destination for sending those alerts. In this case, we'll set up a Slack contact point so that when alerts are triggered, you receive notifications in your Slack channel. To ensure your service runs smoothly and avoid disruptions, setting up alerts is crucial. This involves defining specific metrics and thresholds that trigger notifications when certain conditions are met. A key alert you could create is a Success Rate Alert that notifies you if the success rate falls below 100%, indicating potential problems with your Service. Success Rate Alert Updated 2 months ago",
    "https://docs.truefoundry.com/docs/log-monitor-custom-metrics": "Log & Monitor Custom Metrics Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log & Monitor Custom Metrics All Pages Start typing to search\u2026 Log & Monitor Custom Metrics In this section, we will cover how to log your custom application metrics and then use TrueFoundry to monitor them. Your custom application metrics can be something like the number of times an API was called, the time taken for a function to execute, or something completely custom. TrueFoundry by default uses Prometheus to scrap all the metrics exposed at /metrics endpoint of your server. Logging metrics Let us understand with an example of a FastAPI inference service. You can find the complete code at Github Repository app.py import os import joblib import pandas as pd from fastapi import FastAPI # Loading the model from local model = joblib.load(\"iris_classifier.joblib\") app = FastAPI(docs_url=\"/\", root_path=os.getenv(\"TFY_SERVICE_ROOT_PATH\", \"/\")) @app.post(\"/predict\") def predict( sepal_length: float, sepal_width: float, petal_length: float, petal_width: float ): data = dict( sepal_length=sepal_length, sepal_width=sepal_width, petal_length=petal_length, petal_width=petal_width, ) prediction = int(model.predict(pd.DataFrame([data]))[0]) return {\"prediction\": prediction} This is a simple inference service exposing an /predict endpoint. Now, we would like to track how many times this API has been called and how much time it took for each of these predictions, in other words, what is the latency of this API? Let us start by installing the Python library for Prometheus Shell pip install prometheus-client Update the code to log metrics app.py import os import joblib import pandas as pd from fastapi import FastAPI + from prometheus_client import Counter, Histogram, make_asgi_app # Loading the model from local model = joblib.load(\"iris_classifier.joblib\") + # Define a Histogram metric to track latency of request as percentile. + REQUEST_TIME = Histogram(\"request_latency_seconds\", \"Time spent processing request\") + # Define a Counter metric to track number of requests. + REQUEST_COUNT = Counter(\"request_count\", \"Number of inference request\") app = FastAPI(docs_url=\"/\", root_path=os.getenv(\"TFY_SERVICE_ROOT_PATH\", \"/\")) @app.post(\"/predict\") def predict( sepal_length: float, sepal_width: float, petal_length: float, petal_width: float ): + # Increase the request count by 1 + REQUEST_COUNT.inc() + # Time the predict function, and observe the duration in seconds. + with REQUEST_TIME.time(): data = dict( sepal_length=sepal_length, sepal_width=sepal_width, petal_length=petal_length, petal_width=petal_width, ) prediction = int(model.predict(pd.DataFrame([data]))[0]) return {\"prediction\": prediction} + app.mount(\"/metrics\", make_asgi_app()) Run the FastAPI server with the following command uvicorn app:app --port 8000 --host 0.0.0.0 Now, you can check the exposed metrics at http://localhost:8000/metrics Congratulations! You have logged the metrics successfully. Now, go ahead and deploy this service. To learn more about different kinds of metrics details, please refer to prometheus-client documentation . Monitor exposed metrics Add the following kustomize patch to your service while deploying. This will add necessary annotations to your service Pods for Prometheus to scrape metrics. Please fill the placeholders with the correct service-name and service-port-number . deploy.py servicefoundry.yaml service = Service( ... kustomize = Kustomize( patch = { \"patchesStrategicMerge\": [ \"\"\"kind: Deployment apiVersion: apps/v1 metadata: name: <service-name> spec: template: metadata: annotations: prometheus.io/port: \"<service-port-number>\" prometheus.io/scrape: \"true\" \"\"\" ] } ), ... ) kustomize: patch: patchesStrategicMerge: - | kind: Deployment apiVersion: apps/v1 metadata: name: <service-name> spec: template: metadata: annotations: prometheus.io/port: \"<service-port-number>\" prometheus.io/scrape: \"true\" You can now export the application Grafana dashboard if not done already. Note: Please ensure that Prometheus and Grafana is installed in your cluster. Open your Grafana dashboard for the service. Now, to monitor your custom metrics add a new Visualization in your service dashboard. For counter metric, use the query request_count_total{container=~\"iris-inference\",namespace=~\"demo-ws\"} where container is service_name and namespace is workspace_name. For histogram metric, use multiple Query section with different percentile like round(histogram_quantile(0.99, sum(rate(request_latency_seconds_bucket{namespace=~\"demo-ws\", container=~\"iris-inference\"}[$__rate_interval])) by (le)), 0.001) represents p99 , similarly round(histogram_quantile(0.90, sum(rate(request_latency_seconds_bucket{namespace=~\"demo-ws\", container=~\"iris-inference\"}[$__rate_interval])) by (le)), 0.001) represents p90 where container is service_name and namespace is workspace_name Save the changes and you are ready with your dashboard. Updated 2 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-a-job": "Introduction to a Job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to a Job All Pages Start typing to search\u2026 Introduction to a Job TrueFoundry jobs enable you to run task-oriented workloads which are meant to run for a certain duration to complete a task, then terminate and release the resources. Here are some scenarios where Jobs are particularly well-suited: Model Training : Train machine learning models on large datasets, where the resource gets freed up once the training is complete. Maintenance and Cleanup : Schedule routine maintenance tasks, such as data backups, model retraining, report generation etc. Batch Inference : Perform large-scale batch inference tasks, such as processing large volumes of data using trained models, leveraging Job's ability to handle parallel workloads efficiently. Key considerations when building a Job We need to consider the following things while deploying jobs: Dockerize the code to be deployed. Schedule Job to specify when the Job should run. Define the resources requirements for your service - Define Resources - While the documentation is specific for services, its the same as what is required for jobs. Parameterize a job to enable ease of changing argument values. [Optional] Defining environment variables and secrets to be injected into the code - Environment Variables and Secrets . Set retries and timeout for your jobs in case the job gets stuck or fails for some reason. [Optional] Set Concurrency Limit to specify how many instances of a Job can run at once. [Optional] Mount files or volumes to your job - Mount File or Volumes . Access data from S3 or other clouds Update, Rollback, Promote your Job : While the documentation is specifically for Services, the Update, Rollback, and Promote process follows a similar flow for Jobs. Setting up CI/CD for your Job : While the documentation is specifically for Services, the CI/CD setup process follows a similar flow for Jobs. Running your First Job: To run your first job, choose one of the following guides based on the location of your job code: Code on github repository Code on local machine Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-job-from-a-public-github-repository": "Deploy and Run Job from a public GitHub repository Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy and Run Job from a public GitHub repository All Pages Start typing to search\u2026 Deploy and Run Job from a public GitHub repository In this guide, we'll deploy a Job to train a machine learning model. The model will learn to predict the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already prepared the training script that trains a model on the Iris dataset, and you can find the code in our GitHub Repository . Please visit the repository to familiarise yourself with the code you'll be deploying. Project Structure The project files are organised as follows: Text . \u251c\u2500\u2500 train.py - Contains the training script code. \u2514\u2500\u2500 requirements.txt - Contains the list of all dependencies. All these files are located in the same directory. Prerequisites Before you proceed with the guide, please ensure that you have setup a Workspace . To deploy your job, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Initiating Deployment via UI Use these configs for the deployment form: Repo URL : https://github.com/truefoundry/getting-started-examples Path to build context : ./train-model/ Command : python train.py \ud83d\udcd8 What we did above: In the example above, we only had Python code and a requirements.txt file. The source code is in a Github repo and we don't have a prewritten docker file or a DockerImage - so we chose the Python Code option. Once we submit the form, TrueFoundry templatised a Dockerfile from the details provided and build the Docker image for us. In the Build context field we specified the directory( ./train-model/ ) where our job code resides in the GitHub repository. We also specified the command( python train.py ) that we need to use to run our training script. View your deployed job After you click Submit , your job will be deployed in a few seconds. On successful deployment your Job will be displayed as Suspended (yellow) indicating that your Job has been deployed but it will not run automatically. Run your job To run your Job you will have to trigger it manually. Congratulations! You have successfully deployed and run your training Job. To learn how to interact with your job check out this guide: Interacting with your Job Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-job-using-python-sdk": "Deploy and Run Job using Python SDK Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy and Run Job using Python SDK All Pages Start typing to search\u2026 Deploy and Run Job using Python SDK In this guide, we'll deploy a Job to train a machine learning model. The model will learn to predict the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already prepared the training script that trains a model on the Iris dataset, and you can find the code in our GitHub Repository . Clone the GitHub repository with the following command: Shell git clone https://github.com/truefoundry/getting-started-examples.git Navigate to the project directory: Shell cd train-model Please review the job code to become familiar with the code you'll deploy. Project Structure The project files are organised as follows: Text . \u251c\u2500\u2500 train.py - Contains the training script code. \u2514\u2500\u2500 requirements.txt - Contains the list of all dependencies. All these files are located in the same directory. Prerequisites Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your job, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Deploying the Job Create a deploy.py file in the same directory as your Job code ( app.py ). This file will contain the necessary configuration for your Job. Your directory structure will then appear as follows: File Structure Text . \u251c\u2500\u2500 train.py \u251c\u2500\u2500 deploy.py \u2514\u2500\u2500 requirements.txt deploy.py deploy.py import argparse import logging from truefoundry.deploy import Build, Job, PythonBuild, Resources # Set up logging to display informational messages logging.basicConfig(level=logging.INFO) # Create a TrueFoundry **Job** object to configure your service job = Job( # Specify the name of the job name=\"your-job\", # Define how to build your code into a Docker image image=Build( # `PythonBuild` helps specify the details of your Python Code. # These details will be used to templatize a DockerFile to build your Docker Image build_spec=PythonBuild( # Define the command to run the training script command=\"python train.py\", # Specify the path to requirements file requirements_path=\"requirements.txt\", ) ), # Define the resource constraints. # # Requests are the minimum amount of resources that a container needs to run. # Limits are the maximum amount of resources that a container can use. # # If a container tries to use more resources than its limits, it will be throttled or killed. resources=Resources( # CPU is specified as a number. 1 CPU unit is equivalent to 1 physical CPU core, or 1 virtual core. cpu_request=0.2, cpu_limit=0.5, # Memory is defined as an integer and the unit is Megabytes. memory_request=200, memory_limit=500, # Ephemeral storage is defined as an integer and the unit is Megabytes. ephemeral_storage_request=1000, ephemeral_storage_limit=2000, ), # Define environment variables that your Job will have access to env={ \"ENVIRONMENT\": \"dev\" } ) # Deploy the job to the specified workspace, copy workspace FQN using the following guide # https://docs.truefoundry.com/docs/key-concepts#creating-a-workspace job.deploy(workspace_fqn=\"your-workspace-fqn\") To understand the code, you can click the following recipe: To deploy using Python SDK use the following command from the same directory containing the train.py and requirements.txt files. Shell python deploy.py Run the above command from the same directory containing the train.py and requirements.txt files. \ud83d\udcd8 Exclude files when building and deploying your source code: To exclude specific files from being built and deployed, create a .tfyignore file in the directory containing your deployment script ( deploy.py ). The .tfyignore file follows the same rules as the .gitignore file. If your repository already has a .gitignore file, you don't need to create a .tfyignore file. Service Foundry will automatically detect the files to ignore. Place the .tfyignore file in the project's root directory, alongside deploy.py. After running the command mentioned above, wait for the deployment process to complete. Monitor the status until it shows DEPLOY_SUCCESS: , indicating a successful deployment. Once deployed, you'll receive a dashboard access link in the output, typically mentioned as You can find the application on the dashboard: . Click that link to access the deployment dashboard. View your deployed job On successful deployment your Job will be displayed as Suspended (yellow) indicating that your Job has been deployed but it will not run automatically. Run your job To run your Job you will have to trigger it manually. Congratulations! You have successfully deployed and run your training Job. To learn how to interact with your job check out this guide: Interacting with your Job Updated 4 months ago",
    "https://docs.truefoundry.com/docs/interacting-with-your-job": "Interacting with your Job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Interacting with your Job All Pages Start typing to search\u2026 Interacting with your Job Job-Specific Dashboard To access the dedicated dashboard for your job, click its name. Running a Job When deploying a job you can either schedule it or set it to a manual trigger. If you deploy a job with manual trigger then it won't run automatically, you will have to trigger it. You can do this in one of two ways, either through the User Interface using the Run Job button, or Programmatically using the Python SDK. Trigger a Job through the User Interface (UI) Once you Run your job, Your job will start getting executed and enter the Running state. At this point, the platform will create and launch a pod associated with the Job Run to execute the job. Upon successful completion of your job, the Job Run will transition to the Finished status. Simultaneously, the pod that was created to execute the job will be automatically released, along with the resources they utilised. Trigger a Job Programmatically To trigger Job programmatically, you will need the Application FQN which is the Fully Qualified Name (FQN) of the job you want to run. You can obtain this information from the Job-specific dashboard of the Job you are interested in. Now you can trigger the Job programmatically using the Python SDK Python SDK from truefoundry.deploy import trigger_job # Trigger/Run a Job trigger_job( application_fqn=\"<application-fqn>\", command=\"<Optional Command to Trigger the Job With>\" # command to execute for job run # if job is parameterized, you can pass params instead of command # params={\"param1\": \"value1\"} ) Trigger a Job via API To trigger the job via API, you need to send the POST request at the endpoint <your-control-plane-url>/api/svc/v1/jobs/trigger where you need to pass the the applicationId in the request's body, you can get the application url from the UI, First go to job you want to trigger via API and then you can find the application Id in the URL of that page as shown in Image below. You can also pass the parameters or commands through the input object in the body, as shown below. JSON { \"applicationId\": \"<your-application-id>\", \"input\": { \"command\": \"<your-command>\", \"params\": {} } } Then you have to set the Authorization header whose value will be Bearer <your-api-or-service-account-key> below is the example curl request for the same. cURL curl -X 'POST' \\ 'https://example.truefoundry.cloud/api/svc/v1/jobs/trigger' \\ -H 'accept: */*' \\ -H 'Authorization: Bearer <api-key>' \\ -H 'Content-Type: application/json' \\ -d '{ \"applicationId\": \"string\", \"input\": { \"command\": \"string\", } }' Note : You cannot pass command and params both at same time, either of one can be passed at a time. Viewing your Job Run Viewing List of Job Runs Programmatically You can also view the list of job runs programatically using the following code snippet: Python from truefoundry.deploy import list_job_runs application_fqn = \"<your-application-fqn>\" job_runs = list_job_runs(application_fqn=application_fqn) print(\"Number of Job Runs:\", len(job_runs)) for run in job_runs: print(f\"Job name: {run.name}, status: {run.status}, command: {run.command}\\n\") Viewing a Specific Job Run Programmatically To get details of a Job Run (given the Job Run Name) you can use the following code snippet: Python from truefoundry.deploy import get_job_run application_fqn = \"<your-application-fqn>\" job_run_name = \"<your-job-run-name>\" job_run = get_job_run(application_fqn=application_fqn, job_run_name=job_run_name) print(f\"Job name: {job_run.name}, status: {job_run.status}, command: {job_run.command}\") Terminating an ongoing Job Run You can also terminate a job run from Python SDK using the following code snippet: Python from truefoundry.deploy import terminate_job_run terminated_job_run = terminate_job_run( application_fqn=\"<your-application-fqn>\", job_run_name=\"<your-job-run-name>\" ) print(terminated_job_run) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/monitor-your-job": "Monitor your Job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Monitor your Job All Pages Start typing to search\u2026 Monitor your Job TrueFoundry provides you with Logs, Metrics and Events to monitor your deployments to identify and debug issues. Browse Logs using UI Logs are records of events that occur in your deployment, such as training result logs of your job, errors that occur, and system messages. TrueFoundry provides you with logs at 2 levels: Deployment Level Logs : This will display aggregated logs from all job runs, including both ongoing and past executions Job Run / Pod Level Logs: This allows you to view logs for an individual Job Run and the pods brought to execute that job run. Metrics Dashboard Metrics dashboard provides a visual representation of the metrics collected from your job like CPU Usage, GPU Usage, Network Usage etc. This allows you quick and easy way to identify fluctuations and potential performance bottlenecks. TrueFoundry provides you with metrics dashboards at two levels: Deployment Level Metrics : This will display aggregated Metrics from all job runs, including both ongoing and past executions Job Run / Pod Level Metrics: This allows you to view Metrics for an individual Job Run and the pods brought to execute that job run. Events Events are the occurrences that happen in your TrueFoundry deployment. They can be triggered by a variety of things, a few examples of which are: Deploying a new job version Starting or stopping a pod A pod crashing Events can be used to track the progress of your deployment and to troubleshoot any problems that may occur. These are the standard Kubernetes events - you probably don't need to look at them often unless you are debugging your pod not starting. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-a-cron-job": "Configure Job Trigger Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Job Trigger All Pages Start typing to search\u2026 Configure Job Trigger In many cases, tasks need to be executed either on demand or at regular intervals. For instance, sending a welcome email to a new user upon registration requires immediate execution, while data scraping from a website every few hours demands a recurring schedule. TrueFoundry provides two trigger types for your Jobs: manual triggers and schedule triggers . These trigger types allow you to configure how and when your jobs should be executed. Manual Triggers Manual triggers are ideal for tasks that require on-demand execution. This trigger type allows users to manually initiate job runs programmatically or through the User Interface. This is suitable for tasks such as: Experimenting with different hyperparameters for a machine learning model. Sending a welcome email to new users immediately upon registration. Triggering a data cleaning process when new data is available. Setting up Manual Trigger By default, any job deployed on TrueFoundry is set up as a manual trigger job. You can initiate the job execution either manually through the user interface or programmatically using the Python SDK. To initiate job execution, either follow this guide for manual execution or refer to this guide for programmatic execution. Once you Run your job, Your job will start getting executed and enter the Running state. At this point, the platform will create and launch a pod associated with the Job Run to execute the job. Upon successful completion of your job, the Job Run will transition to the Finished status. Simultaneously, the pod that was created to execute the job will be automatically released, along with the resources they utilised. Schedule Triggers Schedule triggers are designed for tasks that demand recurring execution at specific intervals or on specific dates. This is suitable for tasks such as: Scrape data from a website every 3 hours to maintain an updated dataset. Retrain a machine learning model every month to improve its performance. Generate weekly reports every Monday at 9:30 AM. Setting up Schedule Trigger To set up a schedule trigger, you will need to specify the frequency and timing of job execution. TrueFoundry provides you with an option to configure this time interval using a cron string as described below. These types of jobs also called as cron jobs. \ud83d\udcd8 Note: A Cron Job can still be triggered manually using the UI or programmatically if required Specify the schedule for a cron job You need to use the cron string format to specify job schedule. The cron expression consists of five fields representing the time to execute a specified command. * * * * * | | | | | | | | | |___ day of week (0-6) (Sunday is 0) | | | |_____ month (1-12) | | |_______ day of month (1-31) | |_________ hour (0-23) |___________ minute (0-59) For example, 0 11 1 * * represents \"first day of every month at 11:00 AM\", or 30 9 * * 1 represents \"every Monday at 9:30 AM\" We can use a site like https://crontab.guru/ to tryout cron expression and get a human-readable description of the same. What if a job is still running until the next scheduled run? Say for example, you schedule a daily job which runs at midnight to process and store some data in an S3 bucket but for some reason it is still running even at next midnight. So it is possible that the previous run of the job hasn't been completed while it is already time for the job to run again as per schedule. What should happen in this case? Should we: Skip the new run and continue the current run Stop the ongoing run and start the new one, or Run both in parallel On TrueFoundry you can select the behaviour using something called concurrency policy based on your requirements and use case. The possible options are: Forbid : This is the default . Do not allow concurrent runs. Allow : Allow jobs to run concurrently. Replace : Replace the current job with the new one. Concurrency doesn't apply to manually triggered jobs. In that case, it always creates a new job run. Let's schedule your Job Again, we can do this using two different methods: Through the User Interface (UI) Using the Python SDK Scheduling your Job through the User Interface (UI) Scheduling your Job using the Python SDK deploy.py from truefoundry.deploy import Build, Job, PythonBuild, Schedule job = Job( image=Build( build_spec=PythonBuild( command=\"python train.py\", requirements_path=\"requirements.txt\" ) ), + trigger=Schedule( + schedule=\"0 8 1 * *\", + concurrency_policy=\"Forbid\" # Any one of [\"Forbid\", \"Allow\", \"Replace\"] + ) ) job.deploy(workspace_fqn=\"your-workspace-fqn) Once successfully deployed your cron job will run at the schedule you set for it. You can view the schedule of your job in the job-specific dashboard Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-a-job-with-additional-parameters": "Parameterize a job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Parameterize a job All Pages Start typing to search\u2026 Parameterize a job When you develop a Python script, there are often instances where you want to alter the script's behaviour without having to rewrite the code. This is where arguments come in handy. Arguments allow you to pass values to a script when it is executed, and these values can then be used to control the script's behaviour. For instance, let's consider a script that trains a Support Vector Machine (SVM) model. Arguments can be used to pass the values of the hyper-parameters to your script as command-line arguments. This enables you to easily modify the values of the hyper-parameters without having to modify your code. Here's an example of how to use argparse to pass hyper-parameters to a Python script that trains an SVM model: train.py import argparse parser = argparse.ArgumentParser() parser.add_argument(\"--kernel\", type=str, default=\"rbf\", help=\"The kernel type\") parser.add_argument(\"--C\", type=float, default=1.0, help=\"The regularization parameter\") args = parser.parse_args() kernel = args.kernel C = args.C # Train the model train_model(kernel, C) # Evaluate the model evaluate_model() You can now run this script by passing the following command: Terminal python train.py --kernel linear --C 2.0 This will train the SVM model with a linear kernel and a regularisation parameter of 2.0. You can easily set the kernel type and C parameter without having to change the code repeatedly. Passing arguments while running a Job If you've deployed a job that can accept arguments, you can pass those arguments in the command while running the Job. Entering values for multiple script arguments in the Run Job form can be time-consuming and error-prone. To simplify this process, you can configure parameters directly when creating your Job. This provides a convenient interface for entering parameter values, and the parameterized command is automatically generated for you. Additionally, this method allows you to document the arguments, making it easy for anyone running the job to understand the purpose and usage of each parameter. Configuring a Job with Params To set up Params, follow these steps: Setup the params in job form . Here, you'll provide details regarding the arguments you want to make params. You'll need to provide the name, description, and default value for each parameter. In the command you specify while configuring the Job, you need to specify a command template . It will be like the following: PowerShell python script_name --argument_1_name {{param_1_name}} --argument_2_name {{param_2_name}} For example, python train.py --C {{c_val}} --kernel {{kernel}}. Through User Interface Using the Python SDK In your Job deployment code deploy.py , include the following: deploy.py from truefoundry.deploy import Job, Build, PythonBuild, Resources, Param # First we define how to build our code into a Docker image image = Build( build_spec=PythonBuild( + command=\"python train.py --C {{c_val}} --kernel {{kernel}}\", requirements_path=\"requirements.txt\", ) ) job = Job( name=\"iris-train-args-job\", image=image, + params=[ + Param(name=\"c_val\", description=\"The regularization parameter's value\", default=1), + Param(name=\"kernel\", description=\"The kernel type\", default=\"rbf\"), + ], ) job.deploy(workspace_fqn=args.workspace_fqn) Running a Job with params Once you configure the job with params and deploy it, you will start seeing a form rendered on clicking run job. TrueFoundry automatically generates this form based on the parameters defined in the job's configuration. This dynamic form simplifies parameter handling, where you just need to change the values for the parameters and the command updates accordingly. Through the User Interface Programmatically You can also trigger the job with params programmatically using the Python SDK. Python from truefoundry.deploy import trigger_job trigger_job( application_fqn=\"tfy-ctl-euew1-devtest:tfy-demo:iris-train-job\", params={\"kernel\":\"linear\", \"c_val\":\"w\"} ) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/environment-variables-and-secrets-jobs": "Environment Variables and Secrets Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Environment Variables and Secrets All Pages Start typing to search\u2026 Environment Variables and Secrets Environment variables are key-value pairs stored outside of the code itself, accessible to the application during runtime. This approach effectively separates configuration from the code, offering several benefits: Secure Storage of Sensitive Data : Sensitive information, such as database credentials or API keys, is not embedded directly into the code, reducing the risk of exposure. Ease of Configuration Management : Configuration can be easily managed and adapted based on the environment (development, staging, production, etc.), without modifying the code. Example: Hardcoding Information vs. Using Environment Variables Consider a scenario where you're training a machine learning model and need to store the resulting model in Amazon S3. In a traditional approach, you might hardcode the S3 access credentials directly into the code: Python import boto3 import os # Hardcoded S3 credentials aws_access_key_id = \"your-aws-access-key-id\" aws_secret_access_key = \"your-aws-secret-access-key\" bucket_name = \"your-bucket-name\" # Train the machine learning model # ... # Create an S3 client using the hardcoded credentials s3_client = boto3.client('s3', aws_access_key_id, aws_secret_access_key) # Upload the trained model to S3 s3_client.upload_file('trained_model.pkl', bucket_name, 'trained_model.pkl') However, if we hardcode the values, it will be difficult to manage across different environments since the S3 access credentials and bucket-name will be different across dev, staging and prod. To solve this issue, you can utilize environment variables to store and access these values securely. Configure environment variables like YOUR-AWS-ACCESS-KEY-ID , YOUR-AWS-SECRET-ACCESS-KEY , and YOUR-BUCKET-NAME during deployment configuration, and retrieve them within your code using os.environ(ENV_VAR_NAME) Python import boto3 import os # Hardcoded S3 credentials aws_access_key_id = os.environ[\"YOUR-AWS-ACCESS-KEY-ID\"] aws_secret_access_key = os.environ[\"YOUR-AWS-SECRET-ACCESS-KEY\"] bucket_name = os.environ[\"YOUR-BUCKET-NAME\"] # Train the machine learning model # ... # Create an S3 client using the hardcoded credentials s3_client = boto3.client('s3', aws_access_key_id, aws_secret_access_key) # Upload the trained model to S3 s3_client.upload_file('trained_model.pkl', bucket_name, 'trained_model.pkl') By separating configuration data from the code, you can seamlessly switch between different environments (production, development, etc.) without altering the code itself. Simply modify the environment variables during deployment configuration, allowing you to maintain a consistent codebase across different environments. Secrets: Enhanced Protection for Highly Sensitive Data For sensitive keys like database passwords, api-keys, its not advisable to provide them directly as environment variables in the deployment configuration since everyone can see the deployment spec and get access to these sensitive passwords. Hence, we usually store such sensitive values in secret managers (AWS SSM, AWS Secret Manager, GCP Secret Manager, Azure Vault or Hashicorp Vault) and then only provide the key to the secret in the environment configuration. Translating the key to the actual value is done at a different step. Truefoundry provides an easy way to put the sensitive values in Secret Managers and then just provide the FQN of the secret (of the form tfy-secret://user:my-secret-group:my-secret ) in the deployment spec. You can read about how to create the secrets here . Truefoundry will automatically fetch the value and inject it into the environment at runtime. How to inject environment variables and secrets in TrueFoundry Via the User Interface (UI) Via the Python SDK Both Service and Job classes have an argument env where you can pass a dictionary. The dictionary keys will be assumed as environment variable names and the values will be the environment variable values. In case of Secrets enter the Secret FQN in the value deploy.py from truefoundry.deploy import Build, Job, DockerFileBuild, Port service = Job( name=\"my-job\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + env={ + \"MY_ENV_VAR_NAME\": \"MY_ENV_VAR_VALUE\", + \"MY_SECRET\": \"tfy-secret://user:my-secret-group:my-secret\", + }, ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/mounting-volumes-job": "Mounting Volumes Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Mounting Volumes All Pages Start typing to search\u2026 Mounting Volumes When we run model training jobs, the job will need to download the data from somewhere in order to be able to train the model. While doing this once is fine, but in case if we are planning to run multiple training jobs with different hyperparameters or other settings on the same set of data, it will be wasteful from a time and cost perspective for every job to download the same set of data again and again. This is where volumes can help. We can download the data once to a volume and then mount it to multiple jobs. They can then pickup the data from the volume as if it was downloaded on disk at the location where the volume is mounted. Mounting Volumes To use a persistent volume, we will first need to create one and then attach it to our deployments. You can learn how to create volumes using the Creating a Volume guide Attaching Volumes to a Deployment Here, we'll explore two different methods for attaching volumes to a deployment: Through the User Interface (UI) Using the Python SDK Attaching Volumes through the User Interface (UI) Attaching Volumes using the Python SDK The VolumeMount class is used to specify the details of the volume attachment. You need to provide the mount_path where you want to mount the volume within your Job and the volume_fqn which represents the FQN of the volume you want to attach. deploy.py from truefoundry.deploy import Build, Job, DockerFileBuild, VolumeMount service = Job( name=\"my-job\", image=Build(build_spec=DockerFileBuild()), + mounts=[ + VolumeMount( + mount_path=\"/data\", #or your desired path + volume_fqn=\"your-volume-fqn\" + ) + ] ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Using mounted volumes in your deployment Once you've attached a volume to your deployment, you can use it like any other directory within your job's workflow. For instance, if you've downloaded a dataset, named dataset.csv , to a volume mounted at /data , you can directly access it from within the Job as shown in the code below: Python import os import pandas as pd ... # Load the dataset from the mounted volume data = pd.read_csv(\"/data/dataset.csv\") # Train the model ... Updated 5 months ago",
    "https://docs.truefoundry.com/docs/retries-and-timeout": "Set Retries And Timeout Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Set Retries And Timeout All Pages Start typing to search\u2026 Set Retries And Timeout Retries Retries allow you to retry a job run if it fails due to temporary errors or conditions. This can be helpful in handling temporary disruptions and improving the overall reliability of your job executions. Consider a scenario where your job communicates with an intermittent service. If the service is unavailable or experiencing issues, retrying the job can increase the likelihood of successful communication. Another example might involve a job that processes data from an external source. If the source is temporarily unavailable due to maintenance or overload, retries can ensure that the job eventually retrieves the data and proceeds with processing. By specifying the retries parameter, you can define the maximum number of times the job will be retried before it is marked as failed. The default retry count is 1. Configuring Retries for Job Here, we'll explore two different methods of configure retries for your job: Through the User Interface (UI) Using the Python SDK Through the User Interface (UI) Using the Python SDK In your Job deployment code deploy.py , include the following: Diff from truefoundry.deploy import Build, Job, PythonBuild job = Job( name=\"iris-train-job\", image=Build( build_spec=PythonBuild( command=\"python train.py\", requirements_path=\"requirements.txt\", ) ), + retries=2, ) job.deploy(workspace_fqn=\"...\") Timeouts Timeouts establish a maximum duration for a job's execution, ensuring that jobs don't get stuck in an infinite loop or consume excessive resources. By specifying the timeout parameter, you can define the maximum time (in seconds) a job is allowed to run regardless of its retry attempts. For example, if your machine learning training code typically takes 10 minutes (600 seconds) to train, you can set the timeout to around 15-20 minutes or a more appropriate value. This ensures that if the job gets stuck in an infinite loop or takes an unreasonably long time due to some issue, the timeout will terminate it and you don't incur costs indefinitely. \ud83d\udcd8 Note: ' timeout will take precedence over the retries (retry limit), For instance, if you set the timeout to 300 seconds (5 minutes) and retries to 3, the job will terminate after 5 minutes regardless of how many times it attempted to run. Configuring Timeout for Job Here, we'll explore two different methods of configure timeout for your job: Through the User Interface (UI) Using the Python SDK Through the User Interface (UI) Using the Python SDK In your Job deployment code deploy.py , include the following: Diff from truefoundry.deploy import Build, Job, PythonBuild job = Job( name=\"iris-train-job\", image=Build( build_spec=PythonBuild( command=\"python train.py\", requirements_path=\"requirements.txt\", ) ), + timeout=86400, # in seconds ) job.deploy(workspace_fqn=\"...\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/concurrency-limits": "Set Concurrency Limit Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Set Concurrency Limit All Pages Start typing to search\u2026 Set Concurrency Limit TrueFoundry allows you to set a limit on how many runs of a Job can run concurrently (provided sufficient resources are available in the cluster). By default, this is set to None which allows an unlimited number of runs concurrently. You might want to limit the maximum number of concurrent runs of a job in case we don't want to bring the resources up at the same time - for e.g. if you only have quotas for 3 GPU machines, you can only run 3 jobs in parallel. So the concurrent limit can be set to 3 in that case. \ud83d\udcd8 Please note that if you have to run 100 jobs, triggering 100 job runs at the same time will cost you roughly the same as running 10 jobs in parallel and waiting for 10 such batches to get all the 100 job runs done. This will be true in most cases unless the docker image pull time is close to or greater than the job execution time. This can also be a requirement if only one run of a job should run at a single point - this can be true if you are reading / writing from a volume and only one source can be writing at a point to avoid corruption. We can set the concurrency limit to 1 in this case. If more instances of a job are triggered than the specified concurrency limit, the excess jobs are placed in a queue and executed sequentially once available slots become free. Configuring Concurrency Limit for Job Here, we'll explore two different methods of configuring concurrency limit for your job: Through the User Interface (UI) Using the Python SDK Through the User Interface (UI) Using the Python SDK In your Job deployment code deploy.py , include the following: Diff from truefoundry.deploy import Build, Job, PythonBuild job = Job( name=\"iris-train-job\", image=Build( build_spec=PythonBuild( command=\"python train.py\", requirements_path=\"requirements.txt\", ) ), + concurrency_limit=3, ) job.deploy(workspace_fqn=\"...\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/access-data-from-s3-or-other-clouds-jobs": "Access data from S3 or other clouds Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Access data from S3 or other clouds All Pages Start typing to search\u2026 Access data from S3 or other clouds In some instances, your Service may need to access data stored in S3 or other cloud storage platforms. To facilitate this access, you can employ one of two approaches: Credential-Based Access through environment variables This approach involves defining specific environment variables that contain the necessary credentials for accessing the cloud storage platform. For instance, to access S3, you would set environment variables for the AWS access key ID and secret access key, the environment variables being: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY IAM Role-Based Access through Service Account The second approach is to provide your Job with a Role with the necessary permission through Service Accounts. Service Accounts provide a streamlined approach to managing access without the need for tokens or complex authentication methods. This approach involves creating a Principal IAM Role within your cloud platform, granting it the necessary permissions for your project's requirements. Here are detailed guides for creating Principal IAM Roles in your respective cloud platforms and integrating them as Service Accounts within the workspace: AWS: Authenticate to AWS services using IAM service account GCP: Authenticate to GCP using IAM serviceaccount Once you've configured the Service Account within the workspace, you can simply toggle the Show Advanced Fields option at the bottom of the form. This will reveal an expanded set of options, from which you can select the desired Service Account using the provided dropdown menu. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/add-alerts-to-your-job": "Add Alerts to your Job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Add Alerts to your Job All Pages Start typing to search\u2026 Add Alerts to your Job Pre-Requisites Setup a Notification Channel Integration on the Integrations page. Please follow this document to add an integration. Currently two types of Notification Channels are supported: Slack (Webhook URL based): For this, please create a webhook for your slack and then add it as an integration in Slack Provider Account Slack (Bot Token based): For this, please create a bot token for your slack and then add it as an integration in Slack Provider Account (Slack Bot Integration). It requires chat:write and chat:write:public scope. You can add slack channels to send to respective slack channel. Email (SMTP credentials Integration): This can be found in Custom Provider Account. You can configure the SMTP Credentials of the mail server, from_email and to_emails and use it to send notifications. How to Configure From UI In the Job form, please click on \"Show Advanced Fields\" and find the \"Alerts\" section Select your notification target and the triggers (on_start, on_completion, on_failure). There are three types of targets, one is email and the others are slack webhook and slack bot, you need to add an email/slack webhook/slack bot integration respectively and select it in notification channel field to use that integration for sending notifications. Click on Submit. Once the Alerts are created you can see the alerts in your Slack/Email. Here is a sample Alert: From CLI To configure alerts when deploying from CLI or your CI/CD pipelines, Please add the following to your truefoundry.yaml You will need to copy the FQN of your Notification Channel Integration and paste it as shown below. truefoundry.yaml type: job name: <job-name> ... alerts: - on_start: false on_completion: false on_failure: true notification_target: type: email to_emails: - [email protected] notification_channel: <Paste your notification channel integration fqn here> From Python SDK To configure alerts when deploying from Python SDK, Please add the following to your deploy.py file: deploy.py from truefoundry.deploy import Job, JobAlert job = Job( ... alerts = [ JobAlert( on_start=False, on_completion=True, on_failure=True, notification_channel=\"<Paste your notification channel integration fqn here>\" ) ] ) Updated 8 days ago",
    "https://docs.truefoundry.com/docs/introduction-to-ml-repo": "Introduction to ML Repo Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to ML Repo All Pages Start typing to search\u2026 Introduction to ML Repo ML Repositories play a crucial role in managing, organizing, and tracking machine learning experiments and models. In this guide, we'll explore the key concepts and entities that make up ML Repositories within MLFoundry. Concepts The entities defined in MLFoundry can be understood from the diagram below. Concepts ML-Repo : An ML Repository is a collection of runs, models and artifacts which represents a high-level Machine Learning use-case. All access controls can be configured on the level of ml-repo. You can think of them as equivalent to Git repos - except for Machine learning artifacts. Runs : A run represents a single experiment which in the context of Machine Learning is one specific model (say Logistic Regression), with a fixed set of hyper-parameters. Metrics, and parameters (details below) are all logged under a specific run. Models : Model comprises of model file and some metadata. Each Model can have multiple versions. Artifacts : Artifact is a collection of files. Each Artifact can have multiple versions. With each run user can log metadata with the help of following : Parameters : Parameters or HyperParameters that define your experiment and Machine Learning model. For example, learning_rate, cache_size . Metric : Metrics are values that help you to evaluate and compare different runs. For example, accuracy, f1 score . Tags : Tags are labels for a run. A tag is represented by a string tag name and value. For example, env . Updated 5 months ago",
    "https://docs.truefoundry.com/docs/ml-repo-quickstart": "Quick Start Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Quick Start All Pages Start typing to search\u2026 Quick Start To get started, we need to have the mlfoundry library installed. You can install it following the instructions in the CLI Setup docs. Create a ML Repo \ud83d\udcd8 Prerequiste - Blob Storage Integration Before you can create an ML Repo, you'd need to connect one or more Blob Storages (S3, GCS, Azure Blob, MinIO, etc) to store artifacts and models associated with a ML Repo. If this one time setup is already done, you can skip to next section You can refer to one of the following pages to connect your blob storage to TrueFoundry AWS S3 Google Cloud Storage Azure Blob Storage Any S3 API Compatible Storage You can then create an ML Repo from the ML Repo's tab in the Platform. Create a run and start tracking Initialize a new run. This run will now appear on the TrueFoundry dashboard under ML Repos Tab. Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\", run_name=\"svm-model\") Track parameters Save hyperparameters for your experiment. Python run.log_params({\"learning_rate\": 0.001}) Track metrics Save metrics for your model. Python run.log_metrics({\"accuracy\": 0.7, \"loss\": 0.6}) End a run and stop tracking After completion of your experiment, you can end a run. This function marks the run as \u201cfinished\u201d and stops tracking system metrics. Python run.end() Congratulations! You have successfully created a new ML repository. It is now ready for use, and you can start populating it with your data, code, models, and other related resources. Login without manual interaction ( Non-Interactive Mode ) TrueFoundry offers a convenient option to automate login through the command-line interface (CLI) for integration within your scripts. This non-interactive approach utilizes environment variables, eliminating the need for manual input (approving through clicking the Approve button in the browser that opens) In non-interactive mode, you can set environment variables to automate the login process. To do this, set the following environment variables: TFY_API_KEY : Your TrueFoundry API key. You can obtain your API key by referring to the API Key Generation Documentation . TFY_HOST : The host URL of your MLFoundry instance, e.g., https://your-domain.truefoundry.com . Updated 5 months ago What\u2019s Next Monitor Jobs Deploy a Cron Job Advanced Options",
    "https://docs.truefoundry.com/docs/creating-a-ml-repo": "Creating a ML Repo Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a ML Repo All Pages Start typing to search\u2026 Creating a ML Repo Creating a ML Repo using TrueFoundry UI \ud83d\udcd8 Prerequiste - Blob Storage Integration Before you can create an ML Repo, you'd need to connect one or more Blob Storages (S3, GCS, Azure Blob, MinIO, etc) to store artifacts and models associated with a ML Repo. If this one time setup is already done, you can skip to next section You can refer to one of the following pages to connect your blob storage to TrueFoundry AWS S3 Google Cloud Storage Azure Blob Storage Any S3 API Compatible Storage You can then create an ML Repo from the ML Repo's tab in the Platform. Now you are all set to start Creating a run Log and Get Data Log and Get Models Log and Get Artifacts Updated 4 months ago",
    "https://docs.truefoundry.com/docs/create-run": "Creating a run Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a run All Pages Start typing to search\u2026 Creating a run Runs A run represents a single experiment which in the context of Machine Learning is one specific model (say Logistic Regression), with a fixed set of hyper-parameters. Metrics, and parameters (details below) are all logged under a specific run. Creating Runs in Truefoundry A run is an entity that represents a single experiment. Create a run at the beginning of your script or notebook to start capturing system metrics. \ud83d\udcd8 Creating a ML Repo If you don't have a ML Repo already, you'd need to create the ML Repo either from the Creating a ML Repo or with create_ml_repo function Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\", run_name=\"svm-model\") # Your code here. run.end() You can organize multiple runs under a single ml_repo. For example, the run svm-model will be created under the ml_repo iris-sklearn-example . You can view these runs in the TrueFoundry dashboard. TrueFoundry Dashboard Accessing Runs in TrueFoundry To interact with runs in Truefoundry, you can use the provided methods in the TrueFoundryClient class. Here are the different possibilities to access runs: Get a Run by ID To retrieve an existing run by its ID, use the get_run_by_id method: Python client = TrueFoundryClient() run = client.get_run_by_id(\"run_id_here\") Get a Run by Fully Qualified Name (FQN) If you have the fully qualified name (FQN) of a run, which follows the pattern tenant_name/ml_repo/run_name, you can use the get_run_by_fqn method: Python client = TrueFoundryClient() run = client.get_run_by_fqn(\"tenant_name/ml_repo/run_name\") Get All Runs for a Project To retrieve all the runs' names and IDs for a project, use the get_all_runs method: Python client = TrueFoundryClient() runs_df = client.get_all_runs(ml_repo=\"project_name_here\") Search Runs You can search for runs that match specific criteria using the search_runs method: Python client = TrueFoundryClient() runs = client.search_runs( ml_repo=\"project_name_here\", filter_string=\"metrics.accuracy > 0.75\", order_by=[\"metric.accuracy DESC\"], ) for run in runs: print(run) FAQs What is a ml_repo? A ml_repo embodies the high-level goal of the experiments, like \"predicting the sentiment of product reviews\". To reach the goal, you can experiment with different machine learning algorithms with different parameters. A single run represents a single experiment. TrueFoundry helps you organize these runs and find the best-performing ones under a ml_repo. How can I create a ml_repo? A ml_repo is automatically created when you call the create_run method. A ml_repo is identified by it's owner and name. Can anyone create a run under my ml_repo? No. TrueFoundry provides ml_repo-level authorization. If someone in your team wants to view or create a run under your ml_repo, you need to add them as a collaborator to your ml_repo. How can I create a run under a ml_repo owned by someone else? You can pass the ml_repo argument in the the create run. You should at least have WRITE permission for the ml_repo. If you don't have write access to the ml_repo, the admin needs to provide you atleast WRITE permission to the ml_repo. Python client.create_ml_repo(\"iris-demo\") run = client.create_run( ml_repo=\"iris-demo\", run_name=\"svm-model\", ) # Your code here. run.end() Can I use runs as a context manager? Yes, we can use runs as a context manager. A run will be automatically ended after the execution exits the with block. Python client.create_ml_repo(\"iris-demo\") run = client.create_run(ml_repo=\"iris-demo\", run_name=\"svm-model\") with run: # Your code here. ... # No need to call run.end() Are run names unique? Yes. run names under a ml_repo are unique. If a run name already exists, we add a suffix to make it unique. If you do not pass a run name while creating a run, we generate a random name. Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\") print(run.run_name) run.end() How runs are identified? Runs are identified by by their id . Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\") print(run.run_id) run.end() Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-params": "Log and Get Parameters Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log and Get Parameters All Pages Start typing to search\u2026 Log and Get Parameters Parameters or HyperParameters that define your experiment and Machine Learning model. For example, learning_rate , cache_size Logging the hyperparameters Hyperparameters are independent variables for a run used to control the learning process. We can capture the hyperparameters using the log_params method. Once set, the hyperparameters are immutable. If you need to change the hyperparameter, it basically means that you are changing your model and it's best to create a new run for that. This way, the system exactly tracks the model along with the exact configuration to train it. Note that parameter values are stringified before storing. Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\", run_name=\"svm-model\") run.log_params({\"cache_size\": 200.0, \"kernel\": \"linear\"}) run.end() Viewing logged parameter in dashboard These logged parameters can be seen in the MLFoundry dashboard. View Logged Parameters Accessing parameters for a run You can use the get_params method. It returns a dictionary Python from truefoundry.ml import get_client client = get_client() run = client.get_run(\"run-id-of-the-run\") print(run.get_params()) Filtering runs bases on parameter value To filters runs, click on top right corner of the screen to apply the required filter. Filter based on parameter value Capturing command-line arguments We can capture command-line arguments directly from the argparse.Namespace object. Python import argparse from truefoundry.ml import get_client parser = argparse.ArgumentParser() parser.add_argument(\"--batch_size\", type=int, required=True) args = parser.parse_args() client = get_client() run = client.create_run(ml_repo=\"iris-demo\") run.log_params(args) run.end() Can I change the param value once logged? No you cannot change the value of param once logged. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-metrics": "Log and Get Metrics Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log and Get Metrics All Pages Start typing to search\u2026 Log and Get Metrics Metrics are values that help you to evaluate and compare different runs. For example, accuracy , f1 score . Capturing metrics You can capture metrics using the log_metrics method. Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\") run.log_metrics(metric_dict={\"accuracy\": 0.7, \"loss\": 0.6}) run.end() These metrics can be seen in MLFoundry dashboard. Filters can be used on metrics values to filter out runs as shown in the figure. Filter runs on the basis of metrics These metrics can also be found in the overview section of run in the dashboard. Metrics Overview Accessing the metrics for a run You can use the get_metrics method. It returns a dictionary. Python from truefoundry.ml import get_client client = get_client() run = client.get_run(\"run-id-of-the-run\") metrics = run.get_metrics() for metric_name, metric_history in metrics.items(): print(f\"logged metrics for metric {metric_name}:\") for metric in metric_history: print(f\"value: {metric.value}\") print(f\"step: {metric.step}\") print(f\"timestamp_ms: {metric.timestamp}\") print(\"--\") run.end() Step-wise metric logging You can capture step-wise metrics too using the step argument. Python for global_step in range(1000): run.log_metrics(metric_dict={\"accuracy\": 0.7, \"loss\": 0.6}, step=global_step) The stepwise-metrics can be visualized as graphs in the dashboard. Step-wise metrics Should I use epoch or global step as a value for the step argument? If available you should use the global step as a value for the step argument. To capture epoch-level metric aggregates, you can use the following pattern. Python run.log_metrics( metric_dict={\"epoch/train_accuracy\": 0.7, \"epoch\": epoch}, step=global_step ) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-artifacts": "Log and Get Artifacts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log and Get Artifacts All Pages Start typing to search\u2026 Log and Get Artifacts Upload, Download and Version Artifacts An Artifact on Truefoundry is a bunch of files and folders stored on remote storage (cloud buckets like S3, GCS, Azure Blob) along with optional metadata fields. Additionally, artifacts are versioned automatically i.e. Each artifact once created has an associated version number and is immutable (contents cannot be changed). These artifacts can be retrieved later in any context and used in downstream applications. How artifacts are organized In an organization, you can create multiple ML Repo . You can log one or more artifacts in each of these ML Repo. Under an ML Repo, an artifact is primarily identified by its name. Every time an artifact is created with the same under the same ML Repo, a new version is automatically created for that artifact. The following diagram explains the hierarchy: How Artifact Versions are organized This hierarchy is also reflected in the Fully Qualified Name of any Artifact Version Artifact Version FQN parts \ud83d\udcd8 Artifact vs Artifact Version Keep in mind that when you log an artifact with some name, you get back an artifact version under the same name. Therefore, when we refer to an artifact, we are almost always referring to a specific version of that artifact instead of the artifact itself (which represents all versions under it). Relationship between Artifacts and Runs In the above diagram, we can also see some dotted lines between artifacts and runs under the ML Repo. This is because a logged artifact version can have an associated source run denoting where it came from. Such a relationship is established when an artifact version is created using a Truefoundry Run instead of direct logging. Logging an artifact via a run can be useful, say, when we are performing an ML experiment and we have hyperparameters, metrics, images, and plots along with some files and folders. We can create a run to log hyperparameters, metrics, images, and plots and use the same run to log files and folders as an artifact. This way we can trace back useful metadata from the created artifact version. On the other hand, you might have some dataset that might be used across multiple runs in the same ML Repo. In such a case, it might not make sense to associate logging of the dataset itself with some Run. Logging an Artifact You can log an artifact via two methods Python SDK User Interface Python SDK To log an artifact you need ml_repo - Name of the ML Repo name - Name for the artifact artifact_paths - A list of tuples of (<local file or directory path>, <remote file or directory path>) indicating which file or folder to copy where relative to the root of the artifact. Please see log_artifact for complete reference. Direct Via Client Via Truefoundry Run import os from truefoundry.ml import get_client, ArtifactPath client = get_client() os.makedirs(\"my-folder\", exist_ok=True) with open(\"my-folder/file-inside-folder.txt\", \"w\") as f: f.write(\"Hello!\") with open(\"just-a-file.txt\", \"w\") as f: f.write(\"Hello from file!\") artifact_version = client.log_artifact( ml_repo=\"my-ml-repo-1\", name=\"my-artifact\", artifact_paths=[ # Add files and folders here, `ArtifactPath` takes source and destination # source can be single file path or folder path # destination can be file path or folder path # Note: When source is a folder path, destination is always interpreted as folder path ArtifactPath(src=\"just-a-file.txt\"), ArtifactPath(src=\"my-folder/\", dest=\"cool-dir\"), ArtifactPath(src=\"just-a-file.txt\", dest=\"cool-dir/copied-file.txt\") ], description=\"This is a sample artifact\", metadata={\"created_by\": \"my-username\"} ) print(artifact_version.fqn) import os import truefoundry.ml as tfm from truefoundry.ml import ArtifactPath client = tfm.get_client() client.create_ml_repo(ml_repo=\"my-ml-repo-1\") os.makedirs(\"my-folder\", exist_ok=True) with open(\"my-folder/file-inside-folder.txt\", \"w\") as f: f.write(\"Hello!\") with open(\"just-a-file.txt\", \"w\") as f: f.write(\"Hello from file!\") run = client.create_run(ml_repo=\"my-ml-repo-1\", run_name=\"my-run-name\") artifact_version = run.log_artifact( name=\"my-artifact\", artifact_paths=[ # Add files and folders here, `ArtifactPath` takes source and destination # source can be single file path or folder path # destination can be file path or folder path # Note: When source is a folder path, destination is always interpreted as folder path ArtifactPath(src=\"just-a-file.txt\"), ArtifactPath(src=\"my-folder/\", dest=\"cool-dir\"), ArtifactPath(src=\"just-a-file.txt\", dest=\"cool-dir/copied-file.txt\") ], description=\"This is a sample artifact\", metadata={\"created_by\": \"my-username\"} ) print(artifact_version.fqn) run.end() This is what it looks like on the dashboard. You can find the Artifact Version under ML Repos > Your ML Repo Name > Artifacts > Your Artifact Name UI On the platform's dashboard, click on the \"ML Repos\" tab in the left panel. Select an ML Repo from the list or Create a new ML Repo and then select it. Go to the \"Other Artifacts\" tab at the top. Create a new Artifact by clicking on the \"Create Artifact\" Button. Configure the artifact by providing the following details Artifact Name: Give a descriptive name to your Artifact for easy identification. After filling in the details, click on the \"Create\" button to create the new Artifact. The new Artifact should now be listed among the other Artifacts in your ML Repo. Click on the Artifact Name The Artifact should be empty. On the Artifact page, locate and click the \"New Artifact Version\" button. In the modal that appears, Upload the necessary files or models that you want to include as part of this Artifact version. Optionally, you can also add any relevant Artifact metadata to provide additional information about this version. After filling in the details, click on the \"Create\" button Congratulations! You have successfully uploaded the new Artifact version, which includes the files and resources you added. Get the Artifact Version And Download Contents You can use the get_artifact_version_by_fqn method. It takes in fqn as an argument. FQN can be obtained from the UI dashboard or saved from the fqn property on ArtifactVersion returned by log_artifact \ud83d\udcd8 Access Permissions The user or service account accessing this artifact version should have at least Viewer permission on the ML Repo of the artifact version. Python from truefoundry.ml import get_client client = get_client() artifact_version = client.get_artifact_version_by_fqn(fqn=\"<your artifact fqn>\") # E.g. \"artifact:truefoundry/user/my-classification-project/sklearn-artifact:1\" # download the artifact contents to disk download_info = artifact_version.download(path=\"path/to/your/location\") print(download_info) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-models": "Log and Get Models Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log and Get Models All Pages Start typing to search\u2026 Log and Get Models Log Models and Download Models Model comprises of model file/folder and some metadata. Each Model can have multiple versions. In essence they are just Artifacts with special type model Log Model Version You can automatically save and version model files/folder using the log_model method. The basic usage looks like follows Python from truefoundry.ml import get_client client = get_client() model_version = client.log_model( ml_repo=\"name-of-the-ml-repo\", name=\"name-for-the-model\", model_file_or_folder=\"path/to/model/file/or/folder/on/disk\", framework=<None or Framework object (see below)> ) \ud83d\udcd8 Framework Agnostic Any file or folder can be saved as model by passing it in model_file_or_folder and framework can be set to None . This is an example of storing an sklearn model. To log a model we start a run and then give our model a name and pass in the model saved on disk and the framework instance. Sklearn Huggingface Transformers from truefoundry.ml import get_client, SklearnFramework, infer_signature import joblib import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) joblib.dump(clf, \"sklearn-pipeline.joblib\") client = get_client() client.create_ml_repo( # This is only required once name=\"my-classification-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. storage_integration_fqn='<storage_integration_fqn>' ) model_version = client.log_model( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", description=\"A simple sklearn pipeline\", model_file_or_folder=\"sklearn-pipeline.joblib\", framework=SklearnFramework(), metadata={\"accuracy\": 0.99, \"f1\": 0.80}, ) print(model_version.fqn) from truefoundry.ml import get_client, TransformersFramework import torch from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM pln = pipeline( \"text-generation\", model_file_or_folder=\"EleutherAI/pythia-70m\", tokenizer=\"EleutherAI/pythia-70m\", torch_dtype=torch.float16 ) pln.model.save_pretrained(\"my-transformers-model\") pln.tokenizer.save_pretrained(\"my-transformers-model\") client = get_client() model_version = client.log_model( ml_repo=\"my-llm-project\" name=\"my-transformers-model\", model_file_or_folder=\"my-transformers-model/\", framework=TransformersFramework(pipeline_tag=\"text-generation\"), ) print(model_version.fqn) This will create a new model my-sklearn-model under the ml_repo and the first version v1 for my-sklearn-model . Once created the model version files are immutable, only fields like description, framework, metadata can be updated using CLI or UI. Once created, a model version has a fqn (fully qualified name) which can be used to retrieve the model later - E.g. model:truefoundry/my-classification-project/my-sklearn-model:1 Any subsequent calls to log_model with the same name would create a new version of this model - v2 , v3 and so on. The logged model can be found in the dashboard in the Models tab under your ml_repo. You can view the details of each model version from there on. Get Model Version and Download You can first get the model using the fqn and then download the logged model using the fqn and then use the download() function. From here on you can access the files at download_info.download_dir Sklearn Huggingface Transformers import os import tempfile import joblib from truefoundry.ml import get_client client = get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-classification-project/my-sklearn-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load model = joblib.load( os.path.join(download_info.model_dir, \"sklearn-pipeline.joblib\") ) import torch from transformers import pipeline from truefoundry.ml import get_client client = get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-llm-project/my-transformers-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load pln = pipeline(\"text-generation\", model=download_info.model_dir, torch_dtype=torch.float16)s FAQs What are the frameworks supported by the log_model method? Following framework classes are available in truefoundry.ml FastAIFramework GluonFramework H2OFramework KerasFramework LightGBMFramework ONNXFramework PaddleFramework PyTorchFramework SklearnFramework SpaCyFramework StatsModelsFramework TensorFlowFramework TransformersFramework XGBoostFramework Update Model Version You may want to update fields like description, framework, metadata on an existing model version. You can do so with the .update() call on the Model Version instance. E.g. Python from truefoundry.ml import get_client, SklearnFramework client = get_client() model_version = client.get_model_version_by_fqn( \"model:truefoundry/my-classification-project/my-sklearn-model:1\" ) model_version.description = \"This is my updated description\" model_version.metadata = {\"accuracy\": 0.98, \"f1\": 0.85} model_version.framework = SklearnFramework( model_filepath=\"sklearn-pipeline.joblib\", serialization_format=\"joblib\" ) # Updates the model fields for existing model version. model_version.update() Updated 3 months ago",
    "https://docs.truefoundry.com/docs/log-and-get-data": "Log and Get Data Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log and Get Data All Pages Start typing to search\u2026 Log and Get Data A DataDirectory is a collection of files and folders that are on remote storage (cloud buckets like S3, GCS, Azure Blob). DataDirectories are useful for storing data that is associated with a particular ML Repository. These differ from Artifacts as these aren't versioned and aren't associated with any Runs. This makes them ideal for storing data that is not going to change or is constant among runs. For example, you can store your custom data with which you want to finetune an LLM here, without having to go through the process of creating a Run and storing the data in an Artifact. Creating a Data Directory Before we can start logging our data in a Data Directory, we need to create it: To create a Data Directory you need: ml_repo: Name of the ML Repo in which you want to create data_directory name: Name of the DataDirectory to be created. Python from truefoundry.ml import get_client client = get_client() data_directory = client.create_data_directory(name=\"<data_directory-name>\", ml_repo=\"<repo-name>\") print(data_directory.fqn) Logging data in a Data Directory To log data in Data Directory you need to get the data_directory instance, and then log the data in it using the .add_files method. For this, you will require fqn: Fully qualified name of the data directory. file_paths: A list of pairs of ( \\<source path> , \\<destination path ) to add files and folders to the DataDirectory contents. The first member of the pair should be a file or directory path and the second member should be the path inside the artifact contents to upload to. Python import os from truefoundry.ml import get_client, DataDirectoryPath with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") client = get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<data_directory-fqn>\") data_directory.add_files( file_paths=[DataDirectoryPath('artifact.txt', 'a/b/')] ) this would result in . \u2514\u2500\u2500 a/ \u2514\u2500\u2500 b/ \u2514\u2500\u2500 artifact.txt Downloading data from the Data Directory To download data in Data Directory you need to get the data_directory instance, and then download the data in it using the .download method. For this you will need the: download_path: Relative source path to the desired files. Python from truefoundry.ml import get_client client = get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<your-artifact-fqn>\") data_directory.download(download_path=\"<your-desired-download-path>\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/add-tags": "Add Tags Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Add Tags All Pages Start typing to search\u2026 Add Tags Tags are labels for a run. A tag is represented by a string tag name and value. Adding tags programmatically Python from truefoundry.ml import get_client client = get_client() run = client.create_run(ml_repo=\"iris-demo\", run_name=\"svm-model\") run.set_tags({\"env\": \"development\", \"task\": \"classification\"}) run.end() How can I programmatically fetch the tags for a run? You can use the get_tags method. It returns a dictionary. Python from truefoundry.ml import get_client client = get_client() run = client.get_run(\"run-id-of-the-run\") print(run.get_tags()) Adding tags with UI You can view the tags from the dashboard and also create new tags. Adding tags Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-image": "Log Images Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log Images All Pages Start typing to search\u2026 Log Images Logging images in different format Log images under the current run at the given step . Use the function log_images for a run . PIL package is needed to log images. To install the PIL package, run pip install pillow Here is the sample code to log images from different sources: Python from truefoundry.ml import get_client, Image import numpy as np import PIL.Image client = get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) imarray = np.random.randint(low=0, high=256, size=(100, 100, 3)) im = PIL.Image.fromarray(imarray.astype(\"uint8\")).convert(\"RGB\") im.save(\"result_image.jpeg\") images_to_log = { \"logged-image-array\": Image(data_or_path=imarray), \"logged-pil-image\": Image(data_or_path=im), \"logged-image-from-path\": Image(data_or_path=\"result_image.jpeg\"), } run.log_images(images_to_log, step=1) run.end() Visualizing the logged images You can visualize the logged images in the TrueFoundry dashboard. Logged Images You can also view the images logged step-wise by clicking in an image. Visualizing image at different steps Class mlfoundry.Image Images are represented and logged using this class in Truefoundry. You can initialize truefoundry.ml.Image by either by using a local path or you can use a numpy array / PIL.Image object. You can also log caption and the actual and prodicted values for an image as shown in the examples below. Logging images with caption and a class label Python from keras.datasets import mnist from truefoundry.ml import get_client, Image import time import numpy as np data = mnist.load_data() (X_train, y_train), (X_test, y_test) = data client = get_client() run = client.create_run(\"mnist-sample\") actuals = list(y_test) predictions = list(np.random.randint(9, size=10)) img_dict = {} for i in range(10): img_dict[str(i)] = Image( data_or_path=X_train[i], caption=\"mnist sample\", class_groups={ \"actuals\": str(actuals[i]), \"predictions\": str(predictions[i]) }, ) run.log_images(img_dict) The logged images can be visualized in the Truefoundry dashboard. MNIST sample images Logging image with multi-label classification problems Python images_to_log = { \"logged-image-array\": truefoundry.ml.Image( data_or_path=imarray, caption=\"testing image logging\", class_groups={\"actuals\": [\"dog\", \"human\"], \"predictions\": [\"cat\", \"human\"]}, ), } run.log_images(images_to_log, step=1) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/log-plots": "Log Plots Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Log Plots All Pages Start typing to search\u2026 Log Plots Mlfoundry allows you to log custom plots under the current run at the given step using the log_plot function. You can use this function to log custom matplotlib, plotly plots as shown in examples below: Python from truefoundry.ml import get_client from sklearn.metrics import ConfusionMatrixDisplay import matplotlib.pyplot as plt client = get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) ConfusionMatrixDisplay.from_predictions([\"spam\", \"ham\"], [\"ham\", \"ham\"]) run.log_plots({\"confusion_matrix\": plt}, step=1) You can visualize the logged plots in the Truefoundry Dashboard. Visualizing the logged plots Logging a seaborn plot Python from truefoundry.ml import get_client from matplotlib import pyplot as plt import seaborn as sns # create a run in mlfoundry client = get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) sns.set_theme(style=\"ticks\", palette=\"pastel\") # Load the example tips dataset tips = sns.load_dataset(\"tips\") # Draw a nested boxplot to show bills by day and time sns.boxplot(x=\"day\", y=\"total_bill\", hue=\"smoker\", palette=[\"m\", \"g\"], data=tips) sns.despine(offset=10, trim=True) run.log_plots({\"seaborn\": plt}) run.end() Logging a plotly figure Python from truefoundry.ml import get_client import plotly.express as px client = get_client() run = client.create_run(ml_repo=\"my-classification-project\") df = px.data.tips() fig = px.histogram( df, x=\"total_bill\", y=\"tip\", color=\"sex\", marginal=\"rug\", hover_data=df.columns, ) plots_to_log = { \"distribution-plot\": fig, } run.log_plots(plots_to_log, step=1) run.end() You can find this logged image in the dashboard. Plotly Image Updated 5 months ago",
    "https://docs.truefoundry.com/docs/add-artifacts-via-ui": "Add Artifacts via UI Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Add Artifacts via UI All Pages Start typing to search\u2026 Add Artifacts via UI Apart from the programmatic approach of creating a run and logging artifacts to it (as described in the documentation , you can also directly create an artifact through the user interface and add data to it. This eliminates the need to create a run as an intermediate step. This simplified method is particularly useful when you need to store data in the ML Repo for specific purposes, such as uploading your data for fine-tuning an LLM. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/artifacts-and-model-files-structure-migration-notice": "Artifact and Model Version Files Structure Migration Notice Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Artifact and Model Version Files Structure Migration Notice All Pages Start typing to search\u2026 Artifact and Model Version Files Structure Migration Notice \ud83d\udcd8 Note Outlined migration actions are required if you are using one of the following in your code. ModelVersion.download(...) ArtifactVersion.download(...) Models and Artifacts automatically downloaded using the Service - Artifacts Download feature will keep working without any extra actions. Existing Structure Up until truefoundry<0.5.0 , when you logged an artifact or model all the contents would be logged internally under a files/ folder for an artifact version and files/model/ for a model version E.g. Artifact Python from truefoundry.ml import get_client client = get_client() client.log_artifact( ml_repo=\"...\", name=\"...\", artifact_paths=[ (\"my-artifact-file-1.txt\", None), (\"my-artifact-file-2.txt\", None), (\"my-folder/\", None), ] ) Uploaded Structure: Uploaded structure . \u251c\u2500\u2500 .truefoundry/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 files/ \u251c\u2500\u2500 my-artifact-file-1.txt \u251c\u2500\u2500 my-artifact-file-2.txt \u251c\u2500\u2500 my-folder/ \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 ... Model from truefoundry.ml import get_client client = get_client() client.log_artifact( ml_repo=\"...\", name=\"...\", model_file_or_folder=\"./sample-model/\", ) Uploaded Structure: Uploaded structure . \u251c\u2500\u2500 .truefoundry/ \u2502 \u2514\u2500\u2500 metadata.json \u2514\u2500\u2500 files/ \u2514\u2500\u2500 model/ \u251c\u2500\u2500 model.safetensorts \u251c\u2500\u2500 config.json \u2514\u2500\u2500 ... This was to do some additional book keeping on our end. New Structure Starting truefoundry>=0.5.0 (and new UI), we have removed the files/ hierarchy and now all the files will be stored directly under the root of the artifact. That means for the same code above, after logging using the new sdk or UI you will see the following structure: Artifact Uploaded structure . \u251c\u2500\u2500 my-artifact-file-1.txt \u251c\u2500\u2500 my-artifact-file-2.txt \u2514\u2500\u2500 ... Model: Uploaded structure . \u251c\u2500\u2500 model.safetensorts \u251c\u2500\u2500 config.json \u2514\u2500\u2500 ... \u2757\ufe0f Upcoming Automated Migration The older structure is now considered deprecated. We will perform an automated migration and starting January 2025 we will require all log_artifact , log_model , ArtifactVersion.download and ModelVersion.download invocations to use truefoundry>=0.5.0 Please see the following section for required actions. Required Actions 1. Do not hardcode files/ or files/model/ structure in your code. \ud83d\udc4d You do not need to upgrade truefoundry version for this code change However, we recommend you upgrade as soon as possible If you are relying on appending files/ or files/model/ manually to download path Artifact Version Download \u274c Don't do this Python import os from truefoundry.ml import get_client client = get_client() artifact_version = client.get_artifact_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" artifact_version.download(path=download_path) download_path = os.path.join(download_path, \"files\") # equivalent to download_path = download_path + \"files/\" # Do something with download_path # e.g. load_files(download_path) \u2705 Instead use the return value of download() . It points to the actual contents Python import os from truefoundry.ml import get_client client = get_client() artifact_version = client.get_artifact_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" download_path = artifact_version.download(path=download_path) # Do something with download_path # e.g. load_files(download_path) In Summary Diff import os from truefoundry.ml import get_client client = get_client() artifact_version = client.get_artifact_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" - artifact_version.download(path=download_path) - download_path = os.path.join(download_path, \"files\") - # equivalent to download_path = download_path + \"files/\" + download_path = artifact_version.download(path=download_path) # Do something with download_path # e.g. load_files(download_path) Model Version Download \u274c Don't do this Python import os from truefoundry.ml import get_client client = get_client() model_version = client.get_model_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" model_version.download(path=download_path) download_path = os.path.join(download_path, \"files\", \"model\") # equivalent to download_path = download_path + \"files/model/\" # Do something with download_path # e.g. load_model(download_path) \u2705 Instead use the returned download_info from download() . It has a model_dir attribute that points to the actual contents Python import os from truefoundry.ml import get_client client = get_client() model_version = client.get_model_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" download_info = model_version.download(path=download_path) download_path = download_info.model_dir # Do something with download_path # e.g. load_model(download_path) In Summary Diff import os from truefoundry.ml import get_client client = get_client() model_version = client.get_model_version_by_fqn(\"...\") download_path = \"some/path/to/download/\" - model_version.download(path=download_path) - download_path = os.path.join(download_path, \"files\", \"model\") - # equivalent to download_path = download_path + \"files/model/\" + download_info = model_version.download(path=download_path) + download_path = download_info.model_dir # Do something with download_path # e.g. load_model(download_path) 2. Upgrade to truefoundry>=0.5.0 Once you have ensured you are not hardcoding files/ or files/model/ hierarchy in your code. You can safely upgrade truefoundry as it can handle both older and newer structures. No additional code change will be needed. Compatibility Matrix API truefoundry<0.5.0 truefoundry>=0.5.0 UI log_artifact Uploads in older nested files/ structure Uploads in new flat structure Uploads in new flat structure ArtifactVersion.download Can only handle older nested files/ structure Backwards Compatible , Can handle both structures - log_model Uploads in older nested files/model/ structure Uploads in new flat structure Uploads in new flat structure ModelVersion.download Can only handle older nested files/model/ structure Backwards Compatible , Can handle both structures - Updated 3 months ago",
    "https://docs.truefoundry.com/docs/accessing-and-adding-tag-for-artifact-and-model-versions": "Accessing and Adding version alias for Artifact and Model Versions Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Accessing and Adding version alias for Artifact and Model Versions All Pages Start typing to search\u2026 Accessing and Adding version alias for Artifact and Model Versions Now you can add a version alias for each artifact or model version, this alias will act as a unique identifier to access the model or artifact. In this guide, we will see how to add a version alias for the artifact or model, but before that, there are a few things to consider. The version alias must be unique across artifacts, so you cannot have the same version alias for two different artifacts or model versions within the same parent artifact or model. Alias will not replace the version but they will be accessible with the version, you can still access the artifact version by version number. Adding version alias via Python SDK You can add a version alias via Python SDK by following the way The version alias should start with 'v' followed by alphanumeric and it can include '.' and '-' in between (e.g. v1.0.0, v1-prod, v3-dev, etc) Python from truefoundry.ml import get_client client = get_client() artifact_version = client.log_artifact(ml_repo=\"\", name=\"\", artifact_paths=[]) artifact_version.version_alias = \"v0.0.1\" #It should start with 'v' artifact_version.update() Adding version alias via UI Follow this tutorial to add a version alias for an artifact version via UI Updated 3 months ago",
    "https://docs.truefoundry.com/docs/mlfoundry": "truefoundry.ml Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account truefoundry.ml All Pages Start typing to search\u2026 truefoundry.ml module TrueFoundry.ml Global Variables TRACKING_HOST_GLOBAL function get_client Initializes and returns the truefoundry client. Args: disable_analytics (bool, optional): To turn off usage analytics collection, pass True . By default, this is set to False . Returns: MLFoundry : Instance of MLFoundry class which represents a run . Examples: Get client Python import truefoundry.ml as tfm client = tfm.get_client() class MlFoundry MlFoundry. function create_data_directory Create DataDirectory to Upload the files Args: ml_repo (str): Name of the ML Repo in which you want to create data_directory name (str): Name of the DataDirectory to be created. description (str): Description of the Datset metadata (Dict <str> : Any): Metadata about the data_directory in Dictionary form. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.create_data_directory(name=\"<data_directory-name>\", ml_repo=\"<repo-name>\") print(data_directory.fqn) function create_ml_repo Creates an ML Repository. Args: name (str, optional): The name of the Repository you want to create. if not given, it creates a name by itself. storage_integration_fqn (str): The storage integration FQN to use for the experiment for saving artifacts. Examples: Create Repository Python import truefoundry.ml as tfm client = tfm.get_client() ml_repo = client.create_ml_repo(name=\"my-repo\", storage_integration_fqn=\"truefoundry:example:name:blob-storage:blob\") function create_run Initialize a run . In a machine learning experiment run represents a single experiment conducted under a project. Args: ml_repo (str): The name of the project under which the run will be created. ml_repo should only contain alphanumerics (a-z,A-Z,0-9) or hyphen (-). The user must have ADMIN or WRITE access to this project. run_name (Optional[str], optional): The name of the run. If not passed, a randomly generated name is assigned to the run. Under a project, all runs should have a unique name. If the passed run_name is already used under a project, the run_name will be de-duplicated by adding a suffix. run name should only contain alphanumerics (a-z,A-Z,0-9) or hyphen (-). tags (Optional[Dict[str, Any]], optional): Optional tags to attach with this run. Tags are key-value pairs. kwargs: Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: Create a run under current user. Python import truefoundry.ml as tfm client = tfm.get_client() tags = {\"model_type\": \"svm\"} run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\", tags=tags ) run.end() Creating a run using context manager. Python import truefoundry.ml as tfm client = tfm.get_client() with client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) as run: # ... # Model training code ... # `run` will be automatically marked as `FINISHED` or `FAILED`. Create a run in a project owned by a different user. Python import truefoundry.ml as tfm client = tfm.get_client() tags = {\"model_type\": \"svm\"} run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\", tags=tags, ) run.end() function get_all_runs Returns all the run name and id present under a project. The user must have READ access to the project. Args: ml_repo (str): Name of the project. Returns: pd.DataFrame : dataframe with two columns- run_id and run_name Examples: get all the runs from a ml_repo Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_all_runs(ml_repo='my-repo') function get_artifact_version Get the model version to download contents or load it in memory Args: ml_repo (str): ML Repo to which artifact is logged artifact_name (str): Artifact Name artifact_type (str): The type of artifact to fetch (acceptable values: \"artifact\", \"model\", \"plot\", \"image\") version (str | int): Artifact Version to fetch (default is the latest version) Returns: ArtifactVersion : An ArtifactVersion instance of the artifact Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_artifact_version(ml_repo=\"ml-repo-name\", name=\"artifact-name\", version=1) # load the model into memory clf = model_version.load() # download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info) function get_artifact_version_by_fqn Get the artifact version to download contents Args: fqn (str): Fully qualified name of the artifact version. Returns: ArtifactVersion : An ArtifactVersion instance of the artifact Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn( fqn=\"artifact:truefoundry/my-classification-project/sklearn-artifact:1\" ) # download the artifact to disk temp = tempfile.TemporaryDirectory() download_info = artifact_version.download(path=temp.name) print(download_info) function get_data_directory Get an existing data_directory by name . Args: ml_repo (str): name of an the project of which the data-directory is part of. name (str): the name of the data-directory Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory(ml_repo='my-repo', name=\"<data-directory-name>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_data_directory_by_fqn Get the DataDirectory by DataDirectory FQN Args: fqn (str): Fully qualified name of the artifact version. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<data-dir-fqn>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_data_directory_by_id Get the DataDirectory From the DataDirectory ID Args: id (uuid.UUID): Id of the data_directory. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_id(id=\"<data_directory-id>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_model_version Get the model version to download contents or load it in memory Args: ml_repo (str): ML Repo to which model is logged name (str): Model Name version (str | int): Model Version to fetch (default is the latest version) Returns: ModelVersion : The ModelVersion instance of the model. Examples: Sklearn Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import tempfile import joblib import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", version=1 ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir, download_info.model_filename) # Deserialize and Load model = joblib.load( os.path.join(download_info.model_dir, download_info.model_filename) ) Huggingface Transformers Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import torch from transformers import pipeline import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version( ml_repo=\"my-llm-project\", name=\"my-transformers-model\", version=1 ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load pln = pipeline(\"text-generation\", model=download_info.model_dir, torch_dtype=torch.float16) function get_model_version_by_fqn Get the model version to download contents or load it in memory Args: fqn (str): Fully qualified name of the model version. Returns: ModelVersion : The ModelVersion instance of the model. Examples: Sklearn Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import tempfile import joblib import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-classification-project/my-sklearn-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir, download_info.model_filename) # Deserialize and Load model = joblib.load( os.path.join(download_info.model_dir, download_info.model_filename) ) Huggingface Transformers Python # See `mlfoundry.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import torch from transformers import pipeline import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-llm-project/my-transformers-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load pln = pipeline(\"text-generation\", model=download_info.model_dir, torch_dtype=torch.float16) function get_run_by_fqn Get an existing run by fqn . fqn stands for Fully Qualified Name. A run fqn has the following pattern: tenant_name/ml_repo/run_name If a run svm under the project cat-classifier in truefoundry tenant, the fqn will be truefoundry/cat-classifier/svm . Args: run_fqn (str): fqn of an existing run. Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: get run by run fqn Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_fqn(run_fqn='truefoundry/my-repo/svm') function get_run_by_id Get an existing run by the run_id . Args: run_id (str): run_id or fqn of an existing run . Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: Get run by the run id Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_id(run_id='a8f6dafd70aa4baf9437a33c52d7ee90') function get_run_by_name Get an existing run by run_name . Args: ml_repo (str): name of the ml_repo of which the run is part of. run_name (str): the name of the run required Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: get run by name Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_name(run_name='svm', ml_repo='my-repo') function get_tracking_uri Get the current tracking URI. Returns: The tracking URI. Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() tracking_uri = client.get_tracking_uri() print(\"Current tracking uri: {}\".format(tracking_uri)) function list_artifact_versions Get all the version of na artifact to download contents or load them in memory Args: ml_repo (str): Repository in which the model is stored. name (str): Name of the artifact whose version is required artifact_type (ArtifactType): Type of artifact you want for example model, image, etc. Returns: Iterator[ArtifactVersion] : An iterator that yields non deleted artifact-versions of an artifact under a given ml_repo sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_versions = client.list_artifact_versions(ml_repo=\"my-repo\", name=\"artifact-name\") for artifact_version in artifact_versions: print(artifact_version) function list_artifact_versions_by_fqn List versions for a given artifact Args: artifact_fqn : FQN of the Artifact to list versions for. An artifact_fqn looks like {artifact_type}` : {org}/{user}/{project}/{artifact_name}` or {artifact_type}` : {user}/{project}/{artifact_name}` where artifact_type can be on of (\"model\", \"image\", \"plot\") Returns: Iterator[ArtifactVersion] : An iterator that yields non deleted artifact versions under the given artifact_fqn sorted reverse by the version number Yields: ArtifactVersion : An instance of mlfoundry.ArtifactVersion Examples: Python import truefoundry.ml as tfm tfm.login(tracking_uri=https://your.truefoundry.site.com\") client = tfm.get_client() artifact_fqn = \"artifact:org/my-project/my-artifact\" for av in client.list_artifact_versions(artifact_fqn=artifact_fqn): print(av.name, av.version, av.description) function list_data_directories Get the list of DataDirectory in a ml_repo Args: ml_repo (str): Name of the ML Repository max_results (int): Maximum number of Data Directory to list offset (int): Skip these number of instance of DataDirectory and then give the result from these number onwards Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directories = client.list_data_directories(ml_repo=\"<ml-repo-nam>\") for data_directory in data_directories: print(data_directory.name) function list_ml_repos Returns a list of names of ML Repos accessible by the current user. Returns: List[str] : A list of names of ML Repos function list_model_versions Get all the version of a model to download contents or load them in memory Args: ml_repo (str): Repository in which the model is stored. name (str): Name of the model whose version is required Returns: Iterator[ModelVersion] : An iterator that yields non deleted model versions of a model under a given ml_repo sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_versions = client.list_model_version(ml_repo=\"my-repo\", name=\"svm\") for model_version in model_versions: print(model_version) function list_model_versions_by_fqn List versions for a given model Args: model_fqn : FQN of the Model to list versions for. A model_fqn looks like model` : {org}/{user}/{project}/{artifact_name}` or model` : {user}/{project}/{artifact_name}` Returns: Iterator[ModelVersion] : An iterator that yields non deleted model versions under the given model_fqn sorted reverse by the version number Yields: ModelVersion : An instance of mlfoundry.ModelVersion Examples: Python import truefoundry.ml as tfm mlfoundry.login(tracking_uri=\"https://your.truefoundry.site.com\") client = tfm.get_client() model_fqn = \"model:org/my-project/my-model\" for mv in client.list_model_versions(model_fqn=model_fqn): print(mv.name, mv.version, mv.description) function log_artifact Logs an artifact for the current ml_repo . An artifact is a list of local files and directories. This function packs the mentioned files and directories in artifact_paths and uploads them to remote storage linked to the ml_repo Args: ml_repo (str): Name of the ML Repo to which an artifact is to be logged. name (str): Name of the Artifact. If an artifact with this name already exists under the current ml_repo, the logged artifact will be added as a new version under that name . If no artifact exist with the given name , the given artifact will be logged as version 1. artifact_paths (List[truefoundry.ml.ArtifactPath], optional): A list of pairs of (source path, destination path) to add files and folders to the artifact version contents. The first member of the pair should be a file or directory path and the second member should be the path inside the artifact contents to upload to. E.g. >>> client.log_artifact( ... ml_repo=\"sample-repo\", ... name=\"xyz\", ... artifact_paths=[ tfm.ArtifactPath(\"foo.txt\", \"foo/bar/foo.txt\"), tfm.ArtifactPath(\"tokenizer/\", \"foo/tokenizer/\"), tfm.ArtifactPath('bar.text'), ('bar.txt', ), ('foo.txt', 'a/foo.txt') ] ... ) would result in . \u2514\u2500\u2500 foo/ \u251c\u2500\u2500 bar/ \u2502 \u2514\u2500\u2500 foo.txt \u2514\u2500\u2500 tokenizer/ \u2514\u2500\u2500 # contents of tokenizer/ directory will be uploaded here description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None Returns: truefoundry.ml.ArtifactVersion : an instance of ArtifactVersion that can be used to download the files, or update attributes like description, metadata. Examples: Python import os import truefoundry.ml as tfm with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") client = tfm.get_client() ml_repo = \"sample-repo\" client.create_ml_repo(ml_repo=ml_repo) client.log_artifact( ml_repo=ml_repo, name=\"hello-world-file\", artifact_paths=[tfm.ArtifactPath('artifact.txt', 'a/b/')] ) function log_model Serialize and log a versioned model under the current ml_repo. Each logged model generates a new version associated with the given name and linked to the current run. Multiple versions of the model can be logged as separate versions under the same name . Args: ml_repo (str): Name of the ML Repo to which an artifact is to be logged. name (str): Name of the model. If a model with this name already exists under the current ML Repo, the logged model will be added as a new version under that name . If no models exist with the given name , the given model will be logged as version 1. model_file_or_folder (str): Path to either a single file or a folder containing model files. This folder is usually created using serialization methods of libraries or frameworks e.g. joblib.dump , model.save_pretrained(...) , torch.save(...) , model.save(...) framework (Union[enums.ModelFramework, str]): Model Framework. Ex:- pytorch, sklearn, tensorflow etc. The full list of supported frameworks can be found in mlfoundry.enums.ModelFramework . Can also be None when model is None . description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None Returns: truefoundry.ml.ModelVersion : an instance of ModelVersion that can be used to download the files, load the model, or update attributes like description, metadata, schema. Examples: Sklearn Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import joblib import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) joblib.dump(clf, \"sklearn-pipeline.joblib\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-classification-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) model_version = client.log_model( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", model_file_or_folder=\"sklearn-pipeline.joblib\", framework=ModelFramework.SKLEARN, metadata={\"accuracy\": 0.99, \"f1\": 0.80}, step=1, # step number, useful when using iterative algorithms like SGD ) print(model_version.fqn) Huggingface Transformers Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import torch from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM pln = pipeline( \"text-generation\", model_file_or_folder=\"EleutherAI/pythia-70m\", tokenizer=\"EleutherAI/pythia-70m\", torch_dtype=torch.float16 ) pln.model.save_pretrained(\"my-transformers-model\") pln.tokenizer.save_pretrained(\"my-transformers-model\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-llm-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) model_version = client.log_model( ml_repo=\"my-llm-project\", name=\"my-transformers-model\", model_file_or_folder=\"my-transformers-model/\", framework=ModelFramework.TRANSFORMERS ) print(model_version.fqn) function search_runs The user must have READ access to the project. Returns an iterator that returns a MLFoundryRun on each next call. All the runs under a project which matches the filter string and the run_view_type are returned. Args: ml_repo (str): Name of the project. filter_string (str, optional): Filter query string, defaults to searching all runs. Identifier required in the LHS of a search expression. Signifies an entity to compare against. An identifier has two parts separated by a period : the type of the entity and the name of the entity. The type of the entity is metrics, params, attributes, or tags. The entity name can contain alphanumeric characters and special characters. You can search using two run attributes : status and artifact_uri. Both attributes have string values. When a metric, parameter, or tag name contains a special character like hyphen, space, period, and so on, enclose the entity name in double quotes or backticks, params.\"model-type\" or params. model-type run_view_type (str, optional): one of the following values \"ACTIVE_ONLY\", \"DELETED_ONLY\", or \"ALL\" runs. order_by (List[str], optional): List of columns to order by (e.g., \"metrics.rmse\"). Currently supported values are metric.key, parameter.key, tag.key, attribute.key. The order_by column can contain an optional DESC or ASC value. The default is ASC . The default ordering is to sort by start_time DESC . job_run_name (str): Name of the job which are associated with the runs to get that runs max_results (int): max_results on the total numbers of run yielded through filter Returns: Iterator[MlFoundryRun] : MLFoundryRuns matching the search query. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() with client.create_run(ml_repo=\"my-project\", run_name=\"run-1\") as run1: run1.log_metrics(metric_dict={\"accuracy\": 0.74, \"loss\": 0.6}) run1.log_params({\"model\": \"LogisticRegression\", \"lambda\": \"0.001\"}) with client.create_run(ml_repo=\"my-project\", run_name=\"run-2\") as run2: run2.log_metrics(metric_dict={\"accuracy\": 0.8, \"loss\": 0.4}) run2.log_params({\"model\": \"SVM\"}) # Search for the subset of runs with logged accuracy metric greater than 0.75 filter_string = \"metrics.accuracy > 0.75\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 filter_string = \"metrics.accuracy > 0.7\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 and model=\"LogisticRegression\" filter_string = \"metrics.accuracy > 0.7 and params.model = 'LogisticRegression'\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 and # order by accuracy in Descending order filter_string = \"metrics.accuracy > 0.7\" order_by = [\"metric.accuracy DESC\"] runs = client.search_runs( ml_repo=\"my-project\", filter_string=filter_string, order_by=order_by ) filter_string = \"metrics.accuracy > 0.7\" runs = client.search_runs( ml_repo=\"transformers\", order_by=order_by ,job_run_name='job_run_name', filter_string=filter_string ) order_by = [\"metric.accuracy DESC\"] runs = client.search_runs( ml_repo=\"my-project\", filter_string=filter_string, order_by=order_by, max_results=10 ) Updated 4 months ago",
    "https://docs.truefoundry.com/docs/runs": "Runs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Runs All Pages Start typing to search\u2026 Runs module Runs class MlFoundryRun MlFoundryRun. property auto_end Tells whether automatic end for run is True or False property dashboard_link Get Mlfoundry dashboard link for a run property fqn Get fqn for the current run property ml_repo Get ml_repo name of which the current run is part of property run_id Get run_id for the current run property run_name Get run_name for the current run property status Get status for the current run function auto_log_metrics auto_log_metrics. Args: model_type (enums.ModelType): model_type data_slice (enums.DataSlice): data_slice predictions (Collection[Any]): predictions actuals (Optional[Collection[Any]]): actuals class_names (Optional[List[str]]): class_names prediction_probabilities: Returns: ComputedMetrics: function delete This function permanently delete the run Example: Python import truefoundry.ml as tfm client = tfm.get_client() client.create_ml_repo('iris-learning') run = client.create_run(ml_repo=\"iris-learning\", run_name=\"svm-model1\") run.log_params({\"learning_rate\": 0.001}) run.log_metrics({\"accuracy\": 0.7, \"loss\": 0.6}) run.delete() In case we try to call or acess any other function of that run after deleting then it will through MlfoundryException Example: Python import truefoundry.ml as tfm client = tfm.get_client() client.create_ml_repo('iris-learning') run = client.create_run(ml_repo=\"iris-learning\", run_name=\"svm-model1\") run.log_params({\"learning_rate\": 0.001}) run.log_metrics({\"accuracy\": 0.7, \"loss\": 0.6}) run.delete() run.log_params({\"learning_rate\": 0.001}) function end End a run . This function marks the run as FINISHED . Example: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) # ... # Model training code # ... run.end() In case the run was created using the context manager approach, We do not need to call this function. Python import truefoundry.ml as tfm client = tfm.get_client() with client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) as run: # ... # Model training code ... # `run` will be automatically marked as `FINISHED` or `FAILED`. function get_metrics Get metrics logged for the current run grouped by metric name. Args: metric_names (Optional[Iterable[str]], optional): A list of metric names For which the logged metrics will be fetched. If not passed, then all metrics logged under the run is returned. Returns: Dict[str, List[Metric]] : A dictionary containing metric name to list of metrics map. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) run.log_metrics(metric_dict={\"accuracy\": 0.7, \"loss\": 0.6}, step=0) run.log_metrics(metric_dict={\"accuracy\": 0.8, \"loss\": 0.4}, step=1) metrics = run.get_metrics() for metric_name, metric_history in metrics.items(): print(f\"logged metrics for metric {metric_name}:\") for metric in metric_history: print(f\"value: {metric.value}\") print(f\"step: {metric.step}\") print(f\"timestamp_ms: {metric.timestamp}\") print(\"--\") run.end() function get_params Get all the params logged for the current run . Returns: Dict[str, str] : A dictionary containing the parameters. The keys in the dictionary are parameter names and the values are corresponding parameter values. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.log_params({\"learning_rate\": 0.01, \"epochs\": 10}) print(run.get_params()) run.end() function get_tags Returns all the tags set for the current run . Returns: Dict[str, str] : A dictionary containing tags. The keys in the dictionary are tag names and the values are corresponding tag values. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.set_tags({\"nlp.framework\": \"Spark NLP\"}) print(run.get_tags()) run.end() function list_artifact_versions Get all the version of a artifact from a particular run to download contents or load them in memory Args: artifact_type : Type of the artifact you want Returns: Iterator[ArtifactVersion] : An iterator that yields non deleted artifact-versions of a artifact under a given run sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run(ml_repo=\"iris-learning\", run_name=\"svm-model1\") artifact_versions = run.list_artifact_versions() for artifact_version in artifact_versions: print(artifact_version) run.end() function list_model_versions Get all the version of a models from a particular run to download contents or load them in memory Returns: Iterator[ModelVersion] : An iterator that yields non deleted model-versions under a given run sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run(run_id=\"<your-run-id>\") model_versions = run.list_model_versions() for model_version in model_versions: print(model_version) run.end() function log_artifact Logs an artifact for the current ML Repo. An artifact is a list of local files and directories. This function packs the mentioned files and directories in artifact_paths and uploads them to remote storage linked to the experiment Args: name (str): Name of the Artifact. If an artifact with this name already exists under the current ML Repo, the logged artifact will be added as a new version under that name . If no artifact exist with the given name , the given artifact will be logged as version 1. artifact_paths (List[truefoundry.ml.ArtifactPath], optional): A list of pairs of (source path, destination path) to add files and folders to the artifact version contents. The first member of the pair should be a file or directory path and the second member should be the path inside the artifact contents to upload to. E.g. >>> run.log_artifact( ... name=\"xyz\", ... artifact_paths=[ tfm.ArtifactPath(\"foo.txt\", \"foo/bar/foo.txt\"), tfm.ArtifactPath(\"tokenizer/\", \"foo/tokenizer/\"), tfm.ArtifactPath('bar.text'), ('bar.txt', ), ('foo.txt', 'a/foo.txt') ] ... ) would result in . \u2514\u2500\u2500 foo/ \u251c\u2500\u2500 bar/ \u2502 \u2514\u2500\u2500 foo.txt \u2514\u2500\u2500 tokenizer/ \u2514\u2500\u2500 # contents of tokenizer/ directory will be uploaded here description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None step (int): step/iteration at which the vesion is being logged, defaults to 0. Returns: truefoundry.ml.ArtifactVersion : an instance of ArtifactVersion that can be used to download the files, or update attributes like description, metadata. Examples: Python import os import truefoundry.ml as tfm with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) run.log_artifact( name=\"hello-world-file\", artifact_paths=[tfm.ArtifactPath('artifact.txt', 'a/b/')] ) run.end() function log_images Log images under the current run at the given step . Use this function to log images for a run . PIL package is needed to log images. To install the PIL package, run pip install pillow . Args: images (Dict[str, \"truefoundry.ml.Image\"]): A map of string image key to instance of truefoundry.ml.Image class. The image key should only contain alphanumeric, hyphens(-) or underscores(_). For a single key and step pair, we can log only one image. step (int, optional): Training step/iteration for which the images should be logged. Default is 0 . Examples: Logging images from different sources Python import truefoundry.ml as tfm import numpy as np import PIL.Image client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) imarray = np.random.randint(low=0, high=256, size=(100, 100, 3)) im = PIL.Image.fromarray(imarray.astype(\"uint8\")).convert(\"RGB\") im.save(\"result_image.jpeg\") images_to_log = { \"logged-image-array\": tfm.Image(data_or_path=imarray), \"logged-pil-image\": tfm.Image(data_or_path=im), \"logged-image-from-path\": tfm.Image(data_or_path=\"result_image.jpeg\"), } run.log_images(images_to_log, step=1) run.end() function log_metrics Log metrics for the current run . A metric is defined by a metric name (such as \"training-loss\") and a floating point or integral value (such as 1.2 ). A metric is associated with a step which is the training iteration at which the metric was calculated. Args: metric_dict (Dict[str, Union[int, float]]): A metric name to metric value map. metric value should be either float or int . This should be a non-empty dictionary. step (int, optional): Training step/iteration at which the metrics present in metric_dict were calculated. If not passed, 0 is set as the step . Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.log_metrics(metric_dict={\"accuracy\": 0.7, \"loss\": 0.6}, step=0) run.log_metrics(metric_dict={\"accuracy\": 0.8, \"loss\": 0.4}, step=1) run.end() function log_model Serialize and log a versioned model under the current ML Repo. Each logged model generates a new version associated with the given name and linked to the current run. Multiple versions of the model can be logged as separate versions under the same name . Args: name (str): Name of the model. If a model with this name already exists under the current ML Repo, the logged model will be added as a new version under that name . If no models exist with the given name , the given model will be logged as version 1. model_file_or_folder (str): Path to either a single file or a folder containing model files. This folder is usually created using serialization methods of libraries or frameworks e.g. joblib.dump , model.save_pretrained(...) , torch.save(...) , model.save(...) framework (Union[enums.ModelFramework, str]): Model Framework. Ex:- pytorch, sklearn, tensorflow etc. The full list of supported frameworks can be found in truefoundry.ml.enums.ModelFramework . Can also be None when model is None . description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None step (int): step/iteration at which the model is being logged, defaults to 0. Returns: truefoundry.ml.ModelVersion : an instance of ModelVersion that can be used to download the files, load the model, or update attributes like description, metadata, schema. Examples: Sklearn Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import joblib import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) joblib.dump(clf, \"sklearn-pipeline.joblib\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-classification-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) run = client.create_run( ml_repo=\"my-classification-project\" ) model_version = run.log_model( name=\"my-sklearn-model\", model_file_or_folder=\"sklearn-pipeline.joblib\", framework=ModelFramework.SKLEARN, metadata={\"accuracy\": 0.99, \"f1\": 0.80}, step=1, # step number, useful when using iterative algorithms like SGD ) print(model_version.fqn) Huggingface Transformers Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import torch from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM pln = pipeline( \"text-generation\", model_file_or_folder=\"EleutherAI/pythia-70m\", tokenizer=\"EleutherAI/pythia-70m\", torch_dtype=torch.float16 ) pln.model.save_pretrained(\"my-transformers-model\") pln.tokenizer.save_pretrained(\"my-transformers-model\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-llm-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) run = client.create_run( ml_repo=\"my-llm-project\" ) model_version = run.log_model( name=\"my-transformers-model\", model_file_or_folder=\"my-transformers-model/\", framework=ModelFramework.TRANSFORMERS ) print(model_version.fqn) function log_params Logs parameters for the run. Parameters or Hyperparameters can be thought of as configurations for a run. For example, the type of kernel used in a SVM model is a parameter. A Parameter is defined by a name and a string value. Parameters are also immutable, we cannot overwrite parameter value for a parameter name. Args: param_dict (ParamsType): A parameter name to parameter value map. Parameter values are converted to str . flatten_params (bool): Flatten hierarchical dict, e.g. {'a': {'b': 'c'}} -> {'a.b': 'c'} . All the keys will be converted to str . Defaults to False Examples: Logging parameters using a dict . Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.log_params({\"learning_rate\": 0.01, \"epochs\": 10}) run.end() Logging parameters using argparse Namespace object Python import argparse import truefoundry.ml as tfm parser = argparse.ArgumentParser() parser.add_argument(\"-batch_size\", type=int, required=True) args = parser.parse_args() client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.log_params(args) function log_plots Log custom plots under the current run at the given step . Use this function to log custom matplotlib, plotly plots. Args: plots (Dict[str, \"matplotlib.pyplot\", \"matplotlib.figure.Figure\", \"plotly.graph_objects.Figure\", Plot][str, \"matplotlib.pyplot\", \"matplotlib.figure.Figure\", \"plotly.graph_objects.Figure\", Plot]): A map of string plot key to the plot or figure object. The plot key should only contain alphanumeric, hyphens(-) or underscores(_). For a single key and step pair, we can log only one image. step (int, optional): Training step/iteration for which the plots should be logged. Default is 0 . Examples: Logging a plotly figure Python import truefoundry.ml as tfm import plotly.express as px client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) df = px.data.tips() fig = px.histogram( df, x=\"total_bill\", y=\"tip\", color=\"sex\", marginal=\"rug\", hover_data=df.columns, ) plots_to_log = { \"distribution-plot\": fig, } run.log_plots(plots_to_log, step=1) run.end() Logging a matplotlib plt or figure Python import truefoundry.ml as tfm from matplotlib import pyplot as plt import numpy as np client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\", ) t = np.arange(0.0, 5.0, 0.01) s = np.cos(2 * np.pi * t) (line,) = plt.plot(t, s, lw=2) plt.annotate( \"local max\", xy=(2, 1), xytext=(3, 1.5), arrowprops=dict(facecolor=\"black\", shrink=0.05), ) plt.ylim(-2, 2) plots_to_log = {\"cos-plot\": plt, \"cos-plot-using-figure\": plt.gcf()} run.log_plots(plots_to_log, step=1) run.end() function set_tags Set tags for the current run . Tags are \"labels\" for a run. A tag is represented by a tag name and value. Args: tags (Dict[str, str]): A tag name to value map. Tag name cannot start with mlf. , mlf. prefix is reserved for truefoundry. Tag values will be converted to str . Examples: Python import truefoundry.ml as tfm client = tfm.get_client() run = client.create_run( ml_repo=\"my-classification-project\" ) run.set_tags({\"nlp.framework\": \"Spark NLP\"}) run.end() Updated 5 months ago",
    "https://docs.truefoundry.com/docs/artifact": "Artifact Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Artifact All Pages Start typing to search\u2026 Artifact module artifact.py class ArtifactPath ArtifactPath(src, dest) class ArtifactVersion property artifact_fqn Get fqn of the artifact property created_at Get the time at which artifact was created property created_by Get the information about who created the artifact property description Get description of the artifact property fqn Get fqn of the current artifact version property metadata Get metadata for the current artifact property name Get the name of the artifact property step Get the step in which artifact was created property updated_at Get the information about the when the artifact was updated property version Get version information of the artifact function delete Deletes the current instance of the ArtifactVersion hence deleting the current version. Returns: True if artifact was deleted successfully Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn(fqn=\"<your-artifact-fqn>\") artifact_version.delete() function download Download an artifact file or directory to a local directory if applicable, and return a local path for it. Args: path (str): Absolute path of the local filesystem destination directory to which to download the specified artifacts. This directory must already exist. If unspecified, the artifacts will either be downloaded to a new uniquely-named directory on the local filesystem or will be returned directly in the case of the Local ArtifactRepository. overwrite (bool): If True it will overwrite the file if it is already present in the download directory else it will throw an error Returns: path : Absolute path of the local filesystem location containing the desired artifacts. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn(fqn=\"<your-artifact-fqn>\") download_path = artifact_version.download(path=\"<your-desired-download-path>\") classmethod from_fqn Get the version of an Artifact to download contents or load them in memory Args: fqn (str): Fully qualified name of the artifact version. Returns: ArtifactVersion : An ArtifactVersion instance of the artifact Examples: Python import truefoundry.ml a tfm client = tfm.get_client() artifact_version = tfm.ArtifactVersion.from_fqn(fqn=\"<artifact-fqn>\") function raw_download Download an artifact file or directory to a local directory if applicable, and return a local path for it. Args: path (str): Absolute path of the local filesystem destination directory to which to download the specified artifacts. This directory must already exist. If unspecified, the artifacts will either be downloaded to a new uniquely-named directory on the local filesystem. overwrite (bool): If True it will overwrite the file if it is already present in the download directory else it will throw an error Returns: path : Absolute path of the local filesystem location containing the desired artifacts. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn(fqn=\"<your-artifact-fqn>\") artifact_version.raw_download(path=\"<your-desired-download-path>\") function update Updates the current instance of the ArtifactVersion hence updating the current version. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn(fqn=\"<your-artifact-fqn>\") artifact_version.description = 'This is the new description' artifact_version.update() class ArtifactVersionDownloadInfo class ArtifactVersionInternalMetadata Updated 5 months ago",
    "https://docs.truefoundry.com/docs/models": "Models Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Models All Pages Start typing to search\u2026 Models module model.py function calculate_model_size Tells about the size of the model Args: model_dir (str): directory in which model is present. Returns: total size of the model class ModelVersion property created_at Get the time at which model version was created property created_by Get the information about who created the model version property description Get description of the model property fqn Get fqn of the current model version property metadata Get metadata for the current model property metrics get the metrics for the current version of the model property model_fqn Get fqn of the model property model_schema get the model schema for current model property name Get the name of the model property step Get the step in which model was created property updated_at Get the information about when the model version was updated property version Get version information of the model function delete Deletes the current instance of the ModelVersion hence deleting the current version. Returns: True if model was deleted successfully Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn(fqn=\"<your-model-fqn>\") model_version.delete() function download Download a model file or directory to a local directory if applicable, and return a local path for it. Args: path (str): Absolute path of the local filesystem destination directory to download the specified models. This directory must already exist. If unspecified, the models will either be downloaded to a new uniquely-named directory on the local filesystem or returned directly in the case of the Local ModelRepository. overwrite (bool): If True it will overwrite the file if it is already present in the download directory else it will throw an error Returns: ModelVersionDownloadInfo : Download Info instance containing model_dir (path to downloaded model folder) and other metadata Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn(fqn=\"<your-model-fqn>\") download_info = model_version.download(path=\"<your-desired-download-path>\") print(download_info.model_dir) classmethod from_fqn Get the version of a model to download contents or load them in memory Args: fqn (str): Fully qualified name of the model version. Returns: ModelVersion : An ModelVersion instance of the Model Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_version = tfm.ModelVersion.from_fqn(fqn=\"<your-model-fqn>\") function raw_download Download a model file or directory to a local directory if applicable, and return a local path for it. Args: path (str): Absolute path of the local filesystem destination directory to download the specified models. This directory must already exist. If unspecified, the models will either be downloaded to a new uniquely-named directory on the local filesystem or returned directly in the case of the Local ModelRepository. overwrite (bool): If True it will overwrite the file if it is already present in the download directory else it will throw an error Returns: path : Absolute path of the local filesystem location containing the desired models. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn(fqn=\"<your-model-fqn>\") model_version.raw_download(path=\"<your-desired-download-path>\") function update Updates the current instance of the ModelVersion hence updating the current version. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn(fqn=\"<your-model-fqn>\") model_version.description = 'This is the new description' model_version.update() Updated 5 months ago",
    "https://docs.truefoundry.com/docs/dataset": "Data Directory Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Data Directory All Pages Start typing to search\u2026 Data Directory module dataset.py class DataDirectory property created_at Get the time at which DataDirectory was created property created_by Get the information about who created the DataDirectory property description Get description of the DataDirectory property fqn Get fqn of the DataDirectory property metadata Get metadata for the current DataDirectory property name Get the name of the DataDirectory property storage_root Get storage_root of the DataDirectory property updated_at Get the information about the when the DataDirectory was updated function add_files Logs File in the DataDirectory . Args: file_paths (List[truefoundry.ml.DataDirectoryPath], optional): A list of pairs of (source path, destination path) to add files and folders to the DataDirectory contents. The first member of the pair should be a file or directory path and the second member should be the path inside the artifact contents to upload to. E.g. >>> data_directory.add_files( ... file_paths=[ truefoundry.ml.DataDirectoryPath(\"foo.txt\", \"foo/bar/foo.txt\"), truefoundry.ml.DataDirectoryPath(\"tokenizer/\", \"foo/tokenizer/\"), truefoundry.ml.DataDirectoryPath('bar.text'), ('bar.txt', ), ('foo.txt', 'a/foo.txt') ] ... ) would result in . \u2514\u2500\u2500 foo/ \u251c\u2500\u2500 bar/ \u2502 \u2514\u2500\u2500 foo.txt \u2514\u2500\u2500 tokenizer/ \u2514\u2500\u2500 # contents of tokenizer/ directory will be uploaded here Examples: Python import os import truefoundry.ml as tfm with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<data_directory-fqn>\") data_directory.add_files( file_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) function download Download an file or directory to a local directory if applicable, and return a local path for it. Args: download_path : Relative source path to the desired files. path : Absolute path of the local filesystem destination directory to which to download the specified files. This directory must already exist. If unspecified, the files will either be downloaded to a new uniquely-named directory. overwrite : if to overwrite the files at/inside dst_path if they exist Returns: str : Absolute path of the local filesystem location containing the desired files or folder. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<your-artifact-fqn>\") data_directory.download(download_path=\"<your-desired-download-path>\") classmethod from_fqn Get the DataDirectory to download contents or load them in memory Args: fqn (str): Fully qualified name of the data directory. Returns: DataDirectory : An DataDirectory instance of the artifact Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = tfm.DataDirectory.from_fqn(fqn=\"<data_directory-fqn>\") function list_files List all the files and folders in the data_directory. Args: path : The path of directory in data_directory, from which the files are to be listed. Returns: str : Absolute path of the local filesystem location containing the desired files or folder. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<your-artifact-fqn>\") files = data_directory.list_files() for file in files: print(file.path) function update Updates the current instance of the DataDirectory. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<your-data_directory-fqn>\") data_directory.description = 'This is the new description' data_directory.update() class DataDirectoryPath DataDirectoryPath(src, dest) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-workflow": "Introduction to Workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to Workflow All Pages Start typing to search\u2026 Introduction to Workflow Powered by Flyte (An OpenSource Workflow Orchestrator) A TrueFoundry workflow is a structured sequence of tasks that enables efficient management and execution of complex computational processes, especially in data processing and machine learning. It offers a scalable, maintainable, and reusable approach to orchestrating and automating tasks. Key features of TrueFoundry workflows: Task Orchestration: Seamlessly manage and execute tasks in a defined order using Directed Acyclic Graphs (DAGs) . End-to-End Automation: Streamline the entire lifecycle of data and machine learning workflows, from data ingestion to model deployment. Scalability and Reusability: Effortlessly scale workflows to handle large datasets or complex tasks. Architecture Truefoundry Workflows are based on an OpenSource Workflow Orchestrator Flyte We ship Flyte's Control Plane Components with Truefoundry's Control Plane (No additional Setup Needed). Each Kubernetes cluster where you need to run workflows needs the following setup to be done. (Installation of Flyte's Data Plane Components). Follow this document for the same. Key Consideration while building a workflow. Install the truefoundry workflow and set up the CLI, refer to this guide for setting up the CLI. You need to define the workflow in a python file like this . Functions representing tasks and workflows need to decorated with @workflow and @task decorators. Define task and task config to configure the usage of task in workflow, check the different types of task config that are available for usage in workflow. To know about how to interact with the workflow, refer to Interacting with workflow . To run a raw container as a task, refer to the Using raw container task guide . To run workflow at a fix interval of time, create a cron workflow, to learn more about cron workflow, refer this . To speed up the execution of heavy task, you can use map task to create sub task and run simultaneously, refer to this guide to see how you can define map task. To have a conditional execution of task or to select task based on conditions, use conditional task. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/creating-your-first-workflow": "Creating your first workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating your first workflow All Pages Start typing to search\u2026 Creating your first workflow In this guide we will create a simple workflow to return the sum of the square of all the elements in the list. following are the tasks we will be creating in the workflow Generate a list of numbers. Calculate the square of each number in the list. Sum the squared numbers. Prerequisites Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your workflow, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Creating the Workflow Create a workflow.py where we will write the code for our workflow and place in the project root folder. (With other dependent files and requirements.txt) workflow.py workflow.py from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskPythonBuild from truefoundry.deploy import Resources cpu_task_config = PythonTaskConfig( # `TaskPythonBuild` helps specify the details of your Python Code. # These details will be used to templatize a DockerFile to build your Docker Image image=TaskPythonBuild( python_version=\"3.9\", # Pip packages to install.`truefoundry[workflow]` is a mandatory dependency pip_packages=[\"truefoundry[workflow]==0.4.8\"], # Optionally pass the path to requirements.txt # requirements_path=\"requirements.txt\" ), resources=Resources(cpu_request=0.5) ) # Task 1: Generate a list of numbers @task(task_config=cpu_task_config) def generate_numbers(n: int) -> list[int]: return list(range(1, n + 1)) # Task 2: Calculate the square of each number in the list @task(task_config=cpu_task_config) def square_numbers(numbers: list[int]) -> list[int]: return [x**2 for x in numbers] # Task 3: Sum the squared numbers @task(task_config=cpu_task_config) def sum_squares(squared_numbers: list[int]) -> int: return sum(squared_numbers) # Workflow: Orchestrate the tasks @workflow def simple_pipeline(n: int) -> int: numbers = generate_numbers(n=n) squared_numbers = square_numbers(numbers=numbers) result = sum_squares(squared_numbers=squared_numbers) return result # Runs the pipeline locally if __name__ == \"__main__\": result = simple_pipeline(n=5) print(f\"The sum of squares from 1 to 5 is: {result}\") \u26a0\ufe0f Your workflow function should not contain any code or business logic, only defined tasks can be called in the workflow function and nothing else, else the workflow execution might fail. You can also have map tasks and conditional tasks in your workflow. You can also run raw containers as tasks. Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name my-workflow-name \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" View your deployed workflow You can view the deployed workflow by going to the workflow tab on dashboard. Updated about 1 month ago",
    "https://docs.truefoundry.com/docs/interacting-with-workflow": "Interacting with workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Interacting with workflow All Pages Start typing to search\u2026 Interacting with workflow Triggering the workflow Workflows can be triggered from UI, CLI and also via python SDK Triggering from UI Triggering via cli Make sure the truefoundry[Workflow] is installed and is set up or you can refer to this guide to set up the cli. Then copy the application fqn from the UI of the workflow Then deploy using the following command Shell tfy trigger workflow <application-fqn> if workflow accepts the input parameters you can pass them as arguments Shell tfy trigger workflow <application-fqn> --input1 <input1> --input2 <input2> Triggering via Python SDK trigger.py from truefoundry.deploy import trigger_workflow trigger_workflow(application_fqn=\"<application-fqn>\", inputs={\"input-key\": \"input-value\"}) Save the above file and run the following command Shell python trigger.py Checking workflow graph You can check the workflow graph by clicking on any of the workflow on workfow page and then clicking on graph page as shown in above tutorial The workflow execution graph shows the static graph of the workflow where each rectangle represents the tasks and the edges represent the execution order of each tasks, where a edge from one node to another node represent the connection from one task to another task. Checking live execution graph of a workflow Graph of the workflow can be viewed on the execution page of that particular workflow. The rectangles represent the task and the edges represent the connection between nodes, so the edge from one task to another tells about the next task or step in the workflow after the current task is executed successfully. The yellow color around the task means that the task is running and if it turns red that means it is in a failed state and workflow has failed and if it is green in color then it is executed successfully. The dotted rectangle represents that the task is a conditional task and the highlighted border suggests which tasks were executed based on the condition defined in the workflow. If there are multiple nested conditional tasks then on clicking the task that was executed, then clicking on the conditional task will open another nested condition and now there will be two dotted rectangles around tasks as seen in above tutorial and this will keep on increasing until the task are executed successfully and there are no nested conditions left. Checking Execution Logs, Metrics and Events You can check the logs, metrics, and events for each task individually by clicking on that particular task or node. For conditional task, you can check the metrics and logs of the task that were executed in the workflow. For map task, when you click on map task, a new page opens where you can see all the subtasks and each sub-task has its own logs metrics and events. Terminating the workflow Updated 5 months ago",
    "https://docs.truefoundry.com/docs/workflow-concepts": "Workflow Concepts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Workflow Concepts All Pages Start typing to search\u2026 Workflow Concepts Workflow Definition : A workflow is a composition of tasks, arranged in a specific order, that defines the execution logic. Workflows can include conditional logic, loops, and parallel execution. Right now only static workflows are supported. There can be only one Workflow in an application. Workflow can be defined by wrapping a function with the @workflow decorator. Python from truefoundry.workflow import task, workflow @task(...) def say_hello() -> str: return \"Hello, World!\" @workflow def hello_world_wf() -> str: res = say_hello() return res Task Definition : Task is the smallest unit of execution. It represent a single operation or function, such as a data transformation, model training step, or data retrieval, etc. Types of Tasks: Python Tasks: :Regular Python functions decorated with @task and are called in the workflow function. Container Tasks : A Container Task is a task that runs a Docker container as part of the workflow execution. This allows you to encapsulate any piece of logic or computation within a Docker container and execute it within the workflow. Each task takes a task_config as a parameter to learn more about task config click here . Python from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild, ContainerTaskConfig, ContainerTask from truefoundry.deploy import Resources, Image #Python task config task_config = PythonTaskConfig( ... ) @task(task_config=task_config) def python_task() -> str: ... #Container task with task config container_task = ContainerTask( name=\"container_task\", task_config=ContainerTaskConfig(...), ) Map Task A map task is a specialized task that enables the parallel execution of a single task across multiple inputs, this task is used to speed up the execution and it is used when the input is too large or the function is a heavy task and requires a huge amount of resources and time. To learn more about Map task and its usage, refer to Creating a Map Task guide. Conditional Task A conditional task allows you to decide the execution path of the workflow based on the condition defined. It his used when you have different execution paths that are dependent on the input or output of the previous task. To learn more about Conditional task and its usage, refer to Creating a Conditional Task guide. Cron Workflow A cron workflow is a type of workflow that is scheduled to run at specific intervals, similar to how cron jobs work in Unix-like systems. It allows you to automate the execution of workflows based on a predefined schedule, which can be expressed using cron syntax. Cron jobs are always scheduled in UTC timezone. Each workflow takes an optional execution config as parameter in which you can define the schedule or interval at which you want to run the workflow. The cron Workflow can be scheduled be passing the execution_configs in the workflow decorator Python from truefoundry.workflow import workflow, ExecutionConfig @workflow( execution_configs=[ ExecutionConfig( schedule=\"*/10 * * * *\", ) ] ) def your_workflow(): ... Updated 5 months ago",
    "https://docs.truefoundry.com/docs/creating-a-workflow-with-gpu-and-non-gpu-image": "Creating a workflow with different container images Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a workflow with different container images All Pages Start typing to search\u2026 Creating a workflow with different container images This guide helps you to create a workflow with tasks that have different task configs and different container applications. This can have variety of applications like keeping different configuration (resources and container image) for steps requiring GPUs and steps not requiring GPUs In this example, We will write a simple workflow to get the latest price of Bitcoin, Ethereum, and Litecoin, the main aim of this example is to understand the different types of configs. we will create two configs, one for GPU and one for CPU like this. Prerequsite Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your workflow, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Creating the workflow Create a workflow.py where we will write the code for our workflow and place in the project root folder. (With other dependent files and requirements.txt) workflow.py workflow.py from truefoundry.workflow import ( task, workflow, PythonTaskConfig, TaskPythonBuild, ) from truefoundry.deploy import Resources, NvidiaGPU import requests import json cpu_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.8\"], # requirements_path=\"requirements.txt\" ), resources=Resources(cpu_request=0.5) ) gpu_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.8\", \"pynvml==11.5.0\"], # requirements_path=\"requirements.txt\", cuda_version=\"11.5-cudnn8\", ), env={ \"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility\", \"NVIDIA_VISIBLE_DEVICES\": \"all\", }, resources=Resources(cpu_request=1.5, devices=[NvidiaGPU(name=\"T4\", count=1)]) ) # Python Task: Fetch real-time prices for multiple cryptocurrencies from the CoinGecko API @task(task_config=cpu_config) def fetch_crypto_data() -> str: response = requests.get( \"https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum,litecoin&vs_currencies=usd\" ) return response.text # Python Task: Process the data to extract prices @task(task_config=gpu_config) def extract_prices(data: str) -> dict: from pynvml import nvmlDeviceGetCount, nvmlInit nvmlInit() assert nvmlDeviceGetCount() > 0 json_data = json.loads(data) prices = { \"bitcoin\": json_data[\"bitcoin\"][\"usd\"], \"ethereum\": json_data[\"ethereum\"][\"usd\"], \"litecoin\": json_data[\"litecoin\"][\"usd\"], } return prices # Workflow: Combine all tasks @workflow def crypto_workflow() -> dict: data = fetch_crypto_data() prices = extract_prices(data=data) return prices As you can see for the fetch_crypto_data function we have defined the resource which does not use GPU whereas task extract_prices uses GPU and hence we have defined the env variable and we have also used the pynvml package to check whether GPU is present or not by asserting the condition that assert nvmlDeviceGetCount() > 0 . Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name multi-image-workflow \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" Updated 5 months ago",
    "https://docs.truefoundry.com/docs/python-function-tasks-with-dockefile": "Using dockerfile for python function task Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using dockerfile for python function task All Pages Start typing to search\u2026 Using dockerfile for python function task In Truefoundry workflow you can pass the dockerfile path in the task config, this is useful when you want to install a binary and use it in your python function task like using a jq command, etc. TaskDockerFileBuild To use dockerfile for python task, we need to import TaskDockerFileBuild and use it in PythonTaskConfig . Python from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskDockerFileBuild( dockerfile_path=\"Dockerfile\", ), resources=Resources(cpu_request=0.5) ) Example In this guide, we will see how to write a Python function task with a dockerfile. In this example, we will take a string as an input and then will calculate the SHA56 hash from the input. We will be passing the Prerequisites Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your workflow, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Creating the workflow Create a workflow.py where we will write the code for our workflow and place it in the project root directory with your Dockerfile. Your directory structure will then appear as follows: . \u251c\u2500\u2500 workflow.py \u2514\u2500\u2500 Dockerfile Dockerfile Dockerfile # Use an official Python runtime as a parent image FROM python:3.10-slim # Install jq binary RUN apt-get update && apt-get install -y jq # Set the working directory WORKDIR /app RUN pip install truefoundry[workflow]==0.4.8 # Copy the current directory contents into the container COPY . /app # Set the default command to run Python CMD [\"python\"] workflow.py workflow.py from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskDockerFileBuild( dockerfile_path=\"Dockerfile\", ), resources=Resources(cpu_request=0.45) ) @task(task_config=task_config) def run_jq(input_json: str) -> str: import subprocess # Run the jq binary using Python's subprocess module process = subprocess.run( [\"jq\", \".\"], # jq command to pretty-print JSON input=input_json, text=True, capture_output=True, ) return process.stdout @workflow def my_workflow(input_json: str) -> str: return run_jq(input_json=input_json) As you can see, we have given the dockerfile_path argument in PythonTaskConfig where the path to the docker file is used as its value. Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name dockerfile-build-wf \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" Updated 5 months ago",
    "https://docs.truefoundry.com/docs/container-task": "Using raw container task Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using raw container task All Pages Start typing to search\u2026 Using raw container task In Truefoundry workflow, a container task allows you to execute a command or script inside a Docker container without needing to write a custom Python function for that specific task. It provides a way to use pre-built Docker images and run commands directly within those containers. ContainerTaskConfig Just like python task config, in container task, we have ContainerTaskConfig which take, image spec as image, env, resource and service_account as input. Python from truefoundry.workflow import task, workflow, ContainerTaskConfig, ContainerTask from truefoundry.deploy import Image container_task = ContainerTask( name=\"container_task\", task_config=ContainerTaskConfig( image=Image( image_uri=\"...\", command=[...], ) ), ) Example Now lets take a look at the example where we will just use the alpine base image and then print the number from 1 to 10 and will print their sum. Prerequisites Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your workflow, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Creating the workflow Create a workflow.py where we will write the code for our workflow and place in the project root folder. (With other dependent files and requirements.txt) workflow.py workflow.py from truefoundry.workflow import task, workflow, ContainerTaskConfig, ContainerTask from truefoundry.deploy import Image container_task = ContainerTask( name=\"container_task\", task_config=ContainerTaskConfig( image=Image( image_uri=\"alpine:3.12\", command=[ \"/bin/sh\", \"-c\", \"\"\" # Generate a sequence of numbers from 1 to 10 seq 1 10 | tee /var/data/numbers.txt # Calculate the sum of these numbers sum=$(seq 1 10 | awk '{s+=$1} END {print s}') # Save the sum to a file echo \"Sum: $sum\" \"\"\", ], ) ), ) @workflow def sum_of_numbers() -> str: container_task() return \"The sum of numbers from 1 to 10 has been calculated.\" So here we have defined container_task where we are using alpine image and the command which is basically a bash script to print the number from 1 to 10 and their sum. The container task can be called in the same way as you call the python task. Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name raw-container-wf \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" Updated 5 months ago",
    "https://docs.truefoundry.com/docs/adding-environment-variable": "Adding environment variable Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Adding environment variable All Pages Start typing to search\u2026 Adding environment variable Environment can be added in both Python task and Container task, so when you define the python task config or container task config, we can give the env as follows: Python task config : you can pass the environment variables in the config by using env argument which takes dict as input, where key and value both are strings. Python from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskPythonBuild from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskPythonConfig(...), env={ \"env_variable_name\": \"value_of_env_variable\", }, ... ) @task(task_config=task_config) ... Container task config : The container task config also takes env as input argument, similar to python task config , where you can pass the env variable in key value pair as dict. Python from truefoundry.workflow import task, workflow, ContainerTaskConfig, ContainerTask from truefoundry.deploy import Image container_task = ContainerTask( name=\"container_task\", task_config=ContainerTaskConfig( image=Image(...), env={ \"env_variable_name\": \"value_of_env_variable\", }, ), ) ... Using secrets as an environment variable you can also pass the secret as an env variable in a workflow. To use a secret, instead of passing the value of env, you can pass the secret FQN of the secret in place of value. python from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskDockerFileBuild(...), env={ \"env_variable_name\": \"value_of_env_variable\", # Env with secret fqn \"MY_SECRET\": \"tfy-secret://user:my-secret-group:my-secret\", }, ... ) @task(task_config=task_config) ... Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configuring-resources": "Configuring Resources Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configuring Resources All Pages Start typing to search\u2026 Configuring Resources When we define the task config, we will need to define the resources that the task will need for successful execution. This will ensure that Kubernetes can allot the correct set of resources and the application continues to run smoothly. Setting up the resources in Task Config You can specify the resources for each task in task config. Each task config takes resource as an argument where you can define the resource limit based on the workload of that task. Here is a sample resources object. Setting resources from truefoundry.deploy import Resources, NodeSelector, NvidiaGPU from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskPythonBuild sample_task_config = PythonTaskConfig( # `TaskPythonBuild` helps specify the details of your Python Code. # These details will be used to templatize a DockerFile to build your Docker Image image=TaskPythonBuild( python_version=\"3.9\", # Pip packages to install.`truefoundry[workflow]` is a mandatory dependency pip_packages=[\"truefoundry[workflow]\"], ), + resources=Resources( + cpu_request=2, + cpu_limit=4, + memory_request=12000, + memory_limit=14000, +. # Supported GPUs are -> \"T4\", \"A100_40GB\", \"A100_80GB\", \"V100\" + devices=[NvidiaGPU(name=\"T4\", count=1)]], + node=NodeSelector( + # possible values \"spot\", \"on_demand\", \"spot_fallback_on_demand\" + capacity_type=\"spot_fallback_on_demand\" + ) + ), service_account=\"<service-account>\", ) @task(task_config=sample_task_config) def my_task(): ... Here is a brief explanation of what the different fields refer to: Resource Unit Description CPU CPU cores Processing power required by the application. Defined in terms of CPU cores. 1 CPU unit is equivalent to 1 virtual core. It's possible to ask for fractional CPU resources like 0.1 or even 0.01. You will need to specify CPU requests and limits. The number you define in the CPU request will always be reserved for the application. Your application can occasionaly take more CPU than what is requestes till the CPU limit, but not beyond that. For e.g, if CPU request is 0.5 and limit is 1, it means that the application has 0.5 CPU reserved for itself. CPU usage can go upto 1 if there is CPU available on the machine - else it will be throttled. Memory Megabytes (MB) Defined as an integer and the unit is Megabytes. So a value of 1 means 1 MB of memory and 1000 means 1GB of memory. You need to specify memory requests and limits. The number you define in the memory request will always be reserved for the application. Your application can occasionaly take more memory than what is requested till the memory limit. If the application takes up more memory than the limit, then the application will be killed and restarted. For e.g, if memory request is 500 MB and limit is 1000 MB, it means that you application will always have 500MB of RAM. You can have spikes of memory usage till 1 GB, beyond which the application will be killed and restarted. Ephemeral Storage Megabytes (MB) Temporary disk space to keep code, artifacts, etc which is lost when the pod is terminated. Defined as an integer and the unit is Megabytes. A value of 1 means 1 MB of disk space and 1000 means 1GB of disk space. You need to specify ephemeral Storage requests and limits. If you specify 1 GB as request and 5 GB as limit, you will have guaranteed access to 1GB of disk space. You can go upto 5GB in case there is disk left on the machine, but we shouldn't rely on this. If the application tries to take up more than 5GB, the application will be killed. GPU GPU-Type and Count Graphics processing power required by the application. You need firstly specify which GPU you want to provision for your Application (GPUs can be of the following types: K80, P4, P100, V100, T4, A10G, A100_40GB, A100_80GB, etc.). You can find more details about these GPUs here . In the case of AWS or GCP , you can do this by selecting the GPU-Type (read more here . In case you are using Azure or some other cloud you will just have to specify the Nodepool that contains the GPU (read more here . Secondly, you need to specify the GPU-Count . Please note that if you ask for GPU count as 2 and type as A100, you will get a machine with atleast 2 A100 GPU cards. Its possible in some cloud providers that one machine has 4 A100 GPU cards. In this case, your application will use 2 of the 4 GPU cards and another application can use the rest 2 cards. Shared Memory Megabytes (MB) Shared memory size is needed for data sharing between processes. This is useful in certain scenarios, for example, Torch Dataloaders rely on shared memory to efficiently share data between multiple workers during training. Defined as an integer and the unit is Megabytes. A value of 1 means 1 MB of shared memory and 1000 means 1GB of shared memory. In case your use-case requires Shared memory, and the usage exceeds the Shared memory size, your applications replica will get killed Using spot instances to run tasks TrueFoundry supports different capacity types for node selection, allowing you to optimize costs while maintaining reliability. You can configure this through the NodeSelector in your resource configuration. Available Capacity Types spot - Uses spot instances exclusively Most cost-effective option (up to 70% cheaper than on-demand) Suitable for fault-tolerant workloads Risk of interruption if spot capacity is unavailable on_demand - Uses on-demand instances exclusively Highest availability and reliability No interruption risk Standard pricing spot_fallback_on_demand - Uses spot instances with automatic fallback to on_demand instances Attempts to use spot instances first Automatically falls back to on-demand if spot is unavailable Balances cost savings with reliability Updated 4 months ago",
    "https://docs.truefoundry.com/docs/conditional-task": "Creating a Conditional Task Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a Conditional Task All Pages Start typing to search\u2026 Creating a Conditional Task A special type of task that enables conditional execution paths within a workflow. Conditional tasks allow the workflow to choose different execution paths based on the outcome of a previous task or variable. Conditional task are typically structured with a condition followed by one or more possible execution paths (if-else branches). Example This is how the flow will be of the given example workflow Code Create file names workflow.py and paste the following contents into the file: Python from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskPythonBuild, conditional task_config = PythonTaskConfig(image=TaskPythonBuild( python_version=\"3.11\", pip_packages=[\"truefoundry[workflow]==0.4.8\"], ) ) @task(task_config=task_config) def generate_number() -> int: return 7 @task(task_config=task_config) def process_low() -> str: return \"Low number processing\" @task(task_config=task_config) def process_high() -> str: return \"High number processing\" @workflow def conditional_workflow() -> str: number = generate_number() result = conditional(\"branch\")\\ .if_(number < 5).then(process_low())\\ .else_().then(process_high()) return result Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name conditional-example-wf \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" In this example: generate_number: A task that generates a number. process_low and process_high: Tasks that process the number based on whether it is low or high. conditional: A branch node that checks if the number is less than 5. If true, it executes process_low, otherwise it executes process_high. Conditional task on ui When you see the graph of a workflow containing the conditional task then you can see that the green box here represents the conditional task and the green color indicates that the task execution was successful. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/creating-a-map-task": "Creating a Map Task Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a Map Task All Pages Start typing to search\u2026 Creating a Map Task A Map Task is a specialized task that enables the parallel execution of a single task across multiple inputs. It allows you to apply a task to each element of an input collection (like a list or array) and execute them in parallel. This is particularly useful when you need to perform the same operation on a large dataset or a collection of items, and you want to distribute the workload across multiple tasks Here\u2019s a simple example of how you might define and use a map task. Create a file name workflow.py and place it in your project directory. Python from functools import partial from truefoundry.workflow import task, workflow, map_task, PythonTaskConfig, TaskPythonBuild task_config = PythonTaskConfig(image=TaskPythonBuild( python_version=\"3.11\", pip_packages=[\"truefoundry[workflow]==0.4.8\"], ) ) @task(task_config=task_config) def square(x: int) -> int: return x * x @workflow def my_map_workflow(numbers: list[int]) -> list[int]: square_task = partial(square) squared_array = map_task(square_task)(x=numbers) print(f\"Square of {numbers} is {squared_array}\") return squared_array Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name map-task-test \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" In this example: square: This is a simple task that squares a given integer. map_task: The map_task function is used to apply the square task to each element of the numbers list. my_map_workflow: This workflow demonstrates how to use the map task to process a list of numbers in parallel. Map task on ui Examples of using map tasks with different types of inputs Map task with a single input This is the most basic type of map task where you want to do an operation with just a single input for example in this task we are just returning the square of the input number Python @task(task_config=task_config) def square_numbers(number: int)->int: return number*number Now to call this task in workflow, check the below syntax Python @workflow def square_workflow_number(numbers: List[int] = [1, 2, 3, 4, 5]) -> List[int]: return map_task(square_numbers)(number=numbers) Map task with multiple inputs Now lets say you want to pass multiple inputs which are constants and a list as an input to map tasks, so to accomplish this you can use a partial function from functools library, lets see it in the example. Python @task(task_config=task_config) def multiplier_function(number: int, multiplier: int) -> int: return number * multiplier Now we will import partial from functools and define the map task in the workflow function Python from functools import partial @workflow def map_workflow_with_multipl_input(numbers: List[int] = [1, 2, 3], multiplier: int = 3) -> List[int]: partial_function = partial(multiplier_function, multiplier=multiplier) map_task = map_task(partial_function)(number=numbers) return map_tasks You can provide multiple input arguments in a partial function. \u2139\ufe0f Note It is important to remember that you cannot use list as an input to partial task you can also pass various lists as input to the map task function, let's see an example. Python @task(task_config=task_config) def multiply_numbers(num1: int, num2: int, multiplier: int) -> int: return num1 * num2 * multiplier In the above example, we have two lists of numbers and we want to multiply the numbers at the same position and multiply them with multipler, lets check the workflow function for the same Python from functools import partial @workflow def map_workflow_with_multiple_lists(numbers1: List[int], numbers2: List[int], multiplier: int) -> List[int]: partial_func = partial(multiply_numbers, multiplier=multiplier) return map_task(partial_func)(num1=numbers1, num2=numbers2) \ud83d\udea7 Warning Please not that the length of all the lists should be same while passing muliple list as input in map function. Setting up the concurrency limit You can set the concurrency limit for the map tasks, so that it limits the number of map tasks that can run in parallel to the specified batch size, for example if you set concurrency as 3 , then only 3 map task will run at a given interval of time and the next 3 map task will run only when first 3 task are completed, so it runs in a batch of n numbers of mapped tasks. If the concurrency is not set then the workflow runs as many task concurrently as possible. You can set the concurrency using the below syntax: Python from functools import partial @workflow def map_workflow_with_multiple_lists(numbers1: List[int], numbers2: List[int], multiplier: int) -> List[int]: partial_func = partial(multiply_numbers, multiplier=multiplier) return map_task(partial_func, concurrency=3)(num1=numbers1, num2=numbers2) In the above example the concurrency is set to 3 , so a batch of 3 map task will run at a given period of time. Updated 2 months ago",
    "https://docs.truefoundry.com/docs/cron-workflow": "Creating a Cron Workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a Cron Workflow All Pages Start typing to search\u2026 Creating a Cron Workflow A cron workflow is a type of workflow that is scheduled to run at specific intervals, similar to how cron jobs work in Unix-like systems. It allows you to automate the execution of workflows based on a predefined schedule, which can be expressed using cron syntax. Cron jobs are always scheduled in UTC timezone. The cron job can be scheduled be passing the execution_configs in the workflow decorator Python from truefoundry.workflow import workflow, ExecutionConfig @workflow( execution_configs=[ ExecutionConfig( schedule=\"*/10 * * * *\", ) ] ) def your_workflow(): ... As you can see we have defined the execution config where the schedule is set to run every 10 minutes. The expression is define in cron expression format. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/task-config": "Task Config Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Task Config All Pages Start typing to search\u2026 Task Config Each task takes a task_config parameter which is used to define the config like resource the task execution will require, the python version, libraries, apt packages, cuda version, etc. for the task. Things you can define in task config env : you can pass the environment variables as env in task config, where env is a dictionary of key-value pairs. service_account : you can pass the service_account name in the task config which is necessary to save the input and output data of the task. resources : You can define the resource to allocate to each of the tasks, where you can define the cpu limit, storage limit, memory limit, GPU types, etc. You can refer to this article for more information on each. mounts : You can attach volume mounts such as volume mounts, string mounts or secret mounts. You can learn more about mounts and how to use them in workflow in this guide. Types of task config There are two types of task config PythonTaskConfig and ContainerTaskConfig. PythonTaskConfig : This task config can be passed in the normal python task in the task decorator. You can define the environment variables, Resources , service account, and the image spec in PythonTaskConfig. The image spec can be of two types TaskPythonBuild and TaskDockerFileBuild. TaskPythonBuild is used when you do not have a Dockerfile and you want to build an image where you want to specify the pip packages, apt packages or requirements file path in the build spec, then TaskPythonBuild is used. TaskDockerFileBuild is used when you already have a Dockerfile and you just want to build then you use TaskDockerFileBuild. ContainerTaskConfig : This task config can be used when you already have a docker image and you want to use that as a task in the workflow directly or you have code uploaded on GitHub or the remote source. There you have a docker file which you want to use as a task in the workflow. Python from truefoundry.deploy import Image, NvidiaGPU, Resources from truefoundry.workflow import ( ContainerTask, ContainerTaskConfig, ExecutionConfig, FlyteDirectory, PythonTaskConfig, TaskPythonBuild, conditional, map_task, task, workflow, ) # Python task config example task_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]\"], ), resources=Resources(cpu_request=0.5, cpu_limit=0.5), service_account=\"<service-account>\", ) #container task config example echo = ContainerTask( name=\"echo\", task_config=ContainerTaskConfig( image=Image( image_uri=\"bash:4.1\", command=[\"echo\", \"hello\"], ), service_account=\"<service-account>\", ), ) ... Updated 5 months ago",
    "https://docs.truefoundry.com/docs/attaching-mounts": "Attaching Mounts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Attaching Mounts All Pages Start typing to search\u2026 Attaching Mounts In some Workflows or tasks, you might need a file to be present at a certain path for the workflow to work. Or you have data in directories that need to be mounted for the task to complete. TrueFoundry allows you to mount files in workflow in the following ways: Volume Mount If your task needs access to multiple files of data or multiple directories, you can use a volume to store the data and then mount the volume at the desired path. You can learn more about Volumes here . To use a persistent volume, we will first need to create one and then attach it to your deployment. You can learn how to create volumes using the Creating a Volume guide you can mount the volume for a task in the following way. Python from truefoundry.deploy import VolumeMount, Resources from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild task_config = PythonTaskConfig( image=TaskDockerFileBuild( ... mounts=[ VolumeMount( mount_path=\"/model\", #or your desired path volume_fqn=\"your-volume-fqn\" ) ] ) Secret Mount This is similar to string mount, except, in this case, you will directly provide a TrueFoundry Secret FQN. You can read more about Secrets and how to create them here . The content in the secret will be dumped into a file and mounted in the provided location. A good use-case of this is for mounting Google credentials file which you might need to access Google services in a task. Python from truefoundry.deploy import SecretMount, Resources from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild task_config = PythonTaskConfig( image=TaskDockerFileBuild( ... mounts=[ SecretMount( mount_path=\"/etc/google-credentials.json\", secret_fqn=\"tfy-secret://user:my-secret-group:my-secret\" ) ] ) String Mount This can be useful if you need a small configuration file to be present at a certain file path. To configure this, you need to provide the path where it needs to be mounted and the string data that should be in that file. A example of this can be if you want to apply yaml specs for a service through task, then you can use string mount to define the specs. Python from truefoundry.deploy import StringDataMount, Resources from truefoundry.workflow import task, workflow, PythonTaskConfig, TaskDockerFileBuild task_config = PythonTaskConfig( image=TaskDockerFileBuild( ... mounts=[ StringDataMount( mount_path=\"/etc/nginx/conf.d/my_config.conf\", data=\"server { listen 80; }\" ) ] ) Updated 3 months ago",
    "https://docs.truefoundry.com/docs/passing-filesartifacts-between-tasks": "Passing Files/Artifacts between tasks Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Passing Files/Artifacts between tasks All Pages Start typing to search\u2026 Passing Files/Artifacts between tasks In Workflow, FlyteFile and FlyteDirectory are special types representing files and directory that can be passed between tasks in a workflow. It helps manage and track files and directories, ensuring that they can be accessed and moved across different execution environments. Passing files across tasks Python from truefoundry.workflow import ( PythonTaskConfig, TaskPythonBuild, task, workflow, FlyteFile ) from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.7rc0\"], ), resources=Resources(cpu_request=0.45), service_account=\"default\", ) @task(task_config=task_config) def create_file() -> FlyteFile: file_path = \"/tmp/sample.txt\" with open(file_path, \"w\") as f: f.write(\"Hello World!\") return FlyteFile(file_path) @task(task_config=task_config) def read_and_print_file(flyte_file: FlyteFile): with open(flyte_file, \"r\") as f: content = f.read() print(\"File content:\", content) @workflow def simple_file_workflow(): file = create_file() read_and_print_file(flyte_file=file) if __name__ == \"__main__\": simple_file_workflow() In the above example we are creating a sample.txt file in a create_file task and then returning that file as FlyteFile and then in read_and_print_file task we are taking the flyteFile as input and then printing the content of the file. Passing directory across tasks Python from truefoundry.workflow import ( PythonTaskConfig, TaskPythonBuild, task, workflow, FlyteDirectory ) from truefoundry.deploy import Resources task_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.7rc0\"], ), resources=Resources(cpu_request=0.45), service_account=\"default\", ) @task(task_config=task_config) def create_directory() -> FlyteDirectory: import os dir_path = \"/tmp/sample_directory\" os.makedirs(dir_path, exist_ok=True) # Create multiple text files in the directory for i in range(3): with open(os.path.join(dir_path, f\"file_{i}.txt\"), \"w\") as f: f.write(f\"Content of file {i}\\n\") return FlyteDirectory(dir_path) @task(task_config=task_config) def read_and_print_directory(directory: FlyteDirectory) -> str: import os # List all files in the directory for filename in os.listdir(directory): file_path = os.path.join(directory, filename) with open(file_path, \"r\") as f: content = f.read() print(f\"Contents of {filename}:\") print(content) return \"Done reading and printing the directory.\" @workflow def simple_directory_workflow(): directory = create_directory() read_and_print_directory(directory=directory) In the above example, we are creating multiple files in /tmp/sample_directory directory in create_directory task and then we are passing this directory in read_and_print_directory task to print the files in that directory and the content of these files. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/example-of-task-config-with-different-parameters": "Example of Task config with different parameters Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Example of Task config with different parameters All Pages Start typing to search\u2026 Example of Task config with different parameters A task config takes various parameters as input, in this example, we will see how to use or input all these parameters in detail. Example Python from truefoundry.workflow import ( PythonTaskConfig, TaskPythonBuild, task, workflow, ) from truefoundry.deploy import Resources, SecretMount, VolumeMount, StringDataMount, NvidiaGPU task_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.7rc0\"], #To install pip packages from a requirements file uncomment this and provide the path to the requirements file # requirements_path=\"requirements.txt\", apt_packages=[ \"git\", \"ffmpeg\", ] ), resources=Resources( cpu_request=0.45, cpu_limit=0.5, memory_request=100, memory_limit=150, ephemeral_storage_request=100, ephemeral_storage_limit=150, devices=[NvidiaGPU(name=\"T4\", count=1)], capacity_type=\"spot_fallback_on_demand\" ), service_account=\"default\", mounts=[ # To use volume mount or secret mount uncomment the following lines and provide the correct values # VolumeMount(mount_path=\"/tmp/data\", volume_fqn=\"tfy-volume://tfy-usea1-devtest:nikp-wf-test:wf-test\"), # SecretMount(mount_path=\"/tmp/secret\", secret_fqn=\"tfy-secret://truefoundry:test-secret-1730850194619:abctestkey3\"), StringDataMount(mount_path=\"/tmp/stringdata\", data=\"Enter your data here\"), ] ) # Task 1: Extract data (simulates loading data from an external source) @task(task_config=task_config, retries=3) def print_string_mount_data() -> str: with open(\"/tmp/stringdata\", \"r\") as f: data = f.read() print(f\"Data: {data}\") return data @workflow def simple_workflow() -> str: data = print_string_mount_data() return data if __name__ == \"__main__\": print(f\"data_pipeline(): {simple_workflow()}\") As you can see in the above example, you can pass the path of the requirements file to the requirements_path parameter in image section of task config to install libraries mentioned in requirements files. Make sure that this file path is relative to the root directory. You can also define the apt packages you want to install by defining them apt_packages parameter. To mount the secrets or volume or string data, you can define them in the mounts parameter as seen in example. you can also set the retries on failure for a particular task by passing the retries argument in the @task decorator as seen in above example. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/adding-retries-and-handling-failures": "Adding retries and handling failures Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Adding retries and handling failures All Pages Start typing to search\u2026 Adding retries and handling failures TrueFoundry workflows provide robust mechanisms for handling task failures and retrying failed tasks. Here's how you can implement these features: Task Retries You can configure automatic retries for individual tasks using the retries parameter in the @task decorator: Python @task(task_config=task_config, retries=3) def my_task(): ... This configuration will attempt to execute the task up to 3 additional times if it fails. Note: These retries are specifically user retries (if the code fails due to a code error). If there are infrastructure issues, like spot-interruptions and errors like OOM killed, they are considered as infra failure and can be configured using a parameter called max-node-retries-system-failures which is a cluster level setting. The default value of this field is 3 . Please contact your system admin to change this value. Workflow Failure Handling To handle failures at the workflow level, you can define a failure handler task and specify it using the on_failure parameter in the @workflow decorator: from truefoundry.workflow import task, workflow @task(task_config=task_config) def handle_failure(): print(\"Handling Failure/Sending Notification\") ... @workflow(on_failure=handle_failure) def data_pipeline(): ... If your workflow fails, this will run the \"handle_failure\" task towards the end. This can be used to clean up some resources or database entries or files and also send alert notification. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/running-workflow-locally": "Running workflow locally Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Running workflow locally All Pages Start typing to search\u2026 Running workflow locally You can run workflow locally by simply running your worklow python file, you can do: Python python3 workflow.py Important points to remember: Only your code logic is tested while running the workflow but the environment in which worklow runs can be different from your local environment and there can be difference in execution speed or time taken for workflow to complete. If you are running workflow locally let's say in Windows and then when the workflow runs in Linux environment, then there are chances of code not working properly if any of the library you are using is only compatible with windows or behaves differently in linux. Updated about 1 month ago",
    "https://docs.truefoundry.com/docs/adding-alerts-for-workflow": "Adding Alerts for Workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Adding Alerts for Workflow All Pages Start typing to search\u2026 Adding Alerts for Workflow Pre-Requisites Setup a Notification Channel Integration on the Integrations page. Please follow this document to add an integration. Currently two types of Notification Channels are supported: Slack (Webhook URL based): For this, please create a webhook for your slack and then add it as an integration in Slack Provider Account (Slack Webhook Integration) Slack (Bot Token based): For this, please create a bot token for your slack and then add it as an integration in Slack Provider Account (Slack Bot Integration). It requires chat:write and chat:write:public scope. You can add slack channels to send to respective slack channel. Email (SMTP credentials Integration): This can be found in Custom Provider Account. You can configure the SMTP Credentials of the mail server, from_email and to_emails and use it to send notifications. How to Configure Define alerts in workflow Decorator You can define or configure the alerts in the workflow decorator in your workflow file. Python from truefoundry.workflow import ... from truefoundry.deploy import Resources, WorkflowAlert, Email, SlackWebhook ... ... @workflow( alerts=[ WorkflowAlert( notification_target=Email( notification_channel=\"<Paste your email notification integration fqn here>\", to_emails=[ \" [email protected] \" ], ), on_completion=True, on_failure=True, ), WorkflowAlert( notification_target=SlackWebhook( notification_channel=\"<Paste your slack webhook notification integration fqn here>\", ), on_completion=True, on_failure=True, ), WorkflowAlert( notification_target=SlackBot( notification_channel=\"<Paste your slack bot notification integration fqn here>\", channels:[ \"#workflow-notifications\" ] ), on_completion=True, on_failure=True, ), ] ) def my_workflow(): ... \u26a0\ufe0f Make sure that the truefoundry package version is set to latest version in task config. For example in pip_packages filed in python task config, make sure that trufoundry[workflow] version is 0.6.3 or newer and not older one, or make sure that version which you are mentioning supports alerts. Right now the alert notification is supported only for workflow completion(successful completio and workflow failure(failure includes failed executions and aborted as well as timed out workflow executions). There are three types of targets, one is email and the others are slack webhook and slack bot, you need to add an email/slack webhook/slack bot integration respectively and select it in notification channel field to use that integration for sending notifications. Now when you will deploy the workflow, the alerts will be set. Define alerts in deploy.py file Another way to define alerts is to define workflow in deploy.py file and set alerts in it. deploy.py from truefoundry.deploy import Workflow, WorkflowAlert, SlackWebhook, Email wf = Workflow( name=\"test-email-alerts-2\", workflow_file_path=\"nested.py\", alerts=[ WorkflowAlert( notification_target=SlackWebhook( notification_channel=\"<Paste your slack webhook notification integration fqn here>\", ), on_completion=True, on_failure=True, ), WorkflowAlert( notification_target=Email( notification_channel=\"<Paste your email notification integration fqn here>\", to_emails=[\" [email protected] \"], ), on_completion=True, on_failure=True, ), WorkflowAlert( notification_target=SlackBot( notification_channel=\"<Paste your slack bot notification integration fqn here>\", channels=[\"#workflow-notifications\"], ), on_completion=True, on_failure=True, ), ], ) wf.deploy(\"<workspace-fqn>\", wait=False) After defining this, you can just run python deploy.py and your workflow will be deployed and alerts will be set. Updated 8 days ago",
    "https://docs.truefoundry.com/docs/infra-set-up-for-workflows": "Infra Set-Up for Workflows Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Infra Set-Up for Workflows All Pages Start typing to search\u2026 Infra Set-Up for Workflows Setting up workflows in a Workload Cluster (already connected to truefoundry) requires the following configuration to be done: Requirements: Cloud Storage Bucket (S3/GCS/AzureBlob) Service Account (or Key based access) with \"Admin\" permission to the bucket. Steps Create an Integration of Blob Storage on the platform ( from Integrations section). Ignore this step if you already have the integration added. In the \"Clusters\" section, Edit your cluster and Update the \"Workflow Storage Integration\" with the integration from Step 1. Install Workflow Propeller in the cluster( Dataplane Component of Flyte ) by following the instruction below (Depending on the Cloud Provider). Note : Workflow propeller requires access to the blob storage via a serviceaccount. Note: Ideally the Blob Storage and the cluster should be in the same Region. Setting Up tfy-workflow-propeller To install tfy-workflow-propeller , follow the following steps: Create a workspace in your cluster with the name: tfy-workflow-propeller [ Workspaces -> New Workspace] Open the deployments page and click on New Deployment Select the workspace as tfy-workflow-propeller and chose the deployment type as Helm as shown in image below Once you click on next, fill the following values in the form as shown below: Name: tfy-workflow-propeller Helm repository URL: https://truefoundry.github.io/infra-charts/ Chart Name: tfy-workflow-propeller Version: 0.0.2 Values: <Cloud Specific, refer to the next section of docs> Now, you need fill the values section of tfy-workflow-propeller . These are described in the next section (depending on the cloud) Values of Workflow Propeller AWS Steps: Create an S3 bucket (or use an already existing bucket). This should be in same region as your cluster. Create a Role which has Admin access to the above S3 bucket. Add policy to ensure that this role can be assumed by the following ServiceAccount: flytepropeller in tfy-workflow-propeller namespace [ No need to create service account, it will be created by the helm chart itself] Fill the values in the file below and deploy the tfy-workflow-propeller. YAML global: tenantName: <Enter your tenant name> controlPlaneUrl: <Enter the full control plane url e.g. https://xyz.truefoundry.tech> flyte-core: common: ingress: enabled: false secrets: adminOauthClientCredentials: enabled: true storage: type: s3 limits: maxDownloadMBs: 2 bucketName: <Enter your S3 bucket name> connection: region: <Enter your AWS region> auth-type: iam enable-multicontainer: true webhook: enabled: false configmap: k8s: plugins: k8s: default-env-vars: - TFY_INTERNAL_SIGNED_URL_SERVER_HOST: >- http://tfy-signed-url-server.tfy-workflow-propeller.svc.cluster.local:3001 core: propeller: leader-election: enabled: true retry-period: 2s lease-duration: 15s renew-deadline: 10s lock-config-map: name: propeller-leader namespace: tfy-workflow-propeller metadata-prefix: s3://<Enter your S3 bucket name>/tfy-workflow-propeller/metatdata rawoutput-prefix: s3://<Enter your S3 bucket name>/tfy-workflow-propeller/raw_data publish-k8s-events: true admin: admin: Command: - echo - <Enter your cluster token (can be found in tfy-agent helm chart> AuthType: ExternalCommand # endpoint: app.truefoundry.com:443 endpoint: <Enter your control plane host (without https://)>:443 insecure: false AuthorizationHeader: authorization event: rate: 500 type: admin capacity: 100 logger: level: 5 show-source: true flyteadmin: enabled: false serviceAccount: alwaysCreate: true datacatalog: enabled: false flyteconsole: enabled: false flytepropeller: enabled: true serviceAccount: # service account is created with name `flytepropeller` create: true annotations: eks.amazonaws.com/role-arn: >- <Enter AWS role ARN with access to storage bucket> workflow_scheduler: enabled: false workflow_notifications: enabled: false cluster_resource_manager: enabled: false tfySignedURLServer: env: AWS_REGION: <Enter your AWS region here> S3_BUCKET_NAME: s3://<Enter your s3 bucket name here> DEFAULT_CLOUD_PROVIDER: aws enabled: true GCP Steps: Create a GCS bucket (or use an already existing bucket). This should be in same region as your cluster. Create a Google Cloud Service Account which has Admin access to the above GCS bucket. Add policy to ensure that this role can be assumed by the following Kubernetes ServiceAccount: flytepropeller in tfy-workflow-propeller namespace [ No need to create service account, it will be created by the helm chart itself] You may use the script below to do the following: #!/bin/bash PROJECT=<Enter your GCP PROJECT ID> CLUSTER_NAME=<Enter your GCP Cluster Name> REGION=<Enter your gcp region> BUCKET_NAME=<Enter your GCP bucket name without gs:// prefix> TARGET_NAMESPACE=tfy-workflow-propeller # Clip bucket name to 20 characters CLIPPED_BUCKET_NAME=\"${BUCKET_NAME:0:20}\" K8S_SA_NAME=\"flytepropeller\" GCP_SA_NAME=\"$CLIPPED_BUCKET_NAME-flyte-sa\" GCP_SA_ID=\"$GCP_SA_NAME@$PROJECT.iam.gserviceaccount.com\" gcloud iam service-accounts create $GCP_SA_NAME --project=$PROJECT gcloud storage buckets add-iam-policy-binding gs://$BUCKET_NAME \\ --member \"serviceAccount:$GCP_SA_ID\" \\ --role \"roles/storage.objectAdmin\" \\ --project $PROJECT gcloud iam service-accounts add-iam-policy-binding $GCP_SA_ID \\ --role \"roles/iam.serviceAccountTokenCreator\" \\ --member \"serviceAccount:$GCP_SA_ID\" \\ --project $PROJECT gcloud iam service-accounts add-iam-policy-binding $GCP_SA_ID \\ --role roles/iam.workloadIdentityUser \\ --member \"serviceAccount:$PROJECT.svc.id.goog[$TARGET_NAMESPACE/$K8S_SA_NAME]\" \\ --project $PROJECT gcloud storage buckets add-iam-policy-binding gs://$BUCKET_NAME \\ --member \"serviceAccount:$GCP_SA_ID\" \\ --role \"roles/storage.legacyBucketReader\" \\ --project $PROJECT Fill the values in the file below and deploy the tfy-workflow-propeller. YAML global: tenantName: <Enter your tenant name> controlPlaneUrl: <Enter the full control plane url e.g. https://xyz.truefoundry.tech> flyte-core: common: ingress: enabled: false secrets: adminOauthClientCredentials: enabled: true storage: gcs: projectId: <Enter GCP Project Id here> type: gcs limits: maxDownloadMBs: 2 bucketName: <Enter GCS bucket name> webhook: enabled: false configmap: k8s: plugins: k8s: default-env-vars: - TFY_INTERNAL_SIGNED_URL_SERVER_HOST: >- http://tfy-signed-url-server.tfy-workflow-propeller.svc.cluster.local:3001 core: propeller: leader-election: enabled: true retry-period: 2s lease-duration: 15s renew-deadline: 10s lock-config-map: name: propeller-leader namespace: tfy-workflow-propeller metadata-prefix: gs://<Enter GCS bucket name>/tfy-workflow-propeller/metadata rawoutput-prefix: gs://<Enter GCS bucket name>/tfy-workflow-propeller/raw_data publish-k8s-events: true admin: admin: Command: - echo - <Enter your cluster token (can be found in tfy-agent helm chart> AuthType: ExternalCommand # endpoint: app.truefoundry.com:443 endpoint: <Enter your control plane host (without https://)>:443 insecure: false AuthorizationHeader: authorization event: rate: 500 type: admin capacity: 100 logger: level: 5 show-source: true flyteadmin: enabled: false datacatalog: enabled: false flyteconsole: enabled: false flytepropeller: enabled: true serviceAccount: create: true annotations: iam.gke.io/gcp-service-account: >- <Enter your GCP Service Account Email> workflow_scheduler: enabled: false workflow_notifications: enabled: false cluster_resource_manager: enabled: false tfySignedURLServer: env: GS_BUCKET_NAME: <Enter you you GCS bucket name (without gs:// prefix)> DEFAULT_CLOUD_PROVIDER: gcp enabled: true Azure Create a Azure Blob Storage Account and Container (or use an already existing container). This should be in same region as your cluster. Create a Storage Account Key (Can be found in Connection String) with Admin access to the storage account. Fill the values in the file below and deploy the tfy-workflow-propeller. YAML flyte-core: common: ingress: enabled: false secrets: adminOauthClientCredentials: enabled: true storage: type: custom custom: stow: kind: azure config: key: <Enter your Azure Storage Account Key> account: <Enter your Storage Account Name> type: stow container: <Enter name of the Storage Container> connection: {} enable-multicontainer: true limits: maxDownloadMBs: 2 webhook: enabled: false configmap: k8s: plugins: k8s: default-env-vars: - AZURE_STORAGE_ACCOUNT_NAME: <Enter your Storage Account Name> - AZURE_STORAGE_ACCOUNT_KEY: <Enter your Azure Storage Account Key> core: propeller: leader-election: enabled: true retry-period: 2s lease-duration: 15s renew-deadline: 10s lock-config-map: name: propeller-leader namespace: tfy-workflow-propeller metadata-prefix: abfs://<Enter name of the Storage Container>/tfy-workflow-propeller/metadata rawoutput-prefix: abfs://<Enter name of the Storage Container>/tfy-workflow-propeller/raw_data publish-k8s-events: true admin: admin: Command: - echo - <Enter your cluster token (can be found in tfy-agent helm chart> AuthType: ExternalCommand # endpoint: app.truefoundry.com:443 endpoint: <Enter your control plane host (without https://)>:443 insecure: false AuthorizationHeader: authorization event: rate: 500 type: admin capacity: 100 logger: level: 5 show-source: true flyteadmin: enabled: false datacatalog: enabled: false flyteconsole: enabled: false flytepropeller: enabled: true serviceAccount: create: true workflow_scheduler: enabled: false workflow_notifications: enabled: false cluster_resource_manager: enabled: false Updated 5 months ago",
    "https://docs.truefoundry.com/docs/introduction-1": "Introduction to LLMOps Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to LLMOps All Pages Start typing to search\u2026 Introduction to LLMOps LLMOps has become crucial specially when we are building LLM applications and deploying them in production. While building a demo with LLMs has been made quite easy with libraries like Langchain and LlamaIndex, getting an LLM to production still requires significant effort. If you are working with LLMs, you are probably adopting the approaches mentioned below and will have to solve atleast few of the corresponding issues. Prompt Engineering Storing all prompts and maintaining versions of prompts. Implemeting retries, fallback while calling LLM provider APIs like Cohere, Anthropic. LLM Model deployment if you are hosting an open source model. Logging of all prompt response pairs for auditability and finetuning later. Response moderation from LLMs to remove hate language and or comply with brand guidelines. Monitoring of costs, api requests and latency Cache queries and responses to save costs and latency. Retrieval Augmented Generation Write logic for data loading and chunking Figure out which embedding and LLM Model to use. Deploying VectorDBs Building an feedback collection and evaluation system to evaulate your RAG accuracy. Semantic Caching of queries LLM Finetuning You might need to finetune models if you have unique set of data and you want to alter the behaviour of the LLM You want to finetune smaller LLMs to specific tasks like Classification, etc. Truefoundry assists you in your LLM application journey when you are looking to take your demo built using Langchain/LlamaIndex to production. They key areas where Truefoundry helps in LLM development are: Experiment with multiple LLMs (ChatGPT, Cohere, Anthropic, Llama and other open source LLMs) and embedding models using a single unified API in Truefoundry's LLM Playground . You can get hosted endpoints for most of the popular open source models and you don't need to deploy them to test them out. Implement retries, fallbacks, caching for your API requests to improve reliability and latency. Check out LLM Gateway . Implement logging and monitoring for all LLM requests with an option to record feedback from the user for the requests. Check out LLM Gateway . Build a QA system over docs using RAG - Truefoundry provides a one click RAG production deployment setup which allows you to ask questions over different set of documents, allow incremental indexing and provide the best accuracy. Check out Building a RAG based QA system. Deploying open source models on your own cloud environment - This can be the case when you cannot send your data over an API to an external provider. Truefoundry helps you deploy the LLM models in a reliable, and cost-effective manner on AWS, GCP, Azure or other cloud providers. Check out Deploy LLM. Fine tune LLM Models - Truefoundry can help you finetune LLMs to your own data in a reliable and cost effective way by enabling finetuning on spot instances on your own infrastructure so that data doesn't leave your environment. Check out LLM Finetuning . Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploying-an-llm-model-from-the-model-catalogue": "Deploying LLMs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploying LLMs All Pages Start typing to search\u2026 Deploying LLMs Deploying LLMs on your own cloud infrastructure in an optimal way can be complicated because of the following reasons: Choosing the most optimal model server: There are multiple options to serve LLMs ranging from using fastapi to more optimized model servers like vLLM , Nvidia Triton , etc. GPU Provisioning and choosing the right GPU: Provisioning GPUs and figuring out the right GPU for your LLM requires deep infrastructure understanding and benchmarking. Configure Model Caching : LLMs are quite big in size ranging from 5 GB to 150GB. Downloading such models on every restart incurs huge amount of networking cost and leads to lower startup times. Hence, we need to configure model caching to allow multiple downloads and make autoscaling faster. Configure Autoscaling : The most common metric that works for LLM autoscaling is requests per second. We will need to setup autoscaling metrics to make sure the service can scale up as traffic increases. Hosting on multiple regions or clouds : Sometime, you might not get availability of GPUs in one cloud provider or one region. So you will need to host the LLMs in multiple regions or cloud providers. This can allow you to utilize spot instances effectively and lower your LLM hosting by 70-80%. TrueFoundry simplifies the process of deploying LLM by automaticaly figuring out the most optimal way of deploying any LLM and configuring the correct set of GPUs. We also enable model caching by default and make it really easy to configure autoscaling based on your needs. To deploy a LLM, you can either choose one of the models in the LLM catalogue or paste the HuggingFace LLM url . Truefoundry will try its best to figure out the details from the models page and come up with the best deployment pattern for it. This should work for most of the LLM models - but you might have to tweak in some cases in case we don't find all the information on the HuggingFace page. Pre-requisites Before you begin, ensure you have the following: Workspace : To deploy your LLM, you'll need a workspace. If you don't have one, you can create it using this guide: Create a Workspace Deploying a LLM Let's deploy a Llama2-7B LLM through the Model Catalouge \ud83d\udcd8 Accessing Gated/Private Models To access private models, enable the Is it a private model? toggle and enter a Secret containing your HuggingFace Token from the HuggingFace account that has access to the model Sending Requests to your Deployed LLM You can send requests to each LLM through either the \"Completions\" endpoint or the \"Chat Completions\" . Note: Chat Completions Endpoint is available for models which have prompt templates for chat defined. You can send requests to your LLM using both the normal and streaming methods. Normal method : Sends the entire request at once and receives the response as a JSON object. Streaming method : Sends the request as a stream of events, allowing you to process the response as it arrives. This can be done with streaming APIs or with OpenAI APIs also. Making requests normally You can get the code to send your request in any language using the OpenAPI tab. Follow the instructions below. Request Body (Chat) Request Body (Completions) { \"model\": \"<YOUR_MODEL_NAME_HERE>\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"Hello!\" } ] } { \"model\": \"<YOUR_MODEL_NAME_HERE>\", \"prompt\": \"This is a test prompt\" } You can find MODEL_NAME with a GET request on <YOUR_ENDPOINT_HERE>/v1/models Here is a sample response For example, here is a Python code snippet to send a request to your LLM Python (Chat Endpoint) Python (Completions Endpoint) # pip install requests import json import requests URL = \"<YOUR_ENDPOINT_HERE>/v1/chat/completions\" headers = {\"Content-Type\": \"application/json\", \"Accept\": \"text/event-stream\"} payload = { \"model\": \"<YOUR_MODEL_NAME_HERE>\", \"messages\": [ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": \"Hello!\" } ] } response = requests.post(URL, json=payload, headers=headers) print(response.json()) # pip install requests import json import requests URL = \"<YOUR_ENDPOINT_HERE>/v1/completions\" headers = {\"Content-Type\": \"application/json\", \"Accept\": \"text/event-stream\"} payload = { \"model\": \"llama-2-7b-chat-hf\", \"prompt\": \"This is a test prompt\" } response = requests.post(URL, json=payload, headers=headers) print(response.json()) Making requests via streaming client Streaming requests allow you to receive the generated text from the LLM as it is being produced, without having to wait for the entire response to be generated. This method is useful for applications that require real-time processing of the generated text. Chat Endpoint from openai import OpenAI import json client = OpenAI( base_url=\"<YOUR_ENDPOINT_HERE>/v1\", api_key=\"TEST\", ) stream = client.chat.completions.create( messages = [ {\"role\": \"user\", \"content\": \"Enter your prompt here\"}, ], model= \"<YOUR_MODEL_NAME_HERE>\", stream=True, max_tokens=500, ) for chunk in stream: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Additional Configuration Optionally, in the advanced options, you might want to Add Authentication to Endpoints \ud83d\udcd8 Basic Auth with OpenAI SDK If you add Basic Authentication with username and password, you can pass them to OpenAI SDK using default_headers argument python import base64 from openai import OpenAI username = \"...\" password = \"...\" credentials = f\"{username}:{password}\" encoded_credentials = base64.b64encode(credentials.encode()).decode() client = OpenAI( base_url=\"<YOUR_ENDPOINT_HERE>/v1\", api_key=\"TEST\", default_headers={\"Authorization\": f\"Basic {encoded_credentials}\"}, ) Configure Autoscaling and Rollout Strategy Making LLM in TrueFoundry Model Registry Deployable If you are logging a LLM in TrueFoundry Model Registry, we would need some metadata to make it deployable - Specifically pipeline_tag , library_name and base_model / huggingface_model_url You can add / update these using the truefoundry[ml] Python SDK. Make sure to complete the Setup for CLI steps Shell pip install -U truefoundry[ml] tfy login --host <Your TrueFoundry Platform Url> Here is an example: Say we finetuned a model based on Meta-Llama-3-8B-Instruct and are logging it in TrueFoundry Model Registry, we can add metadata like so: Updating an already logged model Get the model version FQN Python from truefoundry.ml import get_client, ModelFramework # Base Model ID from Huggingface Hub base_model = \"NousResearch/Meta-Llama-3-8B-Instruct\" logged_model_fqn = \"<Copy the FQN from UI and Paste>\" client = get_client() model_version = client.get_model_version_by_fqn(fqn=logged_model_fqn) model_version.metadata.update({ \"pipeline_tag\": \"text-generation\", \"library_name\": \"transformers\", \"base_model\": base_model, \"huggingface_model_url\": f\"https://huggingface.co/{base_model}\" }) model_version.update() Or, While logging a new model Python from truefoundry.ml import get_client, TransformersFramework # Base Model ID from Huggingface Hub base_model = \"NousResearch/Meta-Llama-3-8B-Instruct\" client = get_client() model_version = client.log_model( ml_repo=\"my-ml-repo\", name=\"my-finetuned-llama-3-8b, model_file_or_folder=\"path/to/local/model\", # Model location on disk framework=TransformersFramework( library_name=\"transformers\", pipeline_tag=\"text-generation\", base_model=base_model, ), metadata={ \"library_name\": \"transformers\", \"pipeline_tag\": \"text-generation\", \"base_model\": base_model, \"huggingface_model_url\": f\"https://huggingface.co/{base_model}\" }, ) print(model_version.fqn) With this done, you should see a deploy button on the UI Updated 3 months ago What\u2019s Next Send Requests to the Deployed LLM",
    "https://docs.truefoundry.com/docs/deploy-llms-using-nvidia-tensorrt-llm-trt-llm": "Deploy LLMs using Nvidia TensorRT-LLM (TRT-LLM) Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy LLMs using Nvidia TensorRT-LLM (TRT-LLM) All Pages Start typing to search\u2026 Deploy LLMs using Nvidia TensorRT-LLM (TRT-LLM) In this guide, we will deploy a Llama based LLM using Nvidia's TensorRT-LLM runtime. Specfically for this example we will deploy meta-llama/Llama-3.1-8B-Instruct with bf16 precision on 2 x L40S GPUs but you can choose whatever GPU configuration available to you \ud83d\udea7 Quantization Support coming soon! 1. Add your Huggingface Token as a Secret Since we are going to deploy the official Llama 3.1 8B Instruct model, we'd need a Huggingface Token that has access to the model. Visit the model page and fill the access form. You'd get access to the model in 10-15 mins. Now, Create a token following the HuggingFace Docs and Add it as a secret which we can use later 2. Create a ML Repo and a Workspace with access to ML Repo Follow the docs at Creating a ML Repo to create a ML Repo backed by your Storage Integration. We will use this to store and version TRT-LLM engines. Create a Workspace or use and existing Workspace and Grant it Editor access to the ML Repo 3. Deploy the Engine Builder Job Save the following YAML in a file called builder.truefoundry.yaml builder.truefoundry.yaml name: trtllm-engine-builder-l40sx2 type: job image: type: image image_uri: truefoundrycloud/trtllm-engine-builder:0.13.0 command: >- python build.py --model-id {{model_id}} --truefoundry-ml-repo {{ml_repo}} --truefoundry-run-name {{run_name}} --model-type {{model_type}} --dtype {{dtype}} --max-batch-size {{max_batch_size}} --max-input-len {{max_input_len}} --max-num-tokens {{max_num_tokens}} --kv-cache-type {{kv_cache_type}} --workers {{workers}} --gpt-attention-plugin {{gpt_attention_plugin}} --gemm-plugin {{gemm_plugin}} --nccl-plugin {{nccl_plugin}} --context-fmha {{context_fmha}} --remove-input-padding {{remove_input_padding}} --reduce-fusion {{reduce_fusion}} --enable-xqa {{enable_xqa}} --use-paged-context-fmha {{use_paged_context_fmha}} --multiple-profiles {{multiple_profiles}} --paged-state {{paged_state}} --use-fused-mlp {{use_fused_mlp}} --tokens-per-block {{tokens_per_block}} --delete-hf-model-post-conversion trigger: type: manual env: PYTHONUNBUFFERED: '1' HF_TOKEN: \"YOUR-HF-TOKEN-SECRET-FQN\" params: - name: model_id default: meta-llama/Meta-Llama-3.1-8B-Instruct param_type: string - name: ml_repo default: trtllm-models param_type: ml_repo - name: run_name param_type: string - name: model_type default: llama param_type: string - name: dtype default: bfloat16 param_type: string - name: max_batch_size default: \"256\" param_type: string - name: max_input_len default: \"4096\" param_type: string - name: max_num_tokens default: \"8192\" param_type: string - name: kv_cache_type default: paged param_type: string - name: workers default: \"2\" param_type: string - name: gpt_attention_plugin default: auto param_type: string - name: gemm_plugin default: auto param_type: string - name: nccl_plugin default: auto param_type: string - name: context_fmha default: enable param_type: string - name: remove_input_padding default: enable param_type: string - name: reduce_fusion default: enable param_type: string - name: enable_xqa default: enable param_type: string - name: use_paged_context_fmha default: enable param_type: string - name: multiple_profiles default: enable param_type: string - name: paged_state default: enable param_type: string - name: use_fused_mlp default: disable param_type: string - name: tokens_per_block default: \"64\" param_type: string retries: 0 resources: node: type: node_selector capacity_type: on_demand devices: - type: nvidia_gpu count: 2 name: L40S cpu_request: 6 cpu_limit: 8 memory_request: 54400 memory_limit: 64000 ephemeral_storage_request: 70000 ephemeral_storage_limit: 100000 \ud83d\udea7 resources section varies across cloud provider Based on your cloud provider, the available gpu type and nodepools will be different, you'd need to adjust it before deploying. Replace YOUR-HF-TOKEN-SECRET-FQN with the Secret FQN we created at the beginning. E.g. Diff env: - HF_TOKEN: YOUR-HF-TOKEN-SECRET-FQN + HF_TOKEN: tfy-secret://truefoundry:chirag-personal:HF_TOKEN Generating correct resources section your configuration. Start a New Deployment, scroll to Resources section and select the GPU type and Count and click on the Spec button You can copy this resources section and replace it in builder.truefoundry.yaml After Setting up CLI , deploy the job by mentioning the Workspace FQN Shell tfy deploy --file builder.truefoundry.yaml --workspace-fqn <your-workspace-fqn> --no-wait Once the Job is deployed, Trigger it Enter a Run Name. You can modify other arguments if needed. Please refer to TRT-LLM Docs to know more about these parameters. Click Submit When the run finishes, you'd have the tokenizer and engine ready to use under the Run Details section Engine is available under Models section. Copy the FQN and keep it handy. Tokenizer is available under Artifacts section. Copy the FQN and keep it handy. 4. Deploy with Nvidia Triton Server Finally, let's deploy the engine using Nvidia Triton Server as a TrueFoundry Service. Here is the spec in full: \ud83d\udea7 resources section varies across cloud provider Based on your cloud provider, the available gpu type and nodepools will be different, you'd need to adjust it before deploying. server.truefoundry.yaml name: llama-3-1-8b-instruct-trt-llm type: service env: DECOUPLED_MODE: 'True' BATCHING_STRATEGY: inflight_fused_batching ENABLE_KV_CACHE_REUSE: 'True' TRITON_MAX_BATCH_SIZE: '64' BATCH_SCHEDULER_POLICY: max_utilization ENABLE_CHUNKED_CONTEXT: 'True' KV_CACHE_FREE_GPU_MEM_FRACTION: '0.95' image: type: image image_uri: docker.io/truefoundrycloud/tritonserver:24.09-trtllm-python-py3 ports: - port: 8000 expose: false protocol: TCP app_protocol: http labels: tfy_openapi_path: openapi.json replicas: 1 resources: node: type: node_selector capacity_type: on_demand devices: - name: L40S type: nvidia_gpu count: 2 cpu_limit: 8 cpu_request: 6 memory_limit: 64000 memory_request: 54400 shared_memory_size: 12000 ephemeral_storage_limit: 100000 ephemeral_storage_request: 20000 liveness_probe: config: path: /health/ready port: 8000 type: http period_seconds: 10 timeout_seconds: 1 failure_threshold: 8 success_threshold: 1 initial_delay_seconds: 15 readiness_probe: config: path: /health/ready port: 8000 type: http period_seconds: 10 timeout_seconds: 1 failure_threshold: 5 success_threshold: 1 initial_delay_seconds: 15 artifacts_download: artifacts: - type: truefoundry-artifact artifact_version_fqn: artifact:truefoundry/trtllm-models/tokenizer-meta-llama-3-1-instruct-l40sx2:1 download_path_env_variable: TOKENIZER_DIR - type: truefoundry-artifact artifact_version_fqn: model:truefoundry/trtllm-models/trt-llm-engine-meta-llama-3-1-instruct-l40sx2:1 download_path_env_variable: ENGINE_DIR Adjust the resources section like we did for the builder Job \u2757\ufe0f GPU configuration must be same as the Builder Job Since TRT-LLM optimizes the model for the target GPU type and counts - it is important that the GPU type and count matches while deploying. In artifacts_download , you'd need to change artifact_version_fqn to the tokenizer and engine obtained at the end of the Job Run from previous section Deploy the Service by mentioning the Workspace FQN Shell tfy deploy --file server.truefoundry.yaml --workspace-fqn <your-workspace-fqn> --no-wait Once deployed, we'll make some final adjustments by editing the Service From Ports section Enable Expose and configure Endpoint as needed (Optional) Configure Download Models and Artifacts to prevent re-downloads 5. Run Inferences You can now send a payload like follows to /v1/chat/completions Endpoint via the OpenAPI tab JSON { \"model\": \"ensemble\", \"messages\": [ {\"role\": \"user\", \"content\": \"Describe the moon in 100 words\"} ], \"max_tokens\": 128 } Since the endpoint is OpenAI compatible, you can Add it to LLM Gateway or use it directly with OpenAI SDK Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/finetuning-a-model-from-the-model-catalogue": "Finetuning LLMs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Finetuning LLMs All Pages Start typing to search\u2026 Finetuning LLMs Finetune Llama, Mistral, Mixtral and more on one or more GPUs LLMs are pre-trained on massive datasets of text and code. This makes them versatile for various tasks, but they may not perform optimally on your specific domain or data. Finetuning allows you to train these models on your data, enhancing their performance and tailoring them to your unique requirements. Fine-tuning with TrueFoundry allows you to bring your data, and fine-tune popular Open Source LLM's such as Llama 2, Mistral, Zephyr, Mixtral, and more. This is made easy, as we provide pre-configured options for resources and use the optimal training techniques available. You can choose to perform fine-tuning either using Jobs or Notebooks . You can further, easily track the progress of finetuning through ML-Repositories. \ud83d\udcd8 Supported Architectures and Sizes We support model sizes of up to 70B for the following model architectures llama mistral qwen qwen2 phi phi3 Following architectures are supported on best effort basis mixtral gemma gemma2 falcon deci mpt gpt_bigcode gpt_neox QLoRA For fine-tuning, TrueFoundry embraces the QLoRA technique, a cutting-edge technique that revolutionizes fine-tuning by balancing power and efficiency. This technique uses clever tricks to stay compact, so you can fine-tune on smaller hardware (even just one GPU), saving time, money, and resources, all while maintaining top performance. Pre-requisites Before you begin, ensure you have the following: Workspace : To deploy your LLM, you'll need a workspace. If you don't have one, you can create it using this guide: Create a Workspace or seek assistance from your cluster administrator. Setting up the Training Data We support two different data formats: Chat Data needs to be in jsonl format with each line containing a whole conversation in OpenAI Chat format Each line contains a key called messages . Each messages key contains a list of messages, where each message is a dictionary with role and content keys. The role key can be either user , assistant or system and the content key contains the message content. Example: Chat Type Dataset {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"William Shakespeare\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]} {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"}, {\"role\": \"user\", \"content\": \"Can you be more sarcastic?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]} ... Completion Data needs to be in jsonl format with each line containing a json encoded string containing two keys prompt and completion. Example: Completion Type Dataset {\"prompt\": \"What is 2 + 2?\", \"completion\": \"The answer to 2 + 2 is 4\"} {\"prompt\": \"Flip a coin\", \"completion\": \"I flipped a coin and the result is heads!\"} {\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"} ... You can further split your data into training data and evaluation data. Once your data is prepared, you need to store the data somewhere. You can choose where to store your data: TrueFoundry Artifact : Upload it as a TrueFoundry artifact for easy access. Cloud Storage : Upload it to a cloud storage service. Local Machine : Save it directly on your computer. Upload to a TrueFoundry Artifact If you prefer to upload your training data directly to TrueFoundry as an artifact, follow the Add Artifacts via UI , and Upload your .jsonl training data file. Upload to a cloud storage You can upload your data to a S3 Bucket using the following command: Shell aws s3 cp \"path-to-training-data-file-locally\" s3://bucket-name/dir-to-store-file Once done you can generate a pre-signed URL of the S3 Object using the following command: Shell aws s3 presign s3://bucket-name/path-to-training-data-file-in-s3 The output of uploading the file to AWS S3 and getting the pre-signed URL Now you can use this pre-signed URL in the fine-tuning job / notebook. Similarly, you can also upload to AZURE BLOB and GCP GCS . Fine-Tuning a LLM Now that your data is prepared, you can start the fine-tuning. Once your data is ready, you can now start fine-tuning your LLM. Here you have two options, deploying a fine-tuning notebook for experimentation or launching a dedicated fine-tuning job. Notebooks: Experimentation Playground Notebooks offer an ideal setup for explorative and iterative fine-tuning. You can experiment on a small subset of data, trying different hyperparameters to figure out the ideal configuration for the best performance. Thanks to the interactive setup, you can analyze the intermediate results to gain deeper insights into the LLM's behavior and response to different training parameters. Therefore, notebooks are strongly recommended for early-stage exploration and hyperparameter tuning. Jobs: Reliable and Scalable Once you've identified the optimal hyperparameters and configuration through experimentation, transitioning to a deployment job helps you fine-tune on whole dataset and facilitates rapid and reliable training. It ensures consistent and reproducible training runs, as well as built-in retry mechanisms automatically handle any hiccups, ensuring seamless training without manual intervention Consequently, deployment jobs are the preferred choice for large-scale LLM finetuning, particularly when the optimal configuration has been established through prior experimentation. Hyperparameters Fine-tuning an LLM requires adjusting key parameters to optimize its performance on your specific task. Here are some crucial hyperparameters to consider: Epochs : This determines the number of times the model iterates through the entire training dataset. Too many epochs can lead to overfitting, and too few might leave the model undertrained. You should start with a moderate number and increase until the validation performance starts dropping. Learning Rate : This defines how quickly the model updates its weights based on errors. Too high can cause instability and poor performance, and too low can lead to slow learning. Start small and gradually increase if the finetuning is slow. Batch Size : This controls how many data points the model processes before adjusting its internal parameters. Choose a size based on memory constraints and desired training speed. Too high can strain resources, and too low might lead to unstable updates. Lora Alpha and R : These control the adaptive scaling of weights in the Lora architecture, improving efficiency for large models. These are useful parameters for generalization. High values might lead to instability, low values might limit potential performance. Max Length : This defines the maximum sequence length the model can process at once. Choose based on your task's typical input and output lengths. Too short can truncate context, and too long can strain resources and memory. The optimal values for these hyperparameters depend on your specific LLM, task, and dataset. Be prepared to experiment and iteratively refine your settings for optimal performance. Fine-Tuning using a Notebook Fine-Tuning using a Job Before you start, you will first need to create an ML Repo (this will be used to store your training metrics and artifacts, such as your checkpoints and models) and give your workspace access to the ML Repo. You can read more about ML Repo's here Now that your ML Repo is set up, you can create the fine-tuning job. Deploying the Fine-Tuned Model Once your Fine-tuning is complete, the next step is to deploy the fine-tuned LLM. You can learn more about how to send requests to your Deploy LLM using the following guide Advanced: Merging LoRa Adapters and uploading merged model Currently the finetuning Job/Notebook only converts the best checkpoint to a merged model to save GPU compute time. But sometimes we might want to pick an intermediate checkpoint, merge it and re-upload it as a model. \ud83d\udea7 Upcoming Improvments We are working on building this feature as part of the platform. Till then the code in this section can be run locally or in a Notebook on the platform First, let's make sure we have following requirements installed --extra-index-url https://download.pytorch.org/whl/cu121 torch==2.3.0+cu121 tokenizers==0.19.1 transformers==4.42.3 accelerate==0.31.0 peft==0.11.1 truefoundry[ml]>=0.5.5,<1.0.0 Make sure you are logged in to TrueFoundry tfy login --host <Your TrueFoundry Platform URL> Next, save this script merge_and_upload.py import os import math import os import re import shutil from typing import Any, Dict, Optional import numpy as np from huggingface_hub import scan_cache_dir from truefoundry import ml as mlfoundry import argparse import json from truefoundry.ml import get_client from transformers import AutoModelForCausalLM, AutoTokenizer from peft import PeftModel def get_or_create_run( ml_repo: str, run_name: str, auto_end: bool = False, create_ml_repo: bool = False ): client = mlfoundry.get_client() if create_ml_repo: client.create_ml_repo(ml_repo=ml_repo) try: run = client.get_run_by_name(ml_repo=ml_repo, run_name=run_name) except Exception as e: if \"RESOURCE_DOES_NOT_EXIST\" not in str(e): raise run = client.create_run(ml_repo=ml_repo, run_name=run_name, auto_end=auto_end) return run def log_model_to_mlfoundry( run: mlfoundry.MlFoundryRun, model_name: str, model_dir: str, hf_hub_model_id: str, metadata: Optional[Dict[str, Any]] = None, step: int = 0, ): metadata = metadata or {} print(\"Uploading Model...\") hf_cache_info = scan_cache_dir() files_to_save = [] for repo in hf_cache_info.repos: if repo.repo_id == hf_hub_model_id: for revision in repo.revisions: for file in revision.files: if file.file_path.name.endswith(\".py\"): files_to_save.append(file.file_path) break # copy the files to output_dir of pipeline for file_path in files_to_save: match = re.match(r\".*snapshots\\/[^\\/]+\\/(.*)\", str(file_path)) if match: relative_path = match.group(1) destination_path = os.path.join(model_dir, relative_path) os.makedirs(os.path.dirname(destination_path), exist_ok=True) shutil.copy(str(file_path), destination_path) else: print(\"Python file in hf model cache in unknown path:\", file_path) metadata.update( { \"pipeline_tag\": \"text-generation\", \"library_name\": \"transformers\", \"base_model\": hf_hub_model_id, \"huggingface_model_url\": f\"https://huggingface.co/{hf_hub_model_id}\" } ) metadata = { k: v for k, v in metadata.items() if isinstance(v, (int, float, np.integer, np.floating)) and math.isfinite(v) } run.log_model( name=model_name, model_file_or_folder=model_dir, framework=mlfoundry.ModelFramework.TRANSFORMERS, metadata=metadata, step=step, ) print(f\"You can view the model at {run.dashboard_link}?tab=models\") def merge_and_upload( hf_hub_model_id: str, ml_repo: str, run_name: str, artifact_version_fqn: str, saved_model_name: str, dtype: str = \"bfloat16\", device_map: str = \"auto\", ): import torch client = get_client() if device_map.startswith(\"{\"): device_map = json.loads(device_map) artifact_version = client.get_artifact_version_by_fqn(artifact_version_fqn) lora_model_path = artifact_version.download() tokenizer = AutoTokenizer.from_pretrained(hf_hub_model_id) model = AutoModelForCausalLM.from_pretrained(hf_hub_model_id, device_map=device_map, torch_dtype=getattr(torch, dtype)) model = PeftModel.from_pretrained(model, lora_model_path) model = model.merge_and_unload(progressbar=True) merged_model_dir = os.path.abspath(\"./merged\") os.makedirs(merged_model_dir, exist_ok=True) tokenizer.save_pretrained(merged_model_dir) model.save_pretrained(merged_model_dir) run = get_or_create_run( ml_repo=ml_repo, run_name=run_name, auto_end=False, create_ml_repo=False, ) log_model_to_mlfoundry( run=run, model_name=saved_model_name, model_dir=merged_model_dir, hf_hub_model_id=hf_hub_model_id, metadata={ \"checkpoint\": artifact_version_fqn, }, step=artifact_version.step, ) def main(): parser = argparse.ArgumentParser() parser.add_argument(\"--hf_hub_model_id\", type=str, required=True) parser.add_argument(\"--ml_repo\", type=str, required=True) parser.add_argument(\"--run_name\", type=str, required=True) parser.add_argument(\"--artifact_version_fqn\", type=str, required=True) parser.add_argument(\"--saved_model_name\", type=str, required=True) parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", choices=[\"bfloat16\", \"float16\", \"float32\"]) parser.add_argument(\"--device_map\", type=str, default=\"auto\") args = parser.parse_args() merge_and_upload( hf_hub_model_id=args.hf_hub_model_id, ml_repo=args.ml_repo, run_name=args.run_name, artifact_version_fqn=args.artifact_version_fqn, saved_model_name=args.saved_model_name, dtype=args.dtype, device_map=args.device_map, ) if __name__ == \"__main__\": main() Finally, run this script. Run --help to see help python merge_and_upload.py --help usage: merge_and_upload.py [-h] --hf_hub_model_id HF_HUB_MODEL_ID --ml_repo ML_REPO --run_name RUN_NAME --artifact_version_fqn ARTIFACT_VERSION_FQN --saved_model_name SAVED_MODEL_NAME [--dtype {bfloat16,float16,float32}] [--device_map DEVICE_MAP] options: -h, --help show this help message and exit --hf_hub_model_id HF_HUB_MODEL_ID HuggingFace Hub model id to merge the LoRa adapter with. E.g. `stas/tiny-random-llama-2` --ml_repo ML_REPO ML repo to log the merged model to --run_name RUN_NAME Name of the run to log the merged model to. If the run does not exist, it will be created. --artifact_version_fqn ARTIFACT_VERSION_FQN Artifact version FQN of the LoRa adapter to merge with the HF model --saved_model_name SAVED_MODEL_NAME Name of the model to log to MLFoundry --dtype {bfloat16,float16,float32} Data type to load the base model --device_map DEVICE_MAP device_map to use when loading the model. auto, cpu, or a dictionary of device_map E.g. usage which merges checkpoint artifact:truefoundry/llm-experiments/ckpt-finetune-2024-03-14T05-00-55:7 with its base model stas/tiny-random-llama-2 and uploads it as finetuned-tiny-random-llama-checkpoint-7 to run finetune-2024-03-14T05-00-55 in ML Repo llm-experiments Grab the checkpoint's artifact version FQN from the Artifacts Tab of your Job Run Output python merge_and_upload.py \\ --hf_hub_model_id stas/tiny-random-llama-2 \\ --ml_repo llm-experiments \\ --run_name finetune-2024-03-14T05-00-55 \\ --artifact_version_fqn artifact:truefoundry/llm-experiments/ckpt-finetune-2024-03-14T05-00-55:7 \\ --saved_model_name finetuned-tiny-random-llama-checkpoint-7 \\ --dtype bfloat16 \\ --device_map auto Updated 2 months ago",
    "https://docs.truefoundry.com/docs/benchmarking-llms": "Benchmarking LLMs Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Benchmarking LLMs All Pages Start typing to search\u2026 Benchmarking LLMs Measure token generation throughput, Time to First Token (TTFT), Inter Token Latency of LLMs via the chat completions API Deploying the LLM Benchmarking Tool The LLM Benchmarking Tool allows you to measure key performance metrics of language models, including token generation throughput, Time to First Token (TTFT), and Inter Token Latency. Deployment via Application Catalog The simplest way to deploy the LLM Benchmarking Tool is through the Application Catalog: Navigate to the Application Catalog and select \"Benchmark LLM performance\" Fill in the required deployment information, including Name and Host endpoint for your benchmarking tool. Model Configuration Basic Configuration Parameters Load Test Parameters Number of users (peak concurrency) : Maximum number of concurrent users for the load test Ramp up (users started/second) : Rate at which new users are added to the test Host : Base URL for LLM API Server with OpenAI compatible endpoints (v1/chat/completions) Model Settings Tokenizer (HuggingFace tokenizer to use to count tokens) : HuggingFace tokenizer identifier used to count tokens in prompts and responses Model (Name of the model in chat/completions payload) : Model identifier used in the API request payload Finding Parameters for TrueFoundry Deployed Models When using a model deployed on TrueFoundry, you can find the required parameters in the model's deployment spec: Model Name : Look under the env section for MODEL_NAME YAML env: MODEL_NAME: nousresearch-meta-llama-3-1-8b-instruct Host : Find the host URL under the ports section YAML ports: - host: example-model.truefoundry.tech Tokenizer : Look for the model_id under artifacts_download.artifacts YAML artifacts_download: artifacts: - type: huggingface-hub model_id: NousResearch/Meta-Llama-3.1-8B-Instruct Finding Parameters for External Models When using external models (like GPT-4), you'll need to configure the following parameters: Model Name : Use the model identifier from your provider (e.g., gpt-4 , gpt-4o ) Tokenizer : Find an equivalent tokenizer on HuggingFace (e.g., Quivr/gpt-4o ) Host : Your external provider's API endpoint API Key : Your provider's API key (e.g., OpenAI API key) Finding Parameters for TrueFoundry LLM Gateway Models When using models through TrueFoundry's LLM Gateway: Navigate to the LLM Gateway in your TrueFoundry workspace Select the model you want to benchmark Click on the </> Code button to view the API integration code From the code example, you can find: Host : The base URL in the request (e.g., https://truefoundry.tech/api/llm/api/inference/openai/chat/completions ) Model Name : The model identifier in the request payload (e.g., \"model\": \"openai-main/gpt-4o\" ) OpenAI API Key : Generate one using the \"Generate API Key\" button Tokenizer : Find an equivalent tokenizer on HuggingFace (e.g., Quivr/gpt-4o ) Prompt Configuration Max Output Tokens : Maximum number of tokens allowed in the model's response Use Random Prompts : Whether to use randomly generated prompts for testing Use Single Prompt : Whether to use a single prompt for all test requests Ignore EOS : Whether to ignore end-of-sequence tokens during token counting Prompt Min Tokens : Minimum number of tokens in the input prompt (not used with random or single prompts) Prompt Max Tokens : Maximum number of tokens in the input prompt (not used with random or single prompts) Viewing Benchmark Results After running the benchmark, you'll see comprehensive performance metrics displayed in charts: Requests per Second Active Users Tokens per Second Response Time Seconds Response Time First Token (ms) Inter Token Latency (ms) Updated 19 days ago",
    "https://docs.truefoundry.com/docs/intro-to-llm-gateway": "Intro to LLM Gateway Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Intro to LLM Gateway All Pages Start typing to search\u2026 Intro to LLM Gateway LLM Gateway provides a unified interface to manage your organization's LLM usage. Below are the key features that enhance functionality and security: Unified API : Access multiple LLM providers through a single OpenAI compatible interface, requiring no code changes. API Key Security : Secure and centralized management of credentials. Governance & Control : Set limits, enforce access controls, and apply content filtering to manage how LLMs are used within your organization. Rate Limiting : Implement measures to prevent abuse and ensure fair usage across users. Observability : Track and analyse usage, costs, latency, and overall performance. Cost Management : Monitor spending and configure budget alerts to keep expenses under control. Audit Trails : Maintain logs of all interactions with LLMs to support compliance and auditing requirements. LLM Playground The LLM Playground is a UI for the LLM Gateway where you can tryout different models you've added from across providers like OpenAI, Mistral, Cohere etc. Below is an overview of the features: Support for multiple model types Chat Models Embedding Models Rerank Models Realtime Models Image Upload : Upload images for image captioning or visual question answering. This is only available for models that support images such as GPT-4o. Model Comparison : Compare responses from different completion models to evaluate their performance. System Prompts: Use predefined system prompts to guide model behaviour. System prompt inform how the model should respond. Sample system prompt - Be clear, concise, and polite in your responses. Avoid sharing any sensitive or personal information. Benchmarking Results (LLM Gateway is Blazing Fast!) Near-Zero Overhead: TrueFoundry LLM Gateway adds only extra 3 ms in latency upto 250 RPS and 4 ms at RPS > 300. Scalability: LLM Gateway can scale without any degradation in performance until about 350 RPS with 1 vCPU & 1 GB machine before the CPU utilisation reaches 100% and latencies start to get affected. With more CPU or more replicas, the LLM Gateway can scale to tens of thousands of requests per second. High Capacity: A t2.2xlarge AWS instance (43$ per month on spot) machine can scale upto ~3000 RPS with no issues. Edge Ready: Deploy close to your apps Gateway can be deployed on the edge, close to your applications \ud83d\ude80 Learn more on LLM Gateway Benchmarks here: Read more Unified API The Unified API in the TrueFoundry LLM Gateway provides a standardised interface for accessing and interacting with different language models from various providers. This means you can seamlessly switch between models and providers without changing your application's code structure. By abstracting the underlying complexities, the Unified API simplifies the process of integrating multiple models and ensures consistency in how they are accessed and utilised. Key Features of the Unified API Standardisation : The Unified API standardises requests and responses across different models, making it easier to manage and integrate multiple models. Flexibility : Easily switch between different models and providers without altering the core application code. Efficiency : Streamline the development process by using a single API to interact with various models, reducing the need for multiple integrations and bespoke handling. Compatibility : By adhering to the OpenAI request-response format, the Unified API ensures seamless integration with Python libraries like OpenAI and LangChain. Supported Providers Below is a comprehensive list of popular LLM providers that is supported by TrueFoundry LLM Gateway. Provider Streaming Supported How to add models from this provider? GCP \u2705 https://docs.truefoundry.com/docs/integration-provider-gcp#google-vertex-model-integration AWS \u2705 https://docs.truefoundry.com/docs/integration-provider-aws#policies-required-for-bedrock-integration Azure OpenAI \u2705 By using provider API Key in TrueFoundry Integrations Self Hosted Models on TrueFoundry \u2705 https://docs.truefoundry.com/docs/add-self-hosted-model-to-gateway OpenAI \u2705 By using provider API Key in TrueFoundry Integrations Cohere \u2705 By using provider API Key in TrueFoundry Integrations AI21 \u2705 By using provider API Key in TrueFoundry Integrations Anthropic \u2705 By using provider API Key in TrueFoundry Integrations Anyscale \u2705 By using provider API Key in TrueFoundry Integrations Together AI \u2705 By using provider API Key in TrueFoundry Integrations DeepInfra \u2705 By using provider API Key in TrueFoundry Integrations Ollama \u2705 By using provider API Key in TrueFoundry Integrations Palm \u2705 By using provider API Key in TrueFoundry Integrations Perplexity AI \u2705 By using provider API Key in TrueFoundry Integrations Mistral AI \u2705 By using provider API Key in TrueFoundry Integrations Groq \u2705 By using provider API Key in TrueFoundry Integrations Nomic \u2705 By using provider API Key in TrueFoundry Integrations Updated 26 days ago",
    "https://docs.truefoundry.com/docs/openai-anthropic-deepseek-and-more": "OpenAI, Anthropic, DeepSeek and More Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account OpenAI, Anthropic, DeepSeek and More All Pages Start typing to search\u2026 OpenAI, Anthropic, DeepSeek and More TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on OpenAI, DeepSeek, Anthropic, AWS Bedrock, Google Vertex, Azure OpenAI and many more. View Full List Providers and Models Before we start integrating LLMs, let's quickly understand the concept of a Provider and a Model: Provider - A provider is an organization or company that supplies access to various AI language models. These providers offer multiple models, each varying in performance, features, and pricing. Prominent examples of such providers include OpenAI and Anthropic, which deliver a range of models designed to meet different needs. Model - A model refers to a specific implementation of an AI language model made available by a provider. Each model is tailored with unique features and trained on distinct datasets, which influences its performance and suitability for various tasks. For example, OpenAI's GPT-3 and GPT-4 are different models, each offering unique capabilities to cater to diverse applications. Steps to Add OpenAI Models to Gateway This section outlines the process of adding OpenAI models and configuring the necessary access controls. Access Control In the example above, all users have access to every model. However, access control can be configured at the individual model level. Adding access control through the UI When adding a model, you have the option to configure which users and teams within the organization are permitted to access that model. This ensures that only authorized personnel can use specific models based on your access control settings. You can also give model access to a Virtual Account Read more here Tryout your Integrated Models Once you've added the providers and models through the integrations, you're ready to test them out. Try in the Playground To test your models in the playground, simply visit the playground interface and select the model from the available models dropdown. You can search for your models by the provider account name or model name, as shown below: Try Programatically TrueFoundry provides several options for testing models programmatically, including Curl, REST APIs, and platform libraries such as OpenAI, Langchain, and more. To access code snippets for integration: Visit Gateway. Select a model. Click the \"View Code\" button to see auto-generated code snippets for different libraries, REST APIs, or simple Curl requests, as shown below. To make a request, you'll need the BASE_URL and API_KEY . The BASE_URL is pre-filled in the auto-generated code snippets. For instructions on generating a new API_KEY , refer to the API Key Documentation . Add LLMs from Anthropic, DeepSeek, Ollama and more The process is pretty much the same. Follow these steps - Navigate to Integrations. Click on \"Add New Integration\". Select the provider from the list. Enter the required API keys and access credentials. Once completed, your models will be ready for use in both the playground and your code. Updated 26 days ago",
    "https://docs.truefoundry.com/docs/aws-bedrock": "AWS Bedrock Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account AWS Bedrock All Pages Start typing to search\u2026 AWS Bedrock TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on AWS Bedrock. Adding Models This section explains the steps to add AWS Bedrock models and configure the required access controls. From the TrueFoundry dashboard, navigate to Integrations > Add Provider Integration and select AWS . Complete the form with your AWS account details, including the authentication information (Access Key + Secret) or Assume Role ID. Select the Bedrock Model option and provide the model ID and other required details to add one or more model integrations. Inference After adding the models, you can perform inference using an OpenAI-compatible API via the TrueFoundry LLM Gateway. For instance, you can directly utilize the OpenAI library as well.: Python from openai import OpenAI client = OpenAI(api_key=\"Enter your API Key here\", base_url=\"https://llm-gateway.truefoundry.com/api/inference/openai\") stream = client.chat.completions.create( messages = [ {\"role\": \"system\", \"content\": \"You are an AI bot.\"}, {\"role\": \"user\", \"content\": \"Enter your prompt here\"}, ], model= \"bedrock-provider/llama-70b\", ) Supported Models A list of models supported by AWS Bedrock, along with their corresponding model IDs, can be found here: View Full List The TrueFoundry LLM Gateway supports all text and image models in Bedrock. We are also working on adding support for additional modalities, including speech, in the near future. Extra Parameters Internally, the TrueFoundry LLM Gateway utilizes the Bedrock Converse API for chat completion. To pass additional input fields or parameters, such as top_k, frequency_penalty, and others specific to a model, include them using this key: JSON \"additionalModelRequestFields\": { \"frequency_penalty\": 0.5 } Cross-Region Inference To manage traffic during on-demand inferencing by utilising compute across regions, you can use AWS Bedrock Cross-Region Inference. While setting model ID in TrueFoundry, use the Inference Profile ID instead of the model ID. You can more information about cross-region inferencing here . Use Inference Profile ID as model ID while adding model to TFY LLM Gateway Authentication Methods Using AWS Access Key and Secret Create an IAM user (or choose an existing IAM user) following these steps . Add required permission for this user. The following policy grants permission to invoke all model JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Sid\": \"InvokeAllModels\", \"Action\": [ \"bedrock:InvokeModel\", \"bedrock:InvokeModelWithResponseStream\" ], \"Resource\": [ \"arn:aws:bedrock:us-east-1::foundation-model/*\" ] } ] } Create an access key for this user as per this doc . Use this access key and secret while adding the provider account to authenticate requests to the Bedrock model. Using Assumed Role You can also directly specify a role that can be assumed by the service account attached to the pods running LLM Gateway. Read more about how assumed roles work here . Using Bedrock Guardrails Create a Guardrail in AWS. More information at this link - https://aws.amazon.com/bedrock/guardrails Copy the Guardrails ID and the version number While calling a AWS bedrock model through TFY LLM Gateway, pass the following object along with it: Extra Params \"guardrailConfig\": { \"guardrailIdentifier\": \"your-guardrail-id\", \"guardrailVersion\": \"1\" } This should ensure the response will have guardrails enforced. Consider this input where the guardrail is configured to censor PII like name, email etc.: sample input { \"model\": \"internal-bedrock/claude-3\", \"messages\": [ { \"role\": \"user\", \"content\": \"What are some ideas for email for Elon Musk?\" } ], \"guardrailConfig\": { \"guardrailIdentifier\": \"xyz-123-768\", \"guardrailVersion\": \"1\" } } Sample output: expected output { \"id\": \"1741339101780\", \"object\": \"chat.completion\", \"created\": 1741339101, \"model\": \"\", \"provider\": \"aws\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Here are some ideas for email addresses for {NAME}:\\n\\n1. {EMAIL}\\n2. {EMAIL}\\n3. {EMAIL}\\n4. {EMAIL}\\n5. {EMAIL}\\n6. {EMAIL} (or any relevant year)\\n7. {EMAIL}\\n8. {EMAIL}\\n9. {EMAIL}\\n10. {EMAIL}\\n11. {EMAIL}\\n12. {EMAIL}\\n13. {EMAIL}\\n14. {EMAIL}\\n15. {EMAIL}\\n\\nWhen creating an email address, consider the following tips:\\n\\n1. Keep it professional if it's for work purposes.\\n2. Make it easy to spell and remember.\\n3. Avoid using numbers or special characters unless necessary.\\n4. Consider using a combination of first name, last name, or initials.\\n5. You can use different email addresses for personal and professional purposes.\\n\\nRemember to replace \\\"example.com\\\" with the actual domain you'll be using for your email address.\" }, \"finish_reason\": \"guardrail_intervened\" } ], \"usage\": { \"prompt_tokens\": 25, \"completion_tokens\": 320, \"total_tokens\": 345 } } If you're using a library like Langchain, you might have to pass the extra param in a parameter like extra_body as required by the library. For example, refer this Langchain OpenAI class doc . Updated 26 days ago",
    "https://docs.truefoundry.com/docs/google-vertex": "Google Vertex Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Google Vertex All Pages Start typing to search\u2026 Google Vertex TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on Google Vertex. Adding Models This section explains the steps to add Google Vertex models and configure the required access controls. From TrueFoundry dashboard, go to Integrations > Add Provider Integration and choose Google Add Authentication details by providing Service Account JSON. More details on permission required below. Click on Vertex Model and fill in model ID, region and other fields to add model integrations. Read below for more information on how to obtain the model ID from Vertex. Adding Vertex Models to LLM Gateway Authentication To authenticate Google Vertex model you need to provide: Project ID - Project ID can be found from the top right corner in your console. Service Account JSON - You can generate a service account JSON using the steps mentioned here . The corresponding service account needs to have an IAM role with the following permission: [ \"aiplatform.endpoints.predict\" ] Supported Models Currently TrueFoundry LLM Gateway supports text and embedding models from Vertex AI. Some of these models support function calling and sending files like images or documents attached to their messages and TrueFoundry LLM Gateway also supports the same. Support for Vertex AI models like image generation, audio are coming soon on TrueFoundry LLM Gateway. Model ID While adding models to LLM Gateway, you need to provide the Vertex Model ID. This can be obtained from the models page on Vertex. While adding Google models like Gemini or Gemma, you can simply provide the model ID in the form. For example, you'd fill gemini-1.5-flash for Gemini 1.5 Flash model. While adding models from other Vertex partners like Anthropic or Mistral, you need to provide the model ID in the <partner>/<vertex-model-id . For example, anthropic/claude-3-5-sonnet-v2@20241022 . Adding an anthropic model Updated 26 days ago",
    "https://docs.truefoundry.com/docs/azure-openai": "Azure OpenAI Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Azure OpenAI All Pages Start typing to search\u2026 Azure OpenAI TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on Azure OpenAI. Adding Models This section explains the steps to add Azure OpenAI models and configure the required access controls. From TrueFoundry dashboard, go to Integrations > Add Provider Integration and choose Azure Click on Add Integration and choose Azure OpenAI Model to add an Azure OpenAI model. Fill in the information including the API Key Supported Models Azure OpenAI integration supports all kinds of OpenAI models. To start using OpenAI models through Azure: Deploy Azure OpenAI resource and deploy a model. The instructions are available here . Once deployed, the model names can be found in the Deployments tab under Azure OpenAI Service in Azure AI Foundry. You can use the name from this list as Model ID on while filling TrueFoundry Integrations form. Listing of OpenAI models in Azure OpenAI Service Adding a model to TrueFoundry as an integration Updated 26 days ago",
    "https://docs.truefoundry.com/docs/reasoning-models-claude-only": "Reasoning Models (Claude Only) Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Reasoning Models (Claude Only) All Pages Start typing to search\u2026 Reasoning Models (Claude Only) Thinking/reasoning tokens will be available via TrueFoundry LLM Gateway for Claude 3.7 Sonnet (accessible through Anthropic, AWS Bedrock, and Google Vertex AI ). These models expose their internal reasoning process, allowing you to see how they arrive at conclusions. The thinking/reasoning tokens provide step-by-step insights into the model\u2019s cognitive process. OpenAI Compliance in Responses When the X-TFY-STRICT-OPENAI header is not included in the request, the response remains fully OpenAI-compliant. However, enabling thinking/reasoning tokens makes the response no longer OpenAI-compliant , as it introduces an additional reasoning layer that OpenAI\u2019s compliance framework does not support. A standard OpenAI-compliant response structure looks like this: JSON { \"id\": \"1742890436073\", \"object\": \"chat.completion\", \"created\": 1742890436, \"model\": \"\", \"provider\": \"aws\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"Hello! How can I assist you today? I'm here to provide information, answer questions, or help with tasks. Feel free to ask anything!\" }, \"finish_reason\": \"end_turn\" } ], \"usage\": { \"prompt_tokens\": 20, \"completion_tokens\": 120, \"total_tokens\": 140 } } Enabling Thinking/Reasoning Tokens To enable thinking/reasoning tokens , the request must include: The header: X-TFY-STRICT-OPENAI: false A thinking field in the request body: Example Request Body: JSON { \"messages\": [ { \"role\": \"user\", \"content\": \"How to compute 3^3^3?\" } ], \"model\": \"anthropic/claude-3-7\", \"thinking\": { \"type\": \"enabled\", \"budget_tokens\": 16000 }, \"max_tokens\": 18000, \"stream\": false } Example Response: JSON { \"id\": \"1742890579083\", \"object\": \"chat.completion\", \"created\": 1742890579, \"model\": \"\", \"provider\": \"aws\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": [ { \"type\": \"thinking\", \"thinking\": \"The user has asked a complex question about quantum mechanics. To provide a useful answer, I should first break down the core concepts and then explain them in simple terms before diving into advanced details.\" }, { \"type\": \"text\", \"text\": \"Quantum mechanics is a branch of physics that explains how particles behave at very small scales. Unlike classical physics, where objects have definite positions and velocities, quantum particles exist in a superposition of states until measured. Would you like a more detailed explanation or examples?\" } ] }, \"finish_reason\": \"end_turn\" } ], \"usage\": { \"prompt_tokens\": 45, \"completion_tokens\": 180, \"total_tokens\": 225 } } Streaming Responses with Thinking Tokens For streaming responses, the thinking section is always sent before the content section. Example Chunk - Thinking Token: JSON { \"id\": \"aws-1742890615621\", \"object\": \"chat.completion.chunk\", \"created\": 1742890615, \"model\": \"\", \"provider\": \"aws\", \"choices\": [ { \"index\": 0, \"delta\": { \"role\": \"assistant\", \"thinking\": \"The user is asking about the differences between AI and machine learning. I should start by defining AI in general and then narrow down to how ML fits into it.\" } } ] } Example Chunk - Content Token: JSON { \"id\": \"aws-1742890615621\", \"object\": \"chat.completion.chunk\", \"created\": 1742890615, \"model\": \"\", \"provider\": \"aws\", \"choices\": [ { \"index\": 0, \"delta\": { \"role\": \"assistant\", \"content\": \"Artificial Intelligence (AI) is a broad field of computer science focused on building systems that can perform tasks requiring human intelligence. Machine Learning (ML) is a subset of AI that enables computers to learn patterns from data and improve performance over time without explicit programming.\" } } ] } In a streaming response, the thinking chunk typically arrives first , followed by the content. Updated 26 days ago",
    "https://docs.truefoundry.com/docs/realtime-api": "Realtime Models Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Realtime Models All Pages Start typing to search\u2026 Realtime Models TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including realtime models hosted on various providers. Add Realtime Models to Gateway The process is pretty much the same. Follow these steps - Navigate to Integrations. Click on \"Add New Integration\". Select the provider from the list. Enter the required API keys and access credentials. Once completed, your models will be ready for use in both the playground and your code. Get Started using Rest API Python import asyncio import json import websockets BASE_URL = \"wss://llm-gateway.truefoundry.com/api/llm/api/inference/openai/v1/realtime?model=openai-main/realtime-preview\" headers = { \"Authorization\": f\"Bearer Enter your API Key here\", } async def connect(): async with websockets.connect(BASE_URL, additional_headers=headers) as ws: print(\"WebSocket connection established!\") # Can follow the openai realtime api docs # https://platform.openai.com/docs/api-reference/realtime # Update session session_update = { \"type\": \"session.update\", \"session\": { \"modalities\": [\"text\"], \"model\": \"openai-main/realtime-preview\", }, } await ws.send(json.dumps(session_update)) print(\"Session updated!\") # Send user message message = { \"type\": \"conversation.item.create\", \"item\": { \"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": \"Say hello!\"}], }, } await ws.send(json.dumps(message)) print(\"Conversation item created!\") # Request response response_request = {\"type\": \"response.create\"} await ws.send(json.dumps(response_request)) print(\"Response requested!\") # Read responses async for msg in ws: event = json.loads(msg) if event.get(\"type\") == \"response.text.delta\": print(event[\"delta\"], end=\"\", flush=True) elif event.get(\"type\") == \"response.text.done\": print(\" Response completed.\") elif event.get(\"type\") == \"response.done\": print(\"Session complete, closing connection.\") break asyncio.run(connect()) Get Started using NodeJS node // Requires: yarn add ws @types/ws const { OpenAIRealtimeWS } = require('openai/beta/realtime/ws'); const OpenAI = require('openai'); const openai = new OpenAI({ apiKey: 'Enter your API Key here', baseURL: 'wss://llm-gateway.truefoundry.com/api/llm/api/inference/openai/v1', }); const rt = new OpenAIRealtimeWS( { model: 'openai-main/realtime-preview', options: {} }, openai ); // Access the underlying WebSocket instance rt.socket.on('open', () => { console.log('Connection opened!'); // Can follow the openai realtime api docs // https://platform.openai.com/docs/api-reference/realtime rt.send({ type: 'session.update', session: { modalities: ['text'], model: 'openai-main/realtime-preview', }, }); console.log('Session updated!'); rt.send({ type: 'conversation.item.create', item: { type: 'message', role: 'user', content: [{ type: 'input_text', text: 'Say a couple paragraphs!' }], }, }); console.log('Conversation item created!'); rt.send({ type: 'response.create' }); console.log('Response created!'); }); rt.on('error', (err) => { console.error('WebSocket Error:', err); }); rt.on('session.created', (event) => { console.log('session created!', event.session); console.log(); }); rt.on('response.text.delta', (event) => process.stdout.write(event.delta)); rt.on('response.text.done', () => console.log()); rt.on('response.done', () => rt.close()); rt.socket.on('close', () => console.log(' Connection closed!')); Get Started using OpenAI Library Python import asyncio from openai import AsyncOpenAI async def main(): client = AsyncOpenAI( api_key=\"Enter your API Key here\", base_url=\"wss://llm-gateway.truefoundry.com/api/llm/api/inference/openai/v1\" ) try: async with client.beta.realtime.connect(model=\"openai-main/realtime-preview\") as connection: # Can follow the openai realtime api docs # https://platform.openai.com/docs/api-reference/realtime print(\"WebSocket connection established!\") # Update session await connection.session.update(session={'modalities': ['text']}) print(\"Session updated!\") # Send user message await connection.conversation.item.create( item={ \"type\": \"message\", \"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": \"Say hello!\"}], } ) print(\"Conversation item created!\") # Request response await connection.response.create() print(\"Response requested!\") # Read responses async for event in connection: if event.type == 'response.text.delta': print(event.delta, flush=True, end=\"\") elif event.type == 'response.text.done': print(\" Response completed.\") elif event.type == \"response.done\": print(\"Session complete, closing connection.\") break except Exception as e: print(f\"Error: {e}\") asyncio.run(main()) Get Started using Curl curl # we're using websocat for this example, but you can use any websocket client websocat \"wss://llm-gateway.truefoundry.com/api/llm/api/inference/openai/v1/realtime?model=openai-main/realtime-preview\" \\ -H 'Authorization: Bearer Enter your API Key here' # once connected, you can send your messages as you would with OpenAI's Realtime API Updated 26 days ago What\u2019s Next Request logging is coming soon",
    "https://docs.truefoundry.com/docs/self-hosted-models": "Self-hosted Models Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Self-hosted Models All Pages Start typing to search\u2026 Self-hosted Models TrueFoundry offers a solution to deploy open-source or private models and integrate them directly into the Gateway. This capability also extends to fine-tuned models and embedding models. Refer to our docs on Deploying LLMs From Model Catalog for how to deploy LLMs from our model catalogue in a few clicks. Tryout Self-hosted Model Follow these steps once you have your model deployed: Save costs on Self-hosted Model GPUs are costly, and running GPU-powered deployments can quickly drive up expenses. TrueFoundry offers built-in solutions to help reduce deployment costs. You can choose from two cost-saving options: Auto-shutdown based on inactivity: Set up automatic shutdown of deployment pods if no new requests are received within a specified time frame. Manual pause/resume: You can manually pause deployments that you anticipate won't be used for a certain period. In both scenarios, the model status will be displayed as \"paused\" on the Gateway, as shown below: Updated 24 days ago",
    "https://docs.truefoundry.com/docs/batch-predictions-with-truefoundry-llm-gateway": "Batch Predictions with TrueFoundry LLM Gateway Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Batch Predictions with TrueFoundry LLM Gateway All Pages Start typing to search\u2026 Batch Predictions with TrueFoundry LLM Gateway This guide explains how to perform batch predictions using TrueFoundry's LLM Gateway with different providers. Prerequisites TrueFoundry API Key Provider account configured in TrueFoundry (OpenAI or Vertex AI) Python environment with openai library installed Authentication All API requests require authentication using your TrueFoundry API key and provider integration name. This is handled through the OpenAI client configuration: Python from openai import OpenAI BASE_URL = \"https://internal.devtest.truefoundry.tech/api/llm\" API_KEY = \"your-truefoundry-api-key\" # Configure OpenAI client with TrueFoundry settings client = OpenAI( api_key=API_KEY, base_url=BASE_URL, ) Provider Specific Extra Headers When making requests, you'll need to specify provider-specific headers based on which LLM provider you're using: OpenAI Provider Headers Python extra_headers = { \"x-tfy-provider-name\": \"openai-provider-name\" # name of tfy provider integration } Vertex AI Provider Headers Python extra_headers = { \"x-tfy-provider-name\": \"google-provider-name\", # name of tfy provider integration \"x-tfy-vertex-storage-bucket-name\": \"your-bucket-name\", \"x-tfy-vertex-region\": \"your-region\", # e.g., \"europe-west4\" \"x-tfy-provider-model\": \"gemini-2-0-flash\" # or any other supported model } Input File Format The batch prediction system requires input files in JSONL (JSON Lines) format. Each line in the file must be a valid JSON object representing a single request. The file should not contain any empty lines or comments. JSONL Format Requirements Each line must be a valid JSON object No empty lines between JSON objects No trailing commas No comments Each line must end with a newline character Request Format Example of a valid JSONL file ( request.jsonl ): jsonl {\"custom_id\": \"request-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}} {\"custom_id\": \"request-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-3.5-turbo-0125\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an unhelpful assistant.\"},{\"role\": \"user\", \"content\": \"Hello world!\"}],\"max_tokens\": 1000}} {\"custom_id\": \"request-3\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4-vision-preview\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What's in this image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}]}], \"max_tokens\": 1000}} When using Vertex AI, you can skip method, url and body.model fields since they are not used. Workflow The batch prediction process involves four main steps: Upload input file Create batch job Check batch status Fetch results 1. Upload Input File Upload your JSONL file using the OpenAI client: Python # Upload the input file file = client.files.create( file=open(\"request.jsonl\", \"rb\"), purpose=\"batch\", extra_headers=extra_headers ) # The response will contain the file ID needed for creating the batch job print(file.id) # Example: file-PnFGrFLN5LjjcWr4eFsStK 2. Create Batch Job Create a batch job using the file ID from the upload step: Python batch_job = client.batches.create( input_file_id=file.id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\", extra_headers=extra_headers ) # The response includes a batch ID for tracking print(batch_job.id) # Example: batch_67f7bfc50b288190893f242d9fa47c52 3. Check Batch Status Monitor the batch job status: Python batch_status = client.batches.retrieve( batch_job.id, extra_headers=extra_headers ) print(batch_status.status) # Example: completed, validating, in_progress, etc. The status can be one of: validating : Initial validation of the batch in_progress : Processing the requests completed : All requests processed successfully failed : Batch processing failed 4. Fetch Results Once the batch is completed, fetch the results: Python if batch_status.status == \"completed\": output_content = client.files.content( batch_status.output_file_id, extra_headers=extra_headers ) print(output_content.content) Complete Example Here's a complete example that puts it all together: Python from openai import OpenAI # Initialize client client = OpenAI( api_key=\"your-api-key\", base_url=\"https://internal.devtest.truefoundry.tech/api/llm\", ) extra_headers = {\"x-tfy-provider-name\": \"openai-main\"} # 1. Upload file file = client.files.create( file=open(\"request.jsonl\", \"rb\"), purpose=\"batch\", extra_headers=extra_headers ) # 2. Create batch job batch_job = client.batches.create( input_file_id=file.id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\", extra_headers=extra_headers ) # 3. Check status batch_status = client.batches.retrieve( batch_job.id, extra_headers=extra_headers ) # 4. Fetch results when completed if batch_status.status == \"completed\": output_content = client.files.content( batch_status.output_file_id, extra_headers=extra_headers ) print(output_content.content) Best Practices Use meaningful custom_id values in your JSONL requests to track individual requests Implement proper error handling around API calls Monitor batch status regularly with appropriate polling intervals For Vertex AI: Ensure proper bucket permissions Use appropriate region settings Handle URL encoding for file IDs For OpenAI: Follow OpenAI's rate limits Use appropriate model parameters Store API keys securely and never hardcode them in your application Updated 13 days ago",
    "https://docs.truefoundry.com/docs/databricks-models": "Databricks Models Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Databricks Models All Pages Start typing to search\u2026 Databricks Models TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on Databricks. Adding Models This section explains the steps to add Databricks models and configure the required access controls. From the TrueFoundry dashboard, navigate to Integrations > Add Provider Integration and select OpenAI. Complete the form with your Databricks Token and Base url . Base url is of type - https://<workspace_id>.databricks.com/serving-endpoints . Select the Other OpenAI Model option and provide the model ID and other required details to add one or more model integrations. Supported Models A list of foundation models for Databricks, along with their corresponding model IDs, can be found here: View Full List Updated 14 days ago",
    "https://docs.truefoundry.com/docs/llm-playgound": "LLM Playgound Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account LLM Playgound All Pages Start typing to search\u2026 LLM Playgound The LLM Playground is a UI for the LLM Gateway where you can tryout different models you've added from across providers like OpenAI, Mistral, Cohere etc. Below is an overview of the features: Support for multiple model types Chat Models Embedding Models Rerank Models Realtime Models Image Upload : Upload images for image captioning or visual question answering. This is only available for models that support images such as GPT-4o. Model Comparison : Compare responses from different completion models to evaluate their performance. System Prompts: Use predefined system prompts to guide model behaviour. System prompt inform how the model should respond. Sample system prompt - Be clear, concise, and polite in your responses. Avoid sharing any sensitive or personal information. Updated 20 days ago",
    "https://docs.truefoundry.com/docs/integrate-gateway-to-your-apps": "Integrate Gateway to your Apps Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integrate Gateway to your Apps All Pages Start typing to search\u2026 Integrate Gateway to your Apps The Gateway allows you to generate code for calling the unified API in various languages and SDKs. Code generation in the TrueFoundry LLM Gateway automates the creation of the boilerplate code needed to integrate language models into your applications. The demo below demonstrates how to call any model from any provider using a standardized code through the Gateway. This feature significantly speeds up development by providing ready-to-use code snippets tailored to your specific needs and the models you are using. Updated 19 days ago",
    "https://docs.truefoundry.com/docs/gateway-access-control": "Access Control Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Access Control All Pages Start typing to search\u2026 Access Control Control access of models among teams, users and applications You can add models to LLM Gateway by adding provider accounts like OpenAI, Anthropic, Bedrock etc through the Integrations page. Each model provider can have multiple models within and you can configure access control at the model level. Manage access to models for users and teams You can grant access to users and teams via the integration form as shown in the following demo: To get access to models, users can generate a personal access tokens from the Settings page and use it in the API code. \ud83d\udcd8 When you provide access to a user, all their Private Access Tokens(PATs) get access to the model. Manage access to models for Virtual Accounts When applications are using LLMs via the gateway, its better to get the keys via virtual accounts. Virtual accounts are not tied to a user and hence remain valid even if the employee leaves the company. You can grant access to virtual account via the virtual account form as shown in the following demo: Updated 26 days ago",
    "https://docs.truefoundry.com/docs/ratelimiting": "Rate Limiting Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Rate Limiting All Pages Start typing to search\u2026 Rate Limiting RateLimiting configuration allows you to rate limit requests to a specified tokens/requests per minute/hour/day for certain sets of requests. The rate limiting configuration contains an array of rules. Every request is evaluated against the set of rules, and only the first matching rule is applied\u2014subsequent rules are ignored. So keep generic ones at bottom, specialised configs at top. For each rule, we have three sections: when subjects: An array of user, teams or virutal accounts from which requests is originated - for e.g. user: [email protected] , team:team1, virtualaccount:virtualaccountname models: An array of model ids which will be used to filter the requests. The model ids are the same as what we pass in the model field in the request. metadata. limit_to : Integer value which along with unit specifies the limit (for e.g. 100000 tokens per minute) unit : Possible values are requests_per_minute, requests_per_hour, requests_per_day, tokens_per_minute, tokens_per_hour, tokens_per_day Let's say you want to rate limit requests based on the following rules: Limit all requests to gpt4 model from openai-main account for user: [email protected] to 1000 requests per day Limit all requests to gpt4 model for team:backend to 20000 tokens per minute Limit all requests to gpt4 model for virtualaccount:virtualaccount1 to 20000 tokens per minute Limit all models to have a limit of 1000000 tokens per day Limit all users to have a limit of 1000000 tokens per day Limit all users to have a limit of 1000000 tokens per day for each model Your rate limit config would look like this: YAML name: ratelimiting-config type: gateway-rate-limiting-config # The rules are evaluated in order, and all matching rules are considered. # If any one of them causes a rate limit, the corresponding ID will be returned. rules: # Limit all requests to gpt4 model from openai-main account for user: [email protected] to # 1000 requests per day - id: \"openai-gpt4-dev-env\" when: subjects: [\"user: [email protected] \"] models: [\"openai-main/gpt4\"] limit_to: 1000 unit: requests_per_day # Limit all requests to gpt4 model for team:backend to 20000 tokens per minute - id: \"openai-gpt4-dev-env\" when: subjects: [\"team:backend\"] models: [\"openai-main/gpt4\"] limit_to: 20000 unit: tokens_per_minute # Limit all requests to gpt4 model for virtualaccount:virtualaccount1 to 20000 tokens per minute - id: \"openai-gpt4-dev-env\" when: subjects: [\"virtualaccount:virtualaccount1\"] models: [\"openai-main/gpt4\"] limit_to: 20000 unit: tokens_per_minute # Limit all models to have a limit of 1000000 tokens per day - id: \"{model}-daily-limit\" when: {} limit_to: 1000000 unit: tokens_per_day # Limit all users to have a limit of 1000000 tokens per day - id: \"{user}-daily-limit\" when:{} limit_to: 1000000 # Limit all users to have a limit of 1000000 tokens per day for each model - id: \"{user}-{model}-daily-limit\" when: {} limit_to: 1000000 unit: tokens_per_day Configure Ratelimit on Gateway It's straightforward\u2014simply go to the Config tab in the Gateway, add your configuration, and save. Example to Setup Rate Limits for User, Teams and Virtual Accounts TrueFoundry allows you to setup rate limit for specific users, teams and virtual accounts. Setup rate limit for users Say you want to limit all requests to gpt4 model from openai-main account for users [email protected] and [email protected] to 1000 requests per day YAML name: ratelimiting-config type: gateway-rate-limiting-config # The rules are evaluated in order, and all matching rules are considered. # If any one of them causes a rate limit, the corresponding ID will be returned. rules: # Limit all requests to gpt4 model from openai-main account for user: [email protected] and user: [email protected] to 1000 requests per day - id: \"openai-gpt4-dev-env\" when: subjects: [\"user: [email protected] \", \"user: [email protected] \"] models: [\"openai-main/gpt4\"] limit_to: 1000 unit: requests_per_day Setup rate limit for teams Say you want to limit all requests for team frontend to 5000 requests per day YAML name: ratelimiting-config type: gateway-rate-limiting-config # The rules are evaluated in order, and all matching rules are considered. # If any one of them causes a rate limit, the corresponding ID will be returned. rules: # Limit all requests for team frontend to 5000 requests per day - id: \"openai-gpt4-dev-env\" when: subjects: [\"team:frontend\"] limit_to: 5000 unit: requests_per_day Setup rate limit for virtual accounts Say you want to limit all requests for virtual account va-james to 1500 requests per day YAML name: ratelimiting-config type: gateway-rate-limiting-config # The rules are evaluated in order, and all matching rules are considered. # If any one of them causes a rate limit, the corresponding ID will be returned. rules: # Limit all requests for virtual account va-james to 1500 requests per day - id: \"openai-gpt4-dev-env\" when: subjects: [\"virtualaccount:va-james\"] limit_to: 1500 unit: requests_per_day Updated 20 days ago",
    "https://docs.truefoundry.com/docs/load-balancing": "Load Balancing Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Load Balancing All Pages Start typing to search\u2026 Load Balancing Loadbalance requests between multiple LLMs Load balancing allows users to distribute traffic across multiple models to optimize performance, prevent overload, and ensure high availability. This is especially useful when multiple model providers are available, and we want to distribute requests efficiently based on predefined rules. The load balancing configuration consists of an array of rules. Every request is evaluated against the set of rules, and only the first matching rule is applied\u2014subsequent rules are ignored. Each rule consists of three main sections: when subjects: An array of users or teams originating the request. For example, user:bob, team:team1. models: An array of model IDs to filter requests. The model IDs correspond to those used in the request. metadata: Additional key-value pairs for flexible matching. load_balance_targets : target: The model ID to which traffic should be routed. weight: A numerical value indicating the proportion of traffic that should be routed to the target model. The sum of all weights across different targets should ideally add up to 100, ensuring proper distribution. override_params (optional): A key-value object used to modify or extend the request body parameters when forwarding requests to the target model. Each key corresponds to a parameter name, and the associated value specifies what should be sent to the target. This allows you to override existing parameters from the original request or introduce entirely new ones, enabling flexible customization for each target model during load balancing Let's say you want to setup load balancing based on following rules: Distribute traffic for gpt4 model from openai-main account for user: [email protected] with 70% to azure-gpt4 and 30% to openai-main-gpt4. The azure target also overrides a few request parameters like temperature and max_tokens. Distribute traffic for llama model from bedrock for customer1 with 60% to azure-bedrock and 40% to aws-bedrock Your load balancing config would look like this: YAML name: loadbalancing-config type: gateway-load-balancing-config # The rules are evaluated in order and once a request matches one rule, # the subsequent rules are not checked rules: # Distribute traffic for gpt4 model from openai-main account for user:bob with 70% to azure-gpt4 and 30% to openai-main-gpt4. The azure target also overrides a few request parameters like temperature and max_tokens - id: \"openai-gpt4-dev-env\" when: subjects: [\"user: [email protected] \"] models: [\"openai-main/gpt4\"] metadata: env: dev load_balance_targets: - target: \"azure/gpt4\" weight: 70 override_params: temperature: 0.5 max_tokens: 800 top_p: 0.9 - target: \"openai-main/gpt4\" weight: 30 # Distribute traffic for llama model from bedrock for customer1 with 60% to azure-bedrock and 40% to aws-bedrock - id: \"llama-bedrock-customer1\" when: models: [\"bedrock/llama3\"] metadata: customer-id: customer1 load_balance_targets: - target: \"azure/bedrock-llama3\" weight: 60 - target: \"aws/bedrock-llama3\" weight: 40 Configure Load Balancing on Gateway It's straightforward\u2014simply go to the Config tab in the Gateway, add your configuration, and save. Updated 3 days ago",
    "https://docs.truefoundry.com/docs/canary-testing": "Canary Testing Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Canary Testing All Pages Start typing to search\u2026 Canary Testing You can use TrueFoundry\u2019s AI Gateway to canary test new models or prompts across different environments. This approach leverages load balancing techniques to achieve a different goal: testing new versions or configurations with a small portion of traffic before full deployment. Example: Test aws/bedrock-llama3 on 10% of the traffic Suppose you want to introduce aws/bedrock-llama3 into your system but are unsure of its impact. You can create a configuration specifically for testing Llama3 in a staging environment before deploying it to production. The configuration specification would look like this: YAML name: canary-test-config type: gateway-load-balancing-config # The rules are evaluated in order and once a request matches one rule, # the subsequent rules are not checked rules: # Distribute traffic on staging environment with 90% to openai/gpt4 and 10% to aws/bedrock-llama3 - id: \"openai-gpt4-dev-env\" when: metadata: env: staging load_balance_targets: - target: \"openai/gpt4\" weight: 90 - target: \"aws/bedrock-llama3\" weight: 10 In this configuration, we instruct the gateway to route 10% of the traffic to the aws/bedrock-llama3 model. TrueFoundry manages all necessary request transformations, so there\u2019s no need to modify your existing code. You can easily set this up as a rule in your load balancing configuration, as shown here Once the data begins flowing, you can use TrueFoundry\u2019s analytics dashboards to monitor the impact of the new model on metrics such as cost, latency, errors, and feedback. Updated 22 days ago",
    "https://docs.truefoundry.com/docs/fallback": "Fallback Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Fallback All Pages Start typing to search\u2026 Fallback Fallback a request to another model on failure from one model Fallback allows users to configure fallback models in case failures happen for certain models. This is useful specially in cases, where we have the same model from multiple providers and we want to fallback to the other provider in case one provider is down or has rate limits. It helps provide higher reliability to end users. The fallback configuration contains an array of rules. Every request is evaluated against the set of rules and only the first matching rule is evaluated - the subsequent rules are ignored. For each rule, we have three sections: when subjects: An array of user or teams from which requests is originated - for e.g. user: [email protected] , team:team1 models: An array of model ids which will be used to filter the requests. The model ids are the same as what we pass in the model field in the request. metadata response_status_codes: The response status on which the fallbacks will be executed. This is important since we only want to retry on recoverable errors codes like rate limit exceeded (429), etc. fallback_models : target: The model ID to which traffic should be routed override_params (optional): A key-value object used to modify or extend the request body when falling back to the target model. Each key corresponds to a parameter name, and the associated value specifies what should be sent to the target. This allows you to override existing parameters from the original request or introduce entirely new ones, enabling flexible customization for each target model during fallback. Let's say you want to setup fallback based on following rules: Fallback to gpt-4 of azure, aws if openai-main/gpt-4 fails with 500 or 503. The azure target also overrides a few request parameters like temperature and max_tokens. Fallback to llama3 of azure, aws if bedrock/llama3 fails with 500 or 429 for customer1. Your fallback config would look like this: YAML name: model-fallback-config type: gateway-fallback-config # The rules are evaluated in order and once a request matches one rule, # the subsequent rules are not checked rules: # Fallback to gpt-4 of azure, aws if openai-main/gpt-4 fails with 500 or 503. The openai-main target also overrides a few request parameters like temperature and max_tokens - id: \"openai-gpt4-fallback\" when: models: [\"openai-main/gpt4\"] response_status_codes: [500, 503] fallback_models: - target: openai-main/gpt-4 override_params: temperature: 0.9 max_tokens: 800 # Fallback to llama3 of azure, aws if bedrock/llama3 fails with 500 or 429 for customer1. - id: \"llama-bedrock-customer1-fallback\" when: models: [\"bedrock/llama3\"] metadata: customer-id: customer1 response_status_codes: [500, 429] fallback_models: - target: aws/llama3 - target: azure/llama3 Configure Fallback on Gateway It's straightforward\u2014simply go to the Config tab in the Gateway, add your configuration, and save. Updated 3 days ago",
    "https://docs.truefoundry.com/docs/guardrails": "Guardrails Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Guardrails All Pages Start typing to search\u2026 Guardrails Guardrails allow you to validate and transform LLM inputs and outputs to ensure safety, quality, and compliance. Config The guardrails configuration contains an array of rules that are evaluated for each request. Only the first matching guardrail rule is applied to that request . Each rule can specify input and output guardrails that will be applied. Let's take a look at a sample configuration first. Example Configuration This sample guardrail has one rule that has one input guardrail that masks PIIs and two output guardrails - one for masking PII and other for failing the request if the LLM responds with any of the denied topics. It also has a when block, so only specific requests have these guardrails applied on them. YAML name: guardrails-config type: gateway-guardrails-config guardrails_service_url: https://guardrails.truefoundry.com rules: - id: openai-guardrails when: models: - openai/gpt3-5 - my-bedrock/anthropic-3-7 metadata: internal-service: backend-svc # arbitrary key-value pairs input_guardrails: - type: pii action: transform options: entity_types: - email - ssn - name - address output_guardrails: - type: topics action: validate options: denied_topics: - medical advice - profanity - hate speech - violence - type: pii action: transform options: entity_types: - email - ssn - name - address Guardrails Service URL The guardrails_service_url field specifies the URL of the server that implements the guardrails APIs. This server provides endpoints for validating and transforming content according to the configured guardrails. The server exposes REST APIs that handle the actual implementation of the guardrail rules. In most case, you should be able to use the standard TrueFoundry Guardrails Server. Rules For each rule, we have three sections: id : A unique identifier for the rule when : Conditions for when this rule should be applied (an empty object ( ) means apply to all requests) subjects : An array of user, teams or virutal accounts from which requests is originated - for e.g. user: [email protected] , team:team1, virtualaccount:virtualaccountname models : An array of model ids which will be used to filter the requests. The model ids are the same as what we pass in the model field in the request. metadata : Key value pairs of metadata to filter requests to apply current guardrail on. input_guardrails : An array of guardrails to apply to the input prompt output_guardrails : An array of guardrails to apply to the LLM response Each guardrail (under input_guardrails or output_guardrails ) has: type : The type of guardrail to apply (e.g., \"pii\", \"topics\", \"word\") action : Either \"validate\" (check but don't modify) or \"transform\" (modify the content) options : Configuration specific to that guardrail type Updated 15 days ago",
    "https://docs.truefoundry.com/docs/pii-detection": "PII Detection Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account PII Detection All Pages Start typing to search\u2026 PII Detection This guardrail detects and handles personally identifiable information like emails, SSNs, names, etc. Options: entity_types: An array of PII types to detect, e.g., [\"email\", \"ssn\", \"name\", \"address\"] The following entity types are supported: email phone ssn credit_card address name date_of_birth ip_address passport drivers_license crypto iban nrp medical_license url us_bank_number us_itin uk_nhs uk_nino es_nif es_nie it_fiscal_code it_driver_license it_vat_code it_passport it_identity_card pl_pesel sg_nric_fin sg_uen au_abn au_acn au_tfn au_medicare in_pan in_aadhaar in_vehicle_registration in_voter in_passport fi_personal_identity_code Example YAML name: communication-guardrails type: gateway-guardrails-config rules: - id: privacy-filter when: models: - openai-main/gpt-3-5-turbo subjects: - user: [email protected] input_guardrails: - type: pii action: transform options: entity_types: - email - phone - ssn - name output_guardrails: - type: pii action: transform options: entity_types: - email - phone - ssn - name guardrails_service_url: https://example-guardrails-service.company.com In this example, the guardrail system automatically detects and anonymizes emails, phone numbers, social security numbers, and names in both the incoming and outgoing data streams. This process helps maintain the privacy of individuals while allowing the company to handle sensitive data securely and compliantly. Updated 15 days ago",
    "https://docs.truefoundry.com/docs/topic-filtering": "Topic Filtering Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Topic Filtering All Pages Start typing to search\u2026 Topic Filtering Validates that content does not contain certain topics. Options: denied_topics : An array of topics to disallow, e.g., [\"medical advice\", \"profanity\"] threshold : Float between 0-1, describes how sensitive should the classifier be. A higher threshold means, the content has to be highly related with the topic for the guardrail to fail the request. Example YAML name: content-filter-guardrails type: content-filter-guardrails-config rules: - id: content-standard-enforcement when: models: - openai-main/gpt-3-5-turbo input_guardrails: - type: topics action: validate options: threshold: 0.8 denied_topics: - medical advice - profanity output_guardrails: - type: topics action: validate options: threshold: 0.9 denied_topics: - medical advice - profanity guardrails_service_url: https://content-filter-service.company.com Updated 15 days ago",
    "https://docs.truefoundry.com/docs/word-filtering": "Word Filtering Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Word Filtering All Pages Start typing to search\u2026 Word Filtering Filters specific words and phrases. Options: word_list : An array of words to filter case_sensitive : Whether matching is case sensitive (default false) whole_words_only : Match whole words only (default true) replacement : Text to replace filtered words with (default \"[FILTERED]\") Example Config YAML name: word-filter-guardrails type: word-filter-guardrails-config rules: - id: specific-word-filter when: models: - openai-main/gpt-3-5-turbo input_guardrails: - type: word_filter action: transform options: word_list: - \"unwantedword\" - \"inappropriatephrase\" case_sensitive: false whole_words_only: true replacement: \"[REMOVED]\" output_guardrails: - type: word_filter action: transform options: word_list: - \"unwantedword\" - \"inappropriatephrase\" case_sensitive: false whole_words_only: true replacement: \"[REMOVED]\" guardrails_service_url: https://word-filter-service.company.com Updated 15 days ago",
    "https://docs.truefoundry.com/docs/tracing": "Tracing Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Tracing All Pages Start typing to search\u2026 Tracing TrueFoundry offers robust monitoring for applications by collecting traces, metrics, and logs to deliver real-time insights into performance, efficiency, and cost. At its core, TrueFoundry has built its own tracing solution based on the OpenTelemetry (OTel) standard, ensuring compatibility and flexibility. While TrueFoundry does not rely on Traceloop, we recommend using the Traceloop client SDK to push traces, as it supports a wide range of modes and provides a seamless developer experience for AI and LLM-based applications. Key Concepts Trace A trace would represent the complete lifecycle of a request or task as it flows through the various services and components that interact with the LLM. This could involve multiple stages, such as receiving input from the user, sending the input to the model for inference, processing the model\u2019s output, and delivering the result back to the user. The trace provides a holistic view of how the LLM application processes the request and where any delays or issues may arise in the overall flow, whether it's the input, the model's computation, or the output handling. Span A span represents an individual unit of work or operation that occurs within the trace. In the context of an LLM application, spans are used to capture each distinct task or action that is performed during the lifecycle of a request. Each span provides granular insights into specific stages of the request lifecycle, and together they allow you to understand not only how the LLM system performs overall but also where performance optimizations or troubleshooting may be needed in individual components or operations. For Example: Imagine a user queries an LLM for a recommendation. The trace would look something like this: Trace: Tracks the entire user request from input to response. Span 1: Input preprocessing (parsing and tokenizing the user query). Span 2: Model inference (running the LLM to generate a recommendation). Span 3: Post-processing (formatting the recommendation output). Span 4: Output delivery (returning the recommendation to the user). By using traces and spans in LLM applications, you gain end-to-end visibility of how well your LLM-based system is performing, where the bottlenecks might be, and how to improve the efficiency and responsiveness of the system. TrueFoundry's Tracing UI Getting Started [Supported Providers & Frameworks] If you are using any of the supported Providers and Frameworks then adding tracing is pretty simple. You just need to install, import and initialise the Traceloop SDK. It will automatically log traces your requests. Say for example you are using OpenAI client the follow these steps: lnstall dependencies: First, you need to install the following pip install traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here OpenAI Completion Example Python from openai import OpenAI #Import Traceloop SDK from traceloop.sdk import Traceloop api_key = \"Enter your API Key here\" #Initialise the Traceloop SDK Traceloop.init( app_name=\"openai-example\", disable_batch=True, #This is recommended, only if you are trying this in local machine. This helps to push the traces immediately, without waiting for batching ) client = OpenAI(api_key=api_key, base_url=\"https://internal.devtest.truefoundry.tech/api/llm/api/inference/openai\") stream = client.chat.completions.create( messages = [ {\"role\": \"system\", \"content\": \"You are an AI bot.\"}, {\"role\": \"user\", \"content\": \"Enter your prompt here\"}, ], model= \"openai-main/gpt-4o\", stream=True, temperature=0.7, max_tokens=256, top_p=0.8, frequency_penalty=0, presence_penalty=0, stop=[\"</s>\"], extra_headers={ \"X-TFY-METADATA\": '{\"tfy_log_request\":\"true\"}' } ) for chunk in stream: if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Logged trace on TrueFoundry UI: If you want are using any of the following frameworks then follow these: CrewAI LangGraph Agno Getting Started [Non-Supported Providers & Frameworks] Can you still log traces on TrueFoundry even if you are not using any of the supported provider or framework? YES! Follow these steps and get instant monitoring: lnstall dependencies: First, you need to install the following pip install traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here Initialise Traceloop In your LLM app, initialize the Traceloop tracer like this: Python from traceloop.sdk import Traceloop Traceloop.init() Traceloop.init(disable_batch=True) #This is recommended, only if you are trying this in local machine. This helps to push the traces immediately, without waiting for batching Annotate your Workflows, Agents and Tools For complex workflows or chains, annotating them can help you gain better insights into their operations. With TrueFoundry, you can view the entire trace of your workflow for a comprehensive understanding. Traceloop offers a set of decorators to simplify this process. For instance, if you have a function that renders a prompt and calls an LLM, you can easily add the @workflow decorator to track and annotate the workflow. Let's say your workflow calls more functions you can annotate them as @task Python from openai import OpenAI from traceloop.sdk.decorators import workflow, task client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) @task(name=\"joke_creation\") def create_joke(): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about opentelemetry\"}], ) return completion.choices[0].message.content @task(name=\"signature_generation\") def generate_signature(joke: str): completion = openai.Completion.create( model=\"davinci-002\",[] prompt=\"add a signature to the joke:\\n\\n\" + joke, ) return completion.choices[0].text @workflow(name=\"pirate_joke_generator\") def joke_workflow(): eng_joke = create_joke() pirate_joke = translate_joke_to_pirate(eng_joke) signature = generate_signature(pirate_joke) print(pirate_joke + \"\\n\\n\" + signature) Similarly, when working with autonomous agents, you can use the @agent decorator to trace the agent as a single unit. Additionally, each individual tool within the agent should be annotated with the @tool decorator. Python from openai import OpenAI from traceloop.sdk.decorators import agent, tool client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) @agent(name=\"joke_translation\") def translate_joke_to_pirate(joke: str): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"Translate the below joke to pirate-like english:\\n\\n{joke}\"}], ) history_jokes_tool() return completion.choices[0].message.content @tool(name=\"history_jokes\") def history_jokes_tool(): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"get some history jokes\"}], ) return completion.choices[0].message.content Updated 16 days ago",
    "https://docs.truefoundry.com/docs/tracing-in-crewai": "Tracing in CrewAI Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Tracing in CrewAI All Pages Start typing to search\u2026 Tracing in CrewAI Tracing helps you understand what\u2019s happening under the hood when an agent run is called. You get to understand the path, tools calls made, context used, latency taken when you run your agent using TrueFoundry\u2019s tracing functionality. TrueFoundry enhances CrewAI with powerful observability features, offering real-time monitoring of AI agent workflows, task execution, and LLM performance. By integrating with TrueFoundry's tracing, metrics collection, and error detection capabilities, you gain deep insights into how CrewAI agents interact, complete tasks, and optimize workflows. Additionally, TrueFoundry provides end-to-end traceability for AI-driven systems, making your CrewAI implementations more transparent, reliable, and scalable. lnstall dependencies: First, you need to install the following pip install crewai==0.102.0 langchain-openai==0.2.14 traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here Demo CrewAI Agent The following demo agent is a research agent which researches the latest trends to conduct detailed market research with keen attention to detail. For example, it can generate \"A comprehensive report on AI and machine learning\" Python from crewai import Agent, Task, Crew from langchain_openai import ChatOpenAI #Import Traceloop from traceloop.sdk import Traceloop #Initialize Traceloop SDK to log traces automatically Traceloop.init(app_name=\"crewai\") llm = ChatOpenAI(model=\"gpt-4o-mini\") researcher = Agent( role='Research Analyst', goal='Conduct detailed market research', backstory='Expert in market analysis with keen attention to detail', llm=llm, verbose=True ) research_task = Task( description='Research the latest trends in AI and machine learning', agent=researcher, expected_output='A comprehensive report on current AI and machine learning trends', expected_output_type=str ) crew = Crew( agents=[researcher], tasks=[research_task], verbose=True ) result = crew.kickoff() print(\"Final Result:\", result) View Logged Trace on UI Updated 19 days ago",
    "https://docs.truefoundry.com/docs/tracing-in-langgraph": "Tracing in LangGraph Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Tracing in LangGraph All Pages Start typing to search\u2026 Tracing in LangGraph Tracing helps you understand what\u2019s happening under the hood when an agent run is called. You get to understand the path, tools calls made, context used, latency taken when you run your agent using TrueFoundry\u2019s tracing functionality. With TrueFoundry, you can easily monitor LangGraph by capturing traces. This data provides real-time insights into AI agent interactions, execution paths, and overall system performance. By integrating, you can instrument LangGraph workflows to capture spans and traces. These traces are then visualized in TrueFoundry, giving you a clear view of execution flows. Real-time observability allows you to analyze resource usage and performance trends, enabling you to optimize workflows. This integration helps fine-tune AI-driven automation, boosting both reliability and efficiency in LangGraph-based applications. lnstall dependencies: First, you need to install the following pip install langgraph==0.3.22 traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here Demo LangGraph Agent The following demo agent is a research agent which researches the latest trends to conduct detailed market research with keen attention to detail. For example, it can generate \"A comprehensive report on AI and machine learning\" Python from dotenv import load_dotenv from langchain_core.messages import HumanMessage from langchain_openai import ChatOpenAI from langgraph.graph import END, StateGraph, MessagesState from traceloop.sdk import Traceloop # Load environment variables from a .env file load_dotenv() # initialize the Traceloop SDK Traceloop.init(app_name=\"langgraph\") model = ChatOpenAI(model=\"gpt-4o\", temperature=0) # Define the function that calls the model def call_model(state: MessagesState): messages = state[\"messages\"] response = model.invoke(messages) # We return a list, because this will get added to the existing list return {\"messages\": [response]} # Define a new graph workflow = StateGraph(MessagesState) # Define the two nodes we will cycle between workflow.add_node(\"agent\", call_model) # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point(\"agent\") app = workflow.compile() # Use the Runnable final_state = app.invoke( {\"messages\": [HumanMessage(content=\"Research the latest trends in AI and machine learning\")]}, ) print(final_state[\"messages\"][-1].content) View Logged Trace on UI Updated 16 days ago",
    "https://docs.truefoundry.com/docs/tracing-in-agno": "Tracing in Agno Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Tracing in Agno All Pages Start typing to search\u2026 Tracing in Agno Tracing helps you understand what\u2019s happening under the hood when an agent run is called. You get to understand the path, tools calls made, context used, latency taken when you run your agent using TrueFoundry\u2019s tracing functionality. TrueFoundry enhances Agno with powerful observability features, offering real-time monitoring of AI agent workflows, task execution, and LLM performance. By integrating with TrueFoundry's tracing, metrics collection, and error detection capabilities, you gain deep insights into how CrewAI agents interact, complete tasks, and optimize workflows. Additionally, TrueFoundry provides end-to-end traceability for AI-driven systems, making your CrewAI implementations more transparent, reliable, and scalable. lnstall dependencies: First, you need to install the following pip install agno==1.2.6 traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here Demo Agno Agent The following demo agent is a research agent which researches the latest trends to conduct detailed market research with keen attention to detail. For example, it can generate \"A comprehensive report on AI and machine learning\" Python from agno.agent import Agent from agno.models.openai import OpenAIChat from agno.workflow import RunResponse from dotenv import load_dotenv from traceloop.sdk import Traceloop from traceloop.sdk.decorators import workflow, task load_dotenv() # Initialize the Traceloop SDK Traceloop.init(app_name=\"agno\") @workflow(name=\"research_workflow\") def research(topic: str): research_agent = Agent( model=OpenAIChat(id=\"gpt-4o-mini\"), description=\"Expert in market analysis with keen attention to detail\" ) @task(name=\"research_task\") def research_task(topic: str): researcher_response: RunResponse = research_agent.run(topic) # Convert the response to a serializable format if necessary serializable_response = researcher_response.dict() if hasattr(researcher_response, \"dict\") else str(researcher_response) return serializable_response return research_task(topic) if __name__ == \"__main__\": research(topic=\"Research the latest trends in AI and machine learning\") View Logged Trace on UI Updated 19 days ago",
    "https://docs.truefoundry.com/docs/analytics": "Analytics Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Analytics All Pages Start typing to search\u2026 Analytics The analytics dashboard offers an interactive interface for gaining insights into your LLM application. It displays a variety of graphs and metrics, including data on requests to different LLMs, costs, latencies, tokens, user activity, model usage, errors, requests affected by config, and more. Access cumulative data on key metrics such as input/output tokens, number of requests, and token costs. Additionally, you'll find detailed latency metrics for each individual model, including mean, P99, P90, and P50 percentiles. Analyse and gather insights on request/second for individual models Get detailed breakdown of input/output tokens, total requests and total cost for individual users and models You can also analyse requests that are rate-limited, use fallbacks, or are load-balanced, along with detailed information on the rules that were applied. Updated 19 days ago",
    "https://docs.truefoundry.com/docs/metadata": "Metadata Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Metadata All Pages Start typing to search\u2026 Metadata Metadata provides a way to add additional context to your AI requests. It acts as a way to tag your requests with relevant business context, offering several key benefits: Monitor usage across various users, teams, environments, or even features. Configure rate limit, fallback, and load balance based on metadata. Filter logs to identify specific types of requests. Analyze usage patterns to gain insights into AI interactions. Conduct audits for compliance and security purposes. Here is a sample metadata: JSON // Example metadata { \"tfy_log_request\": \"true\", //Whether to add a log/trace for this request or now \"environment\": \"staging\", // THe environment - dev, staging or prod? \"feature\": \"countdown-bot\", //Which feature initiated the request? } Log metadata To log metadata, the implementation varies depending on the method you're using. Using OpenAI SDK or Langchain SDK If you're using the OpenAI SDK or Langchain SDK, simply pass the extra_headers key, as shown in the following example: Node llm = ChatOpenAI( ... ... extra_headers={ \"X-TFY-METADATA\": '{\"tfy_log_request\":\"true\"}', } ) Using REST API, Stream API Client, or cURL If you're using a REST API client, Stream API client, or cURL, include the X-TFY-METADATA header in your request: Python import requests try: response = requests.post( URL, headers = { ... \"X-TFY-METADATA\": '{\"tfy_log_request\":\"true\"}', }, json = { \"messages\": [ {\"role\": \"system\", \"content\": \"You are an AI bot.\"}, {\"role\": \"user\", \"content\": \"Enter your prompt here\"}, ], \"model\": \"test-ai-inference/llama-new\", \"temperature\": 0.7, \"max_tokens\": 256, ... ] } ) You can include multiple metadata keys with each request. Please note that all values must be strings, with a maximum length of 128 characters. Filter logs and metrics You can filter logs and metrics using metadata. Simply navigate to LLM Gateway , go to the Logs/Metrics tab , click on Filter , and apply the desired filters, as demonstrated in the screenshot below: Rate limit You can also rate limit your requests based on metadata. Say you want to limit all requests to gpt4 model from openai-main account for environment dev: to 1000 requests per day YAML name: ratelimiting-config type: gateway-rate-limiting-config # The rules are evaluated in order, and all matching rules are considered. # If any one of them causes a rate limit, the corresponding ID will be returned. rules: # - id: \"openai-gpt4-dev-env\" when: models: [\"openai-main/gpt4\"] metadata: env: dev limit_to: 1000 unit: requests_per_day Learn more about rate limits here Load Balance You can also load balance your requests based on metadata. Say you want to distribute traffic for llama model from bedrock for customer1 with 60% to azure-bedrock and 40% to aws-bedrock YAML name: loadbalancing-config type: gateway-load-balancing-config ---- ---- - id: \"llama-bedrock-customer1\" when: models: [\"bedrock/llama3\"] metadata: customer-id: customer1 load_balance_targets: - target: \"azure/bedrock-llama3\" weight: 60 - target: \"aws/bedrock-llama3\" weight: 40 Learn more about Load balancing here Fallback You can also add fallback to your requests based on metadata. Say you want to fallback to llama3 of azure, aws if bedrock/llama3 fails with 500 or 429 for customer1. YAML name: model-fallback-config type: gateway-fallback-config ---- ---- - id: \"llama-bedrock-customer1-fallback\" when: models: [\"bedrock/llama3\"] metadata: customer-id: customer1 response_status_codes: [500, 429] fallback_models: [\"azure/llama3\", \"aws/llama3\"] Learn more about fallback here Updated 23 days ago",
    "https://docs.truefoundry.com/docs/export-logstraces": "Export Logs/Traces Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Export Logs/Traces All Pages Start typing to search\u2026 Export Logs/Traces TrueFoundry provides a way to export your logs/traces data. With this, you can easily request and obtain your logs/traces data in a structured JSON format, allowing you to gain valuable insights into your LLM usage, performance, costs, and more. \u200bRequesting Logs Export To submit a data export request, simply follow these steps: Ensure you are an admin of your organization. Send an email to [email protected] with the subject line Logs/Traces Export - [YOUR_ORG_NAME]. In the email body, specify the time frame for which you require the logs data. Our team will process your request and provide you with the exported logs data in JSON format. Updated 22 days ago",
    "https://docs.truefoundry.com/docs/prompt-management": "Prompt Management Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Prompt Management All Pages Start typing to search\u2026 Prompt Management TrueFoundry's LLM Playground is your go-to space for experimenting, testing, and perfecting prompts for your AI application. Here, you can explore different models, test various variables, review completions, and fine-tune your prompt engineering strategy\u2014all before deploying to production. It\u2019s the ideal environment to ensure your prompts are optimized and ready for real-world use. Writing Prompts on Playground Writing prompts on Playground is pretty simple. You start by selecting a model that you have integrated. Add your System Message Add your user message or query Hit Run to generate a response Add Variables to Prompts You can simply create variable by writing {{variable_name}} as part of your messages. Once the variable is added you can provide values which will be reflect in the chat section as shown below: Add Tools to Prompts Some models support function calling, enabling the AI to request specific information or perform actions. The Playground simplifies experimentation with these features. Although the gateway does not directly execute calls to external tools, it allows you to describe the tool and simulate the call within the response. This simulation provides a detailed representation of the request and expected response, allowing developers to understand how the language model would interact with external systems. To define functions that the model can call, simply click the \u201cAdd Tool\u201d button and follow the steps as shown below: Save Prompts as Prompt Template Once you are happy with the prompt. You can click on \"Save Template\" button on the screen. All prompt templates are stored in ML Repos which is like a storage unit on TrueFoundry. To update an existing template, follow the same steps. Prompt Template Versions Whenever you modify a prompt template, TrueFoundry automatically tracks these changes. The versioning system enables you to: Experiment with and test different prompt variations Maintain a full history of all prompt changes Compare different versions Load existing Prompt Template To load a version of an existing prompt template, you just need to visit playground and follow these steps: Use Prompt Template in Code TrueFoundry automatically generates a code snippet for you to use specific version of your prompt template. Simply visit the prompt details page and switch to \"Use in Code\" tab to copy the code snippet. Updated 14 days ago",
    "https://docs.truefoundry.com/docs/launch-notebooks": "Launch Jupyter Notebook Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Launch Jupyter Notebook All Pages Start typing to search\u2026 Launch Jupyter Notebook Jupyter Notebooks are probably the first tool to be used in any ML Project. Truefoundry enables you to run Jupyter Notebooks on Kubernetes on any hardware you need, and auto-shut down when it's not being used hence saving costs. You can read more about Truefoundry Notebooks vs other notebook solutions here . To get started with launching a Jupyter Notebook, you can follow the steps outlined below (You can find details on the configuration options below) The notebook will go live in a few minutes. It takes a few minutes since the image will need to be downloaded on the machine first. Once the notebook becomes live, you will see the status turn to Running and an endpoint will start showing up. Clicking on the endpoint will take you to the Jupyter Notebook. Stop and resume your notebook When you're done working with a notebook instance, you can stop it to conserve resources and reduce costs. Clicking on the Stop button shuts down the notebook instance environment and releases associated resources. Your notebook instance data (apart from the apt packages installed as a root user) persists, and you can easily restart it later by clicking the Resume button. Configure your Notebook In the notebook creation form, you can configure the following options: Image When creating a Jupyter Notebook, you can choose between three image options: Minimal Image : A lightweight image with no pre-installed packages, providing a clean slate for customization and package installation. Cuda 12.1 Image : A pre-configured image with Cuda 12.1 pre-installed Custom Extended Image : You can extend an Image created by truefoundry and add your custom packages to it. You can read about extended images here. Auto Shutdown on Inactivity By default, Notebook instances are configured to automatically stop after 30 minutes of inactivity. This helps prevent unnecessary resource consumption when Notebook instances are left idle. You can change the Stop After (minutes of inactivity) setting within the deployment form. Install Apt packages in the notebook By default anything you install outside of home directory will NOT be persistent across notebook restarts. This means all apt installs (done directly from notebook) will NOT be persistent across restarts of notebook. To make these apt packages persistent across restarts, you can add a \"Build Script\" in the notebook as follows For example, here is a sample build script to install ffmpeg package: sudo apt update sudo apt install ffmpeg Storage Size Specify the amount of storage space you require for your notebook instance. This is persistent storage that will be used to store your notebook files, data, and any other artifacts generated during your work. Set resources for your Notebook Define the computational resources allocated to your notebook instance. You can adjust the CPU and memory allocation to meet the requirements of your data science tasks. Running a Notebook instance with GPU In case you want to use GPU in your Notebook Instance, you can follow these steps: Upon successful deployment, your GPU Notebook instance instance will be provisioned with the specified GPU type. You can then utilize the GPU resources to accelerate your computations, such as training deep learning models or running GPU-intensive workloads. Access data from S3 or other clouds In some instances, your Jupyter Notebooks may need to access data stored in S3 or other cloud storage platforms. To facilitate this access, you can employ one of two approaches: Credential-Based Access through environment variables This approach involves defining specific environment variables that contain the necessary credentials for accessing the cloud storage platform. For instance, to access S3, you would set environment variables for the AWS access key ID and secret access key, the environment variables being: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY IAM Role-Based Access through Service Account The second approach is to provide your Notebook with a Role with the necessary permission through Service Accounts. Service Accounts provide a streamlined approach to managing access without the need for tokens or complex authentication methods. This approach involves creating a Principal IAM Role within your cloud platform, granting it the necessary permissions for your project's requirements. Here are detailed guides for creating Principal IAM Roles in your respective cloud platforms and integrating them as Service Accounts within the workspace: AWS: Authenticate to AWS services using IAM service account GCP: Authenticate to GCP using IAM serviceaccount Once you've configured the Service Account within the workspace, you can simply toggle the Show Advanced Fields option at the bottom of the form. This will reveal an expanded set of options, from which you can select the desired Service Account using the provided dropdown menu. Access data in volume from the notebook Mounting a volume to a notebook allows you to access data stored on the volume from within the notebook environment. This can be useful for a variety of tasks, such as loading data for analysis, training machine learning models, and deploying applications. You can follow these steps for mounting a volume to the notebook: Once mounted, in your Notebook you will be able to access the data in your Volume from within the Notebook Configuring Python Environments Inside a Notebook Truefoundry's Deployed Notebooks by default start with a conda environment with Python Version = 3.11. In case you are working on several projects simultaneously, and you want to maintain multiple Python environments for different Python versions / different sets of tasks. You can do so by following these steps: Command to be executed: Shell # Create a new conda environment named \"py39\" with Python 3.9 # It will take around 2 minutes for the changes to sync # After 2 minutes, You need to hard refresh after executing this command for kernel to show up in the listing page conda create -y -n py39 python=3.9 Launching Jupyter Notebook with Custom Images Instead of using the pre-built Jupyter Lab Images provided by TrueFoundry, you have the flexibility to create and deploy your custom images. This allows you to pre-install specific libraries, tools, and configurations within your notebook environment, tailoring it to your specific needs and project requirements. To create a custom image, start by using the TrueFoundry Jupyter Notebook Image as a base. Below are the docker images TrueFoundry uses to support Jupyter base and Jupyter full notebooks. Image URI Size Jupyter Lab CUDA 12.1 Toolkit Common ML Libraries public.ecr.aws/truefoundrycloud/jupyter:0.3.20-sudo ~0.7 GB \u2705 public.ecr.aws/truefoundrycloud/jupyter:0.3.0-cu121-sudo ~6 GB \u2705 \u2705 \ud83d\udcd8 Latest Images Please visit the following links for latest versions: https://gallery.ecr.aws/truefoundrycloud/jupyter https://gallery.ecr.aws/truefoundrycloud/jupyter-full You can use one of these images as a base for creating your custom image while keeping some invariants unchanged. Let's take an example where we want to customize the image by including a few apt packages like ffmpeg and a few pip packages like gradio . We will create a new Dockerfile using these existing images as the base and push to a registry for later use. docker FROM truefoundrycloud/jupyter:0.2.20 # Install apt packages RUN DEBIAN_FRONTEND=noninteractive apt install -y --no-install-recommends ffmpeg # Install pip packages RUN python3 -m pip install --use-pep517 --no-cache-dir \ud83d\udea7 Do not overwrite the ENTRYPOINT or CMD instructions. These are built into the base images and are critical for things to work correctly Other possible customizations can be found here Build and push the image to a registry for it to be used in TrueFoundry Notebook. Make sure the destination registry is already integrated with the TrueFoundry platform. Detailed instructions are available here Example: Installing CUDA 11.8 and cuDNN 8 Dockerfile FROM truefoundrycloud/jupyter:0.2.20-sudo ENV TORCH_CUDA_ARCH_LIST=\"7.0 7.5 8.0 8.6 9.0+PTX\" ENV DEBIAN_FRONTEND=noninteractive USER root # Install CUDA 11.8 RUN apt update && \\ apt install -y --no-install-recommends git curl wget htop && \\ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb -O /tmp/cuda-keyring_1.1-1_all.deb && \\ dpkg -i /tmp/cuda-keyring_1.1-1_all.deb && \\ apt update && \\ apt install -y --no-install-recommends cuda-toolkit-11-8 libcudnn8=8.9.7.29-1+cuda11.8 libcudnn8-dev=8.9.7.29-1+cuda11.8 USER jovyan Git Login Instructions for JupyterLab Users Git integration is already installed for you via the JupyterLab Git extension. This guide outlines how to configure Git and authenticate using two recommended methods. You can access the extention from the left panel or menu bar from the top as shown below. You can clone your repository directly from this extention or via terminal. Before using Git, set your identity: git config --global user.name \"Your Name\"\\ git config --global user.email \"[ [email protected] ](mailto: [email protected] )\" Option 1: Personal Access Token (Recommended for GitHub/GitLab) GitHub and GitLab have deprecated password authentication. Personal Access Tokens (PATs) are the most secure and recommended way. Steps: Generate a Token: For GitHub: https://github.com/settings/tokens For GitLab: https://gitlab.com/-/profile/personal_access_tokens Scope: Select at least repo or read/write permissions Clone or Pull a Repository: Use the Git extension in JupyterLab or the terminal: git clone <your private repository url> Authenticate: When prompted for a username/password, paste your username in the username section and \"token\" in password section Option 2: SSH Key Authentication Use SSH for password-less authentication after one-time setup. Follow platform-specific instructions to generate and add your SSH key: GitHub SSH Help: https://docs.github.com/en/authentication/connecting-to-github-with-ssh GitLab SSH Guide: https://docs.gitlab.com/user/ssh/ After setup, clone your repositories using the SSH URL: git clone [email protected] :sample-org/sample-repo.git Updated 11 days ago",
    "https://docs.truefoundry.com/docs/launch-an-ssh-server": "Launch a SSH Server Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Launch a SSH Server All Pages Start typing to search\u2026 Launch a SSH Server If you like controlling your coding setup, consider using your own local VS Code instead of the hosted version. By setting up an SSH Server, you can run your code directly on your server, tapping into its robust cloud resources. This way, you maintain full control over your development environment, customizing VS Code with your favourite extensions and settings. How to find your SSH Public Key? Your SSH public key is typically stored in the ~/.ssh/ directory on your local machine. Look for a file ending in .pub , which contains the public key. For example: id_rsa.pub (default key) You can find your public key using the following command: For Windows : type $home\\.ssh\\id_rsa.pub ( windows powershell) For Mac/Linux : cat ~/.ssh/id_rsa.pub Creating a New SSH Key Pair If you don't have an SSH Key Pair. You can generate one using the command: ssh-keygen -t rsa You can refer to this Link for generating an SSH Key Pair. Installing ProxyTunnel To install proxytunnel , follow the steps below for your Operating System: Windows Open PowerShell and run the following command: bash Invoke-Expression (Invoke-WebRequest -Uri \"https://raw.githubusercontent.com/truefoundry/infra-charts/refs/heads/main/scripts/windows_proxytunnel_install.ps1\" -UseBasicParsing).Content MacOS bash brew install proxytunnel Ubuntu bash sudo apt-get update sudo apt-get install proxy-tunnel Other Linux Distros Find proxytunnel in your package manager respositories Or Build from Source: https://github.com/proxytunnel/proxytunnel Connect Local VS Code with SSH To connect Visual Studio Code to a remote SSH server, follow these steps: Install the extension: Remote-SSH After installing the extension, reload VS Code to ensure all changes are applied. Once VS Code reloads, you\u2019ll see a new icon labeled Remote Explorer in the activity bar on the left side. Click on this icon to manage your SSH connections. Enter your password or passphrase to complete the connection. Adding more users to SSH Server To add more users to the SSH Server. You need to add the SSH keys to the users to the mkdir -p /home/jovyan/.ssh/authorized_keys bash mkdir -p /home/jovyan/.ssh echo \"ENTER_PUBLIC_KEY\" >> /home/jovyan/.ssh/authorized_keys You can add multiple users with this. You can also remove users from this file as required. Copying Files from SSH Server to Local Machine To copy files from an SSH server to your local machine, use the following command in PowerShell. Make sure to update the placeholders with the correct values for your environment. bash scp -r <deploymentName>:<remote-file-path> <local-file-path> <deploymentName> : The name of the SSH server on the platform <remote-file-path> : The path to the file or directory on the SSH server that you want to copy. <local-file-path> : The destination path on your local machine where you want to save the copied file. Copying Files from Local Machine to SSH Server To copy files from your local machine to an SSH server, use the following command in PowerShell. Make sure to update the placeholders with the correct values for your environment. bash scp -r <local-file-path> {{deploymentName}}:<remote-file-path> <deploymentName> : The name of the SSH server on the platform <remote-file-path> : The destination path on the SSH server where you want to save the copied file. <local-file-path> : The path to the file or directory on your local machine that you want to copy. Using rsync to Copy Files Between Local Machine and SSH Server rsync is a tool for synchronizing files and directories between a local machine and a remote server via SSH. It only transfers the changes made to files, making it efficient for backups and file synchronization. Copying Files from Local Machine to SSH Server To copy files from your local machine to an SSH server using rsync , use the following command: bash rsync -avz <local-file-path> <deploymentName>:<remote-file-path> <deploymentName> : The name of the SSH server on the platform <remote-file-path> : The path to the file or directory on the SSH server that you want to copy. <local-file-path> : The destination path on your local machine where you want to save the copied file. The -a flag enables archive mode, preserving file permissions and timestamps. The -v flag enables verbose output, so you can see the progress of the transfer. The -z flag enables compression during transfer, which can speed up the process for larger files. Copying Files from SSH Server to Local Machine To copy files from an SSH server to your local machine using rsync , use the following command: rsync -avz {{deploymentName}}:<remote-file-path>:<remote-file-path> <local-file-path> <deploymentName> : The name of the SSH server on the platform <remote-file-path> : The path to the file or directory on the SSH server that you want to copy. <local-file-path> : The destination path on your local machine where you want to save the copied file. The -a flag enables archive mode, preserving file permissions and timestamps. The -v flag enables verbose output, so you can see the progress of the transfer. The -z flag enables compression during transfer, which can speed up the process for larger files. Create More Python Environments You can create python version with the following commands: she conda create -n my-new-env # run the following command to create python environment with different python version conda create -n py39-env python=3.9 Install Apt packages in the SSH Server By default anything you install outside of home directory will NOT be persistent across notebook restarts. This means all apt installs (done directly from notebook) will NOT be persistent across restarts of notebook. To make these apt packages persistent across restarts, you can add a \"Build Script\" in the notebook as follows For example, here is a sample build script to install ffmpeg package: sudo apt update sudo apt install ffmpeg Setting up the scale to zero You can now set up the scale to zero feature for the SSH server to stop the server after X minutes of inactivity. You can add this by setting the value of Stop after field which is the minutes of inactivity after which you want to stop the server. \ud83d\udea7 Scale to zero is only available for ssh server images with tag >= v0.3.10 Following conditions will be considered as an activity while checking the inactivity status An active SSH connection(including connection to vscode). Any application or service running in the foreground. The applications running(both background and foreground) in tmux or screen session. Following conditions will not be considered as activity Applications running in the background in SSH server. for example if you run the below command, then your service will be running in background and it will be considered as an inactivity. python main.py& Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-volume": "Introduction to Volume Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to Volume All Pages Start typing to search\u2026 Introduction to Volume Volumes are used to provide persistent storage to containers within pods so that read and write data to a central disk across multiple pods. They are especially useful in the context of machine learning when you need to store and access data, models, and other artifacts required for training, serving, and inference tasks. A few usecases where volumes turn out to be very useful: Share training data: Its possible multiple datascientists are training on the same data or we are running multiple experiments in parallel on the same dataset. The naive way will be to duplicate the data for multiple data scientists - however this will end up costing us much more. A more efficient way here will be to store the training data in a volume and mount the volume to the notebooks of different datascientists. Model Storage : If we are hosting models as realtime APIs, there will be multiple replicas of the api server to handle traffic. Here, every replica will need to download the model from the model registry (let's say S3) to local disk. If every replica does this repeatedly, it will take more time for starting up and also incur more S3 access cost. By using volumes, you can store your trained models externally and mount them onto the inference server. There is no need to download the model and the api server can just find the model on disk at the mounted path. Artifact Sharing : We might have a usecase wherein the output of one stage of a pipeline needs to be consumed by the next stage. For e.g, after finetuning a model, we might need to host it as an API just for experimentation. While we can write the model to S3 and then download it back again from S3, it will take a lot of time just for the model upload/download process. Instead for faster experimentation, the finetuning job can just write the model to a volume and the inference service can then mount the volume with the model. Checkpointing : During the training of machine learning models, it's common to save checkpoints periodically to resume training in case of failure or to fine-tune models. Volumes can be used to store these checkpoint files, ensuring that training progress is not lost when a job restarts from failure. This also enables you to run training on spot instances, hence saving a lot of cost. When to use Volume vs Blob storage like S3 / GCS / Azure Container? The choice of when to choose a blob storage like S3 vs volume is important from a performance, reliability and cost perspective. Performance: In most cases, reading data from S3 will be slower than reading data directly from a volume. So if the speed of loading is important for you, volume is the right choice. A good example for this is downloading and loading the model at inference time in multiple replicas of the service. A volume is a better choice since you don't incur the time of downloading the model repeatedly, and can load the model in memory from volume much faster. Reliability Blob storages like S3/GCS/ACS will in general be more reliable compared to volumes. So you should ideally always have the raw data backed in one of the blob storages and use volumes only for intermediate data. You should also always save a copy of the models in S3. Cost Access to volumes like EFS is a bit cheaper than using S3 - so if you are doing reading for the same data quite frequently - it might be useful to store it in a volume. If you are reading or writing very infrequently, then S3 should be just fine. Access Constraints Data in volumes should ideally only be accessed among workloads within the same region and cluster. S3 is meant to be accessed globally and also across cloud environments. So volumes is not a great choice if you want to access the data in a different region or cloud provider. Volume Provisioning Modes When deploying a volume on truefoundry, there can be two modes of provisioning: Dynamic: These are volumes which are created dynamically and provisioned as you deploy a volume on truefoundry. E.g. EBS, EFS in AWS, AzureFiles in Azure can be dynamically provisioned on Truefoundry. Static: These are volumes for which a storage volume already exists and we want to mount the data in that storage volume to our service/job. Examples of this include mounting S3 buckets, GCS buckets on the workloads deployed on platform. Things to be careful of when using Volumes The volumes you create in Truefoundry can be mounted to multiple replicas - and hence you can read and write from multiple replicas to the volume. However, we should be very careful about not writing to the volume at the same path from multiple replicas since it can cause data corruption. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/creating-a-volume": "Creating and Utilizing Volume Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating and Utilizing Volume All Pages Start typing to search\u2026 Creating and Utilizing Volume Creating a Volume After a brief moment, the deployment should become active, and your Volume Dashboard will transform to appear as follows: You can create a volume following the flow above, however, a few key things to understand in the Volume Creation form are as follows: Workspace You can read more about workspace and how to create it here . \ud83d\udcd8 Volumes created within a specific workspace in TrueFoundry are intended to be used exclusively by applications deployed within the same workspace. This localized approach ensures that the volume's data is accessible only to the applications that are part of the same environment, enhancing security and organization. Dynamically Provisioned Volumes Storage Classes Storage classes provide a way to specify the type of storage that should be provisioned for a Volume. These storage classes differ in their characteristics, such as performance, durability, and cost. You can select the appropriate storage class for your Volume while creating it, from the Storage Class dropdown menu. The specific storage classes available will depend on the cloud provider you are using and what is preconfigured by the Infra team. \ud83d\udcd8 You can add the truefoundry.com/enabled: 'true' label to the StorageClass resources to recommend only them while creating a Volume. You will usually see the following options in the storage classes based on the cloud provider: AWS Storage Classes TrueFoundry Storage Name Cloud Provider Storage Name Storage Class Description efs-sc Elastic File System (EFS) efs.csi.aws.com A fully managed, scalable, and highly durable elastic file system that offers high availability, automatic scaling, and cost-effective general file sharing. It's suitable for workloads with varying capacity needs. GCP Storage Classes TrueFoundry Storage Name Cloud Provider Storage Name Storage Class Description standard-rwx Google Basic HDD Filestore filestore.csi.storage.gke.io A cost-effective and scalable file storage solution ideal for general-purpose file storage and cost-sensitive workloads. It offers lower cost but also lower performance due to its HDD-based nature. premium-rwx Google Premium Filestore filestore.csi.storage.gke.io Provides higher performance and throughput compared to Basic HDD, making it suitable for I/O-intensive file operations and demanding workloads. It's SSD-based, offering higher performance at a higher cost. enterprise-rwx Google Enterprise Filestore filestore.csi.storage.gke.io Delivers the highest performance, throughput, advanced features, multi-zone support, and high availability, making it ideal for mission-critical workloads and applications with strict availability requirements. It comes with the highest cost. Azure TrueFoundry Storage Name Cloud Provider Storage Name Storage Class Description azurefile Azure File Storage (Standard) file.csi.azure.com Uses Azure Standard storage to create file shares for general file sharing across VMs or containers, including Windows apps. It offers cost-effective performance. azurefile-premium Azure File Storage (Premium) file.csi.azure.com Uses Azure Premium storage for higher performance, making it suitable for I/O-intensive file operations. azurefile-csi Azure File Storage (StandardCSI) file.csi.azure.com Leverages Azure Standard storage with CSI for dynamic provisioning, potentially offering better performance and CSI features. azurefile-csi-premium Azure File Storage (PremiumCSI) file.csi.azure.com Combines Azure Premium storage with CSI for dynamic provisioning and high-performance file operations. azureblob-nfs-premium Azure Blob Storage (NFS Premium) blob.csi.azure.com Uses Azure Premium storage with NFS v3 protocol for accessing large amounts of unstructured data and object storage, catering to demanding workloads with NFS access. azureblob-fuse-premium Azure Blob Storage (Fuse Premium) blob.csi.azure.com Uses Azure Premium storage with BlobFuse for accessing large amounts of unstructured data and object storage, suitable for workloads that require BlobFuse access. Statically Provisioned Volumes Statically provisioned volume enable us to mount an already existing volume to workloads on truefoundry. To create a statically provisioned volume: Find the PersistentVolume name you want to mount. In the form choose Statically Provisioned Enter the Persistent Volume Name and click on Submit Copy Volume FQN (Fully Qualified Name) A volume FQN helps identify a Volume uniquely in Truefoundry. You will need it for mounting Volumes to your deployment A volume FQN looks something like: tfy-volume://your-volume:your-workspace:your-volume-name You can copy the Volume FQN from here: Updated 5 months ago",
    "https://docs.truefoundry.com/docs/adding-volume-browser": "Interacting with your Volume Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Interacting with your Volume All Pages Start typing to search\u2026 Interacting with your Volume Attaching Volumes to a Deployment You can attach Volumes to various types of deployments, including Services and Jobs. Detailed instructions for mounting Volumes in Services and Jobs can be found in the following guides: Mounting Volumes in Services Mounting Volumes in Jobs GUI to Upload / Browse data in Volume Truefoundry provides an easy way to visually browse the data in a volume. For this, you can add a Volume Browser to your Volume Deployment, a user-friendly interface that allows you to effortlessly upload, download, and view files stored within the Volumes. Adding Volume Browser to your Volume Before adding Volume Browser to your Volume, you will need to create a secret with the password in it. Now you can Add the Volume Browser following the demo below: The Volume Browser is now added to your Volume. Upload / Browse data using the Volume Browser Once set-up, you can use Volume Browser to effortlessly upload, download, and view files stored within the Volumes. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/creating-statically-provisioned-volumes": "Creating Statically Provisioned Volumes Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating Statically Provisioned Volumes All Pages Start typing to search\u2026 Creating Statically Provisioned Volumes This section covers how to create statically provisioned volumes. First, you need to create a PersistentVolume object in the cluster. Once this object is created you can use this persistent volume object to create a \"Volume\" on TrueFoundry. In this section, we will cover how to use the following a volume: GCS Bucket S3 bucket Existing EFS Any general volume on Kubernetes Mount a GCS bucket as volume To mount a GCS bucket as a volume on truefoundry, you need to follow the following steps. You can refer to this document for more details: Create a GCS bucket Create a GCS bucket and ensure the following: Should be single region ( multi-region will work but speed will be slower and costs will be higher ) Region should be the same as that of your Kubernetes Cluster Create Serviceaccount and Grant relevant permissions You need to run the following script. This does the following: Enables GCS Fuse Driver on the cluster Create IAM Policy to access your bucket Create K8s service-account and add policy to this service-account Enables role-binding of service account to the desired K8s namespace. Shell #!/bin/bash PROJECT=<Enter your Project Name> CLUSTER_NAME=<Enter your Cluster Name> REGION=<Region of your Cluster> BUCKET_NAME=<Name of GCS Bucket> TARGET_NAMESPACE=<Namespace where you want to mount your GCS bucket> # Clip bucket name to 20 characters CLIPPED_BUCKET_NAME=\"${BUCKET_NAME:0:20}\" K8S_SA_NAME=\"$CLIPPED_BUCKET_NAME-bucket-sa\" GCP_SA_NAME=\"$CLIPPED_BUCKET_NAME-gcp-sa\" GCP_SA_ID=\"$GCP_SA_NAME@$PROJECT.iam.gserviceaccount.com\" gcloud container clusters update $CLUSTER_NAME \\ --update-addons GcsFuseCsiDriver=ENABLED \\ --region=$REGION \\ --project=$PROJECT gcloud iam service-accounts create $GCP_SA_NAME --project=$PROJECT gcloud storage buckets add-iam-policy-binding gs://$BUCKET_NAME \\ --member \"serviceAccount:$GCP_SA_ID\" \\ --role \"roles/storage.objectAdmin\" \\ --project $PROJECT gcloud iam service-accounts add-iam-policy-binding $GCP_SA_ID \\ --role roles/iam.workloadIdentityUser \\ --member \"serviceAccount:$PROJECT.svc.id.goog[$TARGET_NAMESPACE/$K8S_SA_NAME]\" \\ --project $PROJECT Create Service-Account in the Namespace We now need to create a serviceaccount on truefoundry in the same namespace with name: TARGET_NAMESPACE and the serviceaccount must have the name K8S_SA_NAME . apiVersion: v1 kind: ServiceAccount metadata: name: <Enter K8S_SA_NAME same from above step> namespace: <NAMESPACE> annotations: iam.gke.io/gcp-service-account: <Enter GCP IAM Principal Here> \ud83d\udcd8 Important The service account name and workspace should be exactly same as the previous step. Create a PersistentVolume object as shown below: Create a persistent volume object with the following step. (by doing a kubectl apply ) YAML apiVersion: v1 kind: PersistentVolume metadata: name: <Enter a unique PV name here> spec: capacity: storage: 30Gi csi: driver: gcsfuse.csi.storage.gke.io volumeHandle: <Enter your Bucket Name here> accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: np-fuse-test mountOptions: - implicit-dirs volumeMode: Filesystem Create a Volume on TrueFoundry: Please follow this section to create volume on TrueFoundry Mount an S3 bucket as a Volume To mount an S3 bucket as a volume on Truefoundry, you need to follow the following steps: Setting up IAM Policies and Relevant Roles Please follow this document of AWS to set up mount point of S3 in an EKS cluster. This will guide you to do the following things: Create an IAM policy to give permissions for mount point to access the s3 bucket Create an IAM role. Install the mountpoint for Amazon S3 CSI driver and attach the role that was created above. Creating a Persistent Volume on the Kubernetes Cluster Create a PV with the following spec (by doing a kubectl apply ): YAML apiVersion: v1 kind: PersistentVolume metadata: name: <Enter persistent volume name here> spec: capacity: storage: 100Gi csi: driver: s3.csi.aws.com volumeHandle: s3-csi-driver-volume # must be unique volumeAttributes: bucketName: <Enter s3 bucket name here> accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: s3-test # put any value here mountOptions: - allow-delete - region <enter aws region here> - allow-other - uid=1000 volumeMode: Filesystem Create a Volume on TrueFoundry: Please follow this section to create volume on TrueFoundry Mount an Existing EFS as a Volume To mount a S3 bucket as a volume on truefoundry, you need to follow the following steps: Install EFS CSI driver on your cluster To install EFS CSI driver on your cluster, go to Truefoundry UI -> Clusters -> Installed Applications -> Manage From the Volumes section click on install AWS EFS CSI driver and click on Install. Create an Access Point for your EFS Locate your EFS in the AWS console and open it. Ensure that the EFS and the K8S cluster are in the same VPC. Click on \"Create access point\" Enter details like name, Root directory path (please make sure you fill the Root directory creation permissions section, can fill in with UID:1000, GID:1000 if you want to attach it to notebook) Click on create. Create a PersistentVolume on the cluster Create a PV with the following spec (by doing a kubectl apply ): YAML apiVersion: v1 kind: PersistentVolume metadata: name: <Enter your PV name here> spec: capacity: storage: 5Gi # this number doesn't matter for EFS, any number will work csi: driver: efs.csi.aws.com volumeHandle: <file_system_id>::<access_point_id> # e.g. fs-036e93cbb1fabcdef::fsap-0923ac354cqwerty accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: efs-sc volumeMode: Filesystem Create a Volume on TrueFoundry: Please follow this section to create volume on TrueFoundry Updated 5 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-async-service": "Introduction to Async Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to Async Service All Pages Start typing to search\u2026 Introduction to Async Service Async Service in Truefoundry allows us to put input requests in a queue, process them asynchronously and then put the outputs in another queue. You can use an async service if one or more of the following scenarios applies to your use case: Large payload size: If the input to your service / model is really large, it might be useful to put the payload in S3 or some blob storage and then trigger the async service to process the payload. The async service can then download the S3 object and do the processing. Large processing time: If the time takes to respond to a request is high(from seconds to minutes), it almost becomes essential to use an async service since HTTP requests in a normal service can start timing out. Scale to 0: You can scale an async service to 0 if there are no items in the queue. High reliability in case of traffic surges: If you are using a simple HTTP service and traffic suddenly rises, your service will start throwing 5XX errors in case autoscaling is slow. Async service offers a higher reliability since the messages will be stored in the queue and processed when the service scales up. To build an async service, we need to provision an input queue where we will be writing the messages. Truefoundry supports the following queues for input queues. Each of the queue points to a page on how you can provision the queue. AWS SQS Nats queue Kafka Google AMQP You can consume messages from the queue either via our async library or via a sidecar. Use async library Since an async service consumes messages from a queue, we will need the logic to pull and push items to the queue. While you can write this logic in your own code, it can be cumbersome and you will have to change your code if you ever decide to switch the queue system underneath. To overcome this, Truefoundry provides a simple open-source Python library that integrates with different queue frameworks and you just need to implement the processing handler. You can follow the README in the Github repository to integrate it into your codebase. Architecture: \ud83d\udcd8 We recommend using the Python Library only if your processing code is in Python and the time to process is greater than 1 minute. For all other use cases, the approach provided below using a sidecar is recommended. Use sidecar In this case, the python library mentioned above is packaged into another docker image and runs alongside your main code in a separate container as a sidecar. You will need to write your processing code as HTTP service. Architecture The tfy-async-sidecar component consumes items from the queue and calls your HTTP service with the payload as a POST request. The tfy-async sidecar code is open source and available here . It has adapters to consume the messages from the different types of queue as mentioned above. If you need support for additional queues please feel free to reach out, or contribute the queue adapter on this repository . Once the sidecar pulls the message from the queue, it needs to deliver it to the HTTP service which is written by the user. There are no constraints on the HTTP service and you can write it using your own preferred framework like FastAPI, Flask, ExpressJS, etc. The service needs to expose an endpoint where it will accept the messages in the queue as the body in the POST request. You can provide that endpoint as an input in the async service deployment spec. After the HTTP service receives the message, it processes it and returns the response to the sidecar. If we configure an output queue, the sidecar can then go and write the data back to the output queue. You can also handle the writing in your service code and not add the output queue configuration if you want. Acking Logic with the Input queue The sidecar acks the message to the input queue once it has received the response from the HTTP service and written the response to the output queue. If there is a failure at any of the intermediate steps, the queue will redeliver the message to one of the replicas again for processing. This ensures that there is a high level of reliability for the input messages to be processed. Getting Started If you're new to the Truefoundry Async Service, follow the comprehensive guide given below on how to deploy your service as an asynchronous service. Please make sure you have deployed a simple service on Truefoundry before embarking on this tutorial. You will need to have the following components ready before deploying an async service. HTTP service Provisioned Queue You can follow Deploy your first Async Service guide to understand the basics of deploying Async Service. You can configure the settings further using the guides below: Dockerize the code to be deployed. Define the resources requirements for your service - Define Resources - While the documentation is specific for services, it's the same as what is required for Async services. Define Queue Configuration [Optional] Defining environment variables and secrets to be injected into the code - Environment Variables and Secrets . Update, Rollback, Promote your Async Service : While the documentation is specifically for Services, the Update, Rollback, and Promote process follows a similar flow for Jobs. Setting up CI/CD for your Async Service : While the documentation is specifically for Services, the CI/CD setup process follows a similar flow for Jobs. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/deploy-your-first-asyncservice": "Deploy your first Async Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy your first Async Service All Pages Start typing to search\u2026 Deploy your first Async Service To deploy an async service, we will need to write an HTTP service using any framework like Fastapi, Flask, etc. In this example, we will be using FastAPI . We will need to expose a POST endpoint. We will also need to provision a queue to which we will be pushing the messages. Pre-requisites Before you proceed with the guide, please ensure that you have setup a Workspace. To deploy your service, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace HTTP Service We've already created a FastAPI service for the Iris classification problem, and you can find the code in our GitHub Repository . Please visit the repository to familiarize yourself with the code you'll be deploying. Project Structure The project files are organized as follows: Text . \u251c\u2500\u2500 app.py - Contains FastAPI code for inference. \u251c\u2500\u2500 iris_classifier.joblib - The model file. \u2514\u2500\u2500 requirements.txt - Lists dependencies. All these files are located in the same directory. Queue We will be using SQS for this first example. You can use any of the other queue integrations if you want. To provision the AWS SQS Queue, you can refer to the AWS SQS guide. Initiating Deployment via UI Use these configs for the deployment form: Repo URL : https://github.com/truefoundry/getting-started-examples Path to build context : ./deploy-ml-model/ Command : uvicorn app:app --host 0.0.0.0 --port 8000 Port : 8000 Destination URL (Inside Sidecar): http://0.0.0.0:8000/predict \ud83d\udcd8 What we did above: In the example above, We firstly specified the details regarding our HTTP Service. Given we only had Python code and a requirements.txt. We didn't have a prewritten docker file - so we chose the Python Code option - to let Truefoundry templatize a Dockerfile from the details provided about the application and build the Docker image for us. We give these details in the Build context field, where we specify the directory in the GitHub repository where our service code resides ( ./deploy-ml-model/ ). We also specify the command that we need to use to run our service ( uvicorn app:app --host 0.0.0.0 --port 8000 ). Next, we setup the AWS SQS Queue following the Creating and Utilizing an AWS SQS Queue documentation. We specify the port that we want our service to listen on ( 8000 ). Finally, we configure the Sidecar . Sidecar is a component that will consume message from a queue and forwards them as POST requests to your HTTP service. View your deployed service Once you click Submit , your deployment will be successful in a few seconds, and your Async Service will be displayed as active (green), indicating that it's up and running. Congratulations! You've successfully deployed your first Async service. Interacting with the Application Async Service Specific Dashboard Sending Requests to your Async Service The way to trigger the async service will be to push messages to the queue so that the sidecar and pull messages from the queue and call the HTTP service that we have written above. We can verify if things are working by seeing the logs of the service. To dump messages into the queue, we will have to use the queue library to push messages to the queue. A sample code to push messages to the SQS queue is as follows: import json import uuid import boto3 def send_request(input_sqs_url: str): sqs = boto3.client(\"sqs\") request_id = str(uuid.uuid4()) payload = { \"sepal_length\": 7.0, \"sepal_width\": 3.2, \"petal_length\": 4.7, \"petal_width\": 1.4, } sqs.send_message( QueueUrl=input_sqs_url, MessageBody=json.dumps({\"request_id\": request_id, \"body\": payload}) ) if __name__ == \"__main__\": send_request(input_sqs_url=\"YOUR_INPUT_SQS_URL\") Run the above Python script using python send_async_request.py Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configure-ports": "Configure Ports Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Ports All Pages Start typing to search\u2026 Configure Ports If you are using Async Service via the library route, you need not have any ports exposed in your service. However, if you are using HTTP service along with a sidecar, you need to provide a port on your HTTP service on which the sidecar will send the request. You need not expose this port externally since the sidecar calls the HTTP service calls it via the localhost url. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configure-queue": "Configure Queue Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Queue All Pages Start typing to search\u2026 Configure Queue The Queue configuration is the most important configuration in the Async Service. You will find these settings under the Worker Config section in the form. There are two sections: Input Config and Output Config Input Config This is the input queue configuration and has to be provided. This configuration comprises of primarily two parts: Url of the Queueing system. Authentication config to talk to the queueing system. The exact settings differs based on the queue. You can find more details about it in the Queue Integrations for AWS SQS, Kafka, NATS and Google AQMP. OutputConfig The output queue configuration is optional and only needed if you want to sidecar to write the output back to a queue. In many cases, your HTTP service might be writing the output directly to S3 or updating some database - in which case you can leave the output config empty. If you want to write the output back to a queue so that it can be consumed by some other system, then we will need to provision a output queue. Truefoundry supports the following queue integrations for Output Queue: SQS, Core Nats, Nats, Kafka and AMQP. If you want to support for some other queue, feel free to reach out to us or contribute the queue adapter at https://github.com/truefoundry/async_service . The settings for the Queue are very similar to the input configs and can be found in the docs of the individual Queue integrations. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/autoscaling": "Autoscaling Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Autoscaling All Pages Start typing to search\u2026 Autoscaling When traffic or resource usage isn't constant, we want the number of replicas to dynamically adjust based on the Queue Backlog. We need to define the minimum and maximum number of replicas in this case and the autoscaling strategy (based on the Queue Backlog) will decide what should be the number of replicas between the min and max replicas. Autoscaling Configuration Autoscaling configuration involves setting minimum and maximum replica counts as well as defining metrics that trigger autoscaling actions. Here are the available settings for autoscaling: Minimum Replicas : The minimum number of replicas to keep available. Maximum Replicas : The maximum number of replicas to keep available. Cooldown Period (Advanced Settings): The period to wait after the last trigger is reported active before scaling the resource back to 0. Polling Interval (Advanced Settings): This is the interval to check each trigger on Configuring Autoscaling via UI To configure autoscaling parameters for your service via the UI, follow these steps: Async Service Autoscaling Metrics For your async service, autoscaling metrics play a crucial role in dynamically adjusting resource allocation to meet changing demands while maintaining optimal performance. AWS SQS Average Backlog : AWS SQS pending queue length averaged over all replicas which autoscaler will try to maintain. This option is only available when the Input Worker is based on AWS SQS. Learn more about AWS SQS Backlog Metric Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configure-sidecar": "Configure Sidecar Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Sidecar All Pages Start typing to search\u2026 Configure Sidecar To run the async service in sidecar mode as mentioned in Introduction , we will need to enable the sidecar in the deployment form. This is available in the advanced settings in the Deployment form. We will have to provide the url in your service to which the sidecar can send the HTTP request. For e.g. if you have a path called /predict in your service and you are running it on port 8000, the destination url will be http://0.0.0.0:8000/predict \ud83d\udcd8 Sidecar destination url should be http://0.0.0.0:<port>/<path> The port number is the same as what you provided in the Ports section in the deployment form. The reason you have to provide it here again since you might have multiple ports and you want the sidecar to call one of the ports specified. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/aws-sqs": "AWS SQS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account AWS SQS All Pages Start typing to search\u2026 AWS SQS Provisioning the queue Creating an AWS Simple Queue Service (SQS) queue is a straightforward process that can be accomplished using the AWS Management Console or AWS Command Line Interface (CLI). Here's a step-by-step guide for creating an AWS SQS queue through the AWS Management Console: \u2757\ufe0f Note: The visibility timeout has a significant impact on the behavior of asynchronous services. It determines how long a message remains hidden from consumers after it's fetched from the queue, playing a vital role in preventing multiple consumers from processing the same message concurrently. For example, if your worker process typically takes around 5 seconds to complete a task, it's advisable to set the Visibility Timeout to at least twice that duration, which in this case would be 10 seconds. If the Visibility Timeout is set too low, there's a risk of multiple consumers attempting to process the same message simultaneously, potentially leading to conflicts and errors in your system. It's essential to strike the right balance to ensure efficient and orderly message processing. Once you click on Create queue , you'll receive a confirmation message indicating the successful creation of the queue. Configuring Truefoundry Async Service with AWS SQS You will have to specify these configurations for AWS SQS Input Worker: Configuring Autoscaling for AWS SQS Queue AWS SQS Average Backlog is defined as the AWS SQS pending queue length averaged over all replicas that the autoscale is designed to maintain. The pending queue length refers to the number of messages that are currently in the queue but have not yet been processed. These are messages waiting to be consumed and processed by the relevant workers or services. The average backlog is the average or mean value of the pending queue length across multiple replicas. In a distributed and auto-scaling system, there can be multiple instances or replicas of your service, each with its queue. The average backlog provides a way to measure the workload across all replicas. This Average Backlog is a valuable metric for determining how to scale your application efficiently. \ud83d\udcd8 Note: This metric is only available in case you are using AWS SQS for your input queue Parameters for SQS Average Backlog Queue lag threshold : This is the maximum number of messages each replica should handle. If there are more messages than the threshold, the auto-scaler adds replicas to share the workload. Configuring AWS SQS Average Backlog Through the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: Diff from truefoundry.deploy import AsynceService, Build, DockerFileBuild, Port, AsyncServiceAutoscaling,SQSQueueMetricConfig service = AsyncService( name=\"my-async-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + replicas = AsyncServiceAutoscaling( + min_replicas=1, + max_replicas=3, + metrics=SQSQueueMetricConfig( + queue_length=30 + ), + cooldown_period=300, + polling_interval=30 + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/monitoring-your-async-service": "Monitor your Async Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Monitor your Async Service All Pages Start typing to search\u2026 Monitor your Async Service Async service involves your main HTTP service code which you are writing and a sidecar component which picks up items from the queue and calls yoru HTTP service with the payload. The sidecar code has been written by Truefoundry and emits useful metrics which can help you understand the overall state of the system. Metrics To see the metrics for a particular Async Service, you need to select that Async Service from the deployments section, then click on the Metrics button in the top right corner of the dashboard. The key metrics to monitor and pay attention to are: Consumer Lag : If you had to choose one metric to look at to understand the state of the service, Consumer Lag would be that metric. Consumer Lag denotes how many items are present in the queue that have not yet been processed by your service. Ideally the consumer lag should be less than the number of messages that one pod can process at any given point of time. If the consumer lag rises, it means that your service is processing messages at a slower rate than the rate at which items are enqueue into the queue. If its rising, you should increase the number of replicas of your service so that your processing rate can be faster. Messages Processing Rate : This number shows how many messages per second if your service processing. This is the combined processing rate of all the pods of your service. This number should be equal to the rate at which messages are enqueud in the queue, else the consumer lag will start growing. Processing Time (ms) : Processing time is the time taken by your service to process the message. This will be the same time which your service would have taken if it would have been directly invoked via HTTP. This is important to understand the latency that the clients will incur for each request. Please note that the total latency that the client will see is more than the processing time. Total Latency = Time to put message in queue + time spent in queue + time to move message from queue to HTTP service by sidecar + processing time + response publish time Response Publish Time(ms) : This is the time taken by sidecar to publish the response received from the HTTP service to the output queue. This number should usually be very small unless the sidecar is having issues talking to the output queue. Message Processing Failure % (%): Tracks the percentage of messages for which your service returns non-2XX HTTP codes. This helps keep track of the cases when your service is either erroring out or you have intentionally provided error return codes. Consumer Pull Failure Rate: This denotes errors from sidecar while pulling messages from queue and should almost never happen. The most probably reason for this number being greater than 0 will be issue connecting to the input queue which might be a network or authentication error. If you are ever encountering this, please see the logs of the service from async_processor and that should indicate the error. Logs Service Level Logs You can see your own HTTP service logs by clicking on the logs button. You will see your own service logs inteleaved with the sidecar logs. All the log lines starting with async_processor:INFO belong to the sidecar. In steady state, you will hardly see any logs from the sidecar and most logs will belong to your service. The sidecar outputs the logs when its facing any sort of errors which might help you debug your service. Pod Level Logs You can also filter out the sidecar logs in the pod logs by choosing the sidecar container. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/introduction-to-secrets": "Introduction to Secrets Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Introduction to Secrets All Pages Start typing to search\u2026 Introduction to Secrets When building applications, you might need to use sensitive information like API keys and database passwords in your code. Storing them in code is not a great idea due to security reasons - hence they are injected as environment variables. However, the values of these environment variables cannot be exposed to all developers. For e.g. everyone shouldn't know the password of the production database. These values are then stored in Secret Managers like AWS SSM, GCP Secret Manager, Azure Vault, etc and injected in the code as environment variables. Truefoundry makes it easy to store the actual values in the SecretManagers that you are using and then inject them into the code without exposing the value of these keys to developers. To do this, you can integrate your preferred secret manager with Truefoundry and then store the secret using Truefoundry's UI. Whenever you save a secret using Truefoundry's UI, the actual secrets are stores in your SecretManager and Truefoundry never stores the secret with itself. You will then get a fqn (fully-qualified-name) for your secret which you can then use in your deployments. Secret groups In a project, you likely have a set of secrets associated with it. Managing access to each individual secret can become cumberson. Hence, we have the concept of secretgroups which lets you organize and manage related secrets for a specific project. Within a Secret Group, you can easily add, remove, and update secrets. You can configure access control on each secret group and grant users/teams read, write or admin access. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/manage-secrets": "Manage Secrets Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Manage Secrets All Pages Start typing to search\u2026 Manage Secrets Store Secrets in your Secret Store with Truefoundry Suppose your backend service needs to load a database password and an API key for an external service. You can create a secret group for that backend service and add the database password and the API key as secrets under that secret group. To do so, follow the steps below: Collaborators & Access control There are 3 roles possible in Secret groups - Secret Group Admin, Secret Group Editor, and Secret Group Viewer. Each granting permissions as shown below: By default, a tenant admin has access to all secret groups. For tenant members and teams you need to assign roles for each secret group. Use Secrets in your Secret Store You can now copy your Secret's FQN using the instructions given below. You can also see in which deployments, the secrets are being used. Attaching Secrets to a Deployment You can attach and use your Secrets in various types of deployments, including Services and Jobs. Detailed instructions for using Secrets in Services and Jobs can be found in the following guides: Using Secrets in Services Using Secrets in Jobs Updated 5 months ago",
    "https://docs.truefoundry.com/docs/collaboration-and-access-control": "Collaboration and Access Control Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Collaboration and Access Control All Pages Start typing to search\u2026 Collaboration and Access Control Truefoundry provides a suite of collaboration features with role-based access control to manage projects among users and teams. Using Truefoundry, you can achieve the following: Invite your team members to join Truefoundry Add team members to different entities like Cluster, Workspace, MLRepo, DockerRegistry, and SecretGroup (You can understand more about the permission management system at Truefoundry here Create teams to provide access to similar resources to a group of people. Users Inviting Users to the Platform You can invite users to join the TrueFoundry platform following the instructions below. An email notification will be sent to the specified email address, prompting the user to create their TrueFoundry account and join your team. Roles for a user Within the TrueFoundry platform, there are two primary user roles: Admins : These individuals hold the highest level of access and are responsible for managing the overall TrueFoundry environment. They have full control over resources, including users, clusters, and workspaces. Usually there should be only a few admins in an organization. Members : These are the general users of the platform. They have access to resources that have been explicitly granted to them, typically aligned with their specific projects or tasks. Updating roles The role associated with a user can only be modified by Admins . To update a user's role, follow these steps: Deactivating a User Adminstators can deactivate a user's account. This will prevent the user from logging in to the platform. Resetting Password Administrators can initiate a password reset process for a user. This will send an email to the user with a link to reset their password. Teams It might get cumbersome to add each individual user to each resource repeatedly. To solve this problem, we have the concept of teams using which you can add a team to a resource and then add or remove members from the team. Create Team To create a team in TrueFoundry, follow these steps: Grant Access of a Resource to Team Once a team has been created, you can grant it access to resources. To do so, follow these steps: Virtual Accounts Create and assign permissions to a Virtual account as shown below: Configure Access Control In TrueFoundry, are organized around the following key entities that help manage the entire organization's resources effectively. Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/truefoundry-architecture": "Modes of Deployment Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Modes of Deployment All Pages Start typing to search\u2026 Modes of Deployment Understanding modes of deployment in truefoundry Truefoundry provides a split plane architecture which comprises of a control plane and a compute plane. The control-plane orchestrates the deployments across multiple compute plane Kubernetes clusters connected to the control plane. You can read more about the Truefoundry architecture TrueFoundry Split-Plane Architecture Because of this, Truefoundry can be deployed in two ways on your own cloud: Compute-plane in your own cloud environment and use hosted Truefoundry control plane Truefoundry hosts a multi-tenant control plane which can orchestrate deployments across multiple compute-planes. In this case, you create a Kubernetes cluster in your own cloud account and connect it to the hosted control plane. The connection is done via installing the tfy-agent in your own kubernetes cluster which helps it connect to the hosted control plane. In this case, all compute and data stays in your own cloud environment - however the control plane has access to fetch the artifacts and logs of your applications. This is a more managed setup wherein the upgrade of Truefoundry is automatically handled by the Truefoundry team. To understand all the steps involved in connecting cluster to control plane, you can read Deploy Compute Plane . To get started with deploying, follow the guide below based on your cloud provider. Deploy Compute Plane on AWS Deploy Compute Plane on GCP Deploy Compute Plane on Azure Both compute and control plane in your own cloud environments (Enterprise Plan) In this case, both the control plane and the compute plane are hosted on your own cloud environment. The control plane comprises of the following components: Truefoundry helm chart (This can be installed on any Kubernetes cluster - including one of the compute plane clusters itself) Postgres Database One Blob Storage (S3 bucket / Azure Container / Google Storage Bucket / Minio) To get started with installing the control plane and compute plane, please follow the guide: Deploy Control Plane and Compute Plane Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/deploy-compute-plane": "Deploy Compute Plane Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Compute Plane All Pages Start typing to search\u2026 Deploy Compute Plane Connect Kubernetes Cluster in your cloud account to Truefoundry Control Plane In this mode, we will be connecting a Kubernetes cluster to the Truefoundry control plane. The steps will vary slightly according based on your cloud provider - AWS, GCP or Azure. The key outline of the steps that need to be done are mentioned below. The entire setup process takes between 30 mins to 1 hour if you are using our onboarding cli script. 1. Connect your Kubernetes Cluster You can choose to either create or onboard an existing cluster to Truefoundry platform. You can follow the guides for AWS, GCP or Azure to connect your cluster. 2. Connect your Docker Registry You need to integrate your docker registry so that the control plane can push the docker images to your registry after building the source code. TrueFoundry providers integrations for all the major docker registries. Follow the guides below to integrate your docker registry. AWS ECR Google Artifact Registry Azure Container Registry DockerHub Quay If you are using the Truefoundry generated terraform code, docker registry for all the major cloud providers will be created by default. You just need to check the output of the terraform run to connect the docker registry for AWS, GCP and Azure. 3. Connect your Blob Storage (AWS S3 / GCS Bucket / Azure Container) We need to integrate with atleast one blob storage bucket to store the ML models and artifacts. You can integrate multiple blob storage buckets to segregate development and production environments. If you are using Truefoundry generated terraform code then a bucket in the respective cloud provider is created by default. To access these buckets a role is also created which can access it. 4. Connect Your Secret Store (OPTIONAL) You can integrate your secret store with Truefoundry so that developers can save the secrets and use them in their applications. If a secret store is integrated, all secrets are stored in your actual Secret Store and Truefoundry only store the link to the secret in its own databases. This makes sure that your secrets are not stored in the ControlPlane. Follow the guides below to integrate with the following secret store: AWS SSM Google Secrets Manager Azure Vault If you are using Truefoundry generated terraform code then access to SSM is created through a role which can access it. 5. Connect Git Repository (OPTIONAL) Integrating with the Git repository allows developers to deploy directly from their Git repository by specifying the repository name, branch and commit SHA. Follow the guides below to integrate with the following Git repositories. Install Application Components Truefoundry requires some open-source components to be installed on the cluster to be able to deploy services and models. If you are using the Truefoundry generated terraform code, it will automatically setup all the components for you. You can install the applications or view the installed applications on each of the cluster in Truefoundry. The set of mandatory dependencies are: ArgoCD , Argo Rollout : TrueFoundry relies on ArgoCD Application object to deploy the applications inside a Kubernetes cluster. The infra applications are deployed in the default project in argocd while the user deployed applications are deployed in tfy-apps project. ArgoRollouts is used to power the rollout process of deployments - enabling blue-green, canary and different rollout strategies. Argo Workflows : Truefoundry uses ArgoRollouts to run the jobs deployed on the platform. Istio : Istio is used as the Ingress controller and also to power functionalities of Oauth authentication for Notebooks and traffic shaping abilities. Truefoundry doesn't impose the sidecar inject by default - its only done if we try to do request count based autoscaling or are trying to mirror or intercept traffic. The rest of the dependencies are more use-case based and optional depending on if you are using that feature. Keda for workload Autoscaling : Truefoundry uses Keda to autoscale your workloads based on time, requests count, CPU or queue length. Prometheus for Metrics : Prometheus powers the metrics dashboard in Truefoundry. Prometheus also helps provide some of the metrics for autoscaling. Loki for Logs : The logs feature in Truefoundry is powered via Loki. This is optional and you can choose to provide your own logging solution. GPU operator : This is Truefoundry provided helm chart for brining up GPU nodes in different clouds. Its based on Nvidia's GPU operator. Grafana : You can install the Truefoundry grafana helm chart that comes with a lot of inbuilt dashboards for cluster monitoring. AWS Specific Components: Metrics-Server : This is required on AWS EKS cluster for metrics collection. AWS Ebs CSI Driver : This is required for supporting EBS volumes on EKS cluster. AWS Efs CSI Driver : Required for supporting EFS volumes for EFS cluster. TFY Inferentia operator : This is required for supporting Inferentia machines on EKS. Azure Specific Components: Cert-Manager : This is needed for provisioning certificates on Azure AKS cluster. Updated 4 months ago",
    "https://docs.truefoundry.com/docs/overview": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on AWS TrueFoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled EKS cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VPC + New EKS cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VPC + New EKS cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing EKS cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for AWS EKS depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue after making sure you have all the requirements satisfied. A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/infrastructure-requirements": "Architecture Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Architecture All Pages Start typing to search\u2026 Architecture This guide describes the architecture diagram and created access policies in a compute plane in your AWS account Please refer to the \"Access Policies\" section for details of each access policy that is present in a truefoundry compute. Access Policies Overview Access Policy Role Reason ELBControllerPolicy - link <cluster_name>-elb-controller Role assumed by load balancer controller to provision ELB when a service of type LoadBalancer is created KarpenterPolicy - link SQSPolicy - link <cluster_name>-karpenter Role assumed by Karpenter to dynamically provision nodes. Karpenter has an additional role to listen to interruption events coming from SQS to safely handle spot node termination EFSPolicy - link <cluster_name>-efs Role assumed by EFS CSI to provision and attach EFS volumes EBSPolicy - link <cluster_name>-csi-ebs Role assumed by EBS CSI to provision and attach EBS volumes RolePolicy - ECR - link S3 - link SSM - link <cluster_name>-platform-iam-role Role assumed by TrueFoundry to allow for ECR S3 SSM The role attaches these policies - AmazonEKSClusterPolicy - link AmazonEKSVPCResourceControllerPolicy - link EncryptionPolicy to create and manage key for encryption:\\ { \"Statement\": \\[ { \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ListGrants\", \"kms:DescribeKey\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:kms:<region>:\\<acc_id>:key/\\<key_id>\" } ], \"Version\": \"2012-10-17\" } <cluster_name>-cluster-<random_string> This role provides Kubernetes the permissions needed to manage the cluster. This includes permissions needed to Manage the end to end lifecycle of EC2 instances used as EKS nodes Assign networking components to EC2 instances Perform encryption at rest The role attaches these policies - AmazonEC2ContainerRegistryReadOnlyPolicy - link AmazonEKS_CNI_Policy - link AmazonEKSWorkerNodePolicy - link AmazonSSMManagedInstanceCorePolicy - link initial-eks-node-group-<random_string> Role assumed by EKS nodes to work with the AWS resources for these purposes - Pull images from ECR Assign IPs to the EC2 instance Register itself with the cluster Perform disk encryption Updated 5 months ago",
    "https://docs.truefoundry.com/docs/requirements": "Requirements Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Requirements All Pages Start typing to search\u2026 Requirements Requirements for Truefoundry installation on AWS Following is the list of requirements to set up compute plane in your AWS account AWS Infra Requirements New VPC + New Cluster These are the requirements for a fresh Truefoundry installation. If you are reusing an existing network or cluster, refer to the sections further below, in addition to this one Requirements Description Reason for Requirement AWS Account Billing must be enabled for the AWS account. VPC VPC CIDR should be atleast /20 for the VPC Min 2 availability zone and /24 for private subnets This is needed to ensure around 250 instances and 4096 pods can be run in the Kubernetes cluster. If we expect the scale to be higher, the subnet range should be increased. A NAT Gateway must be connected in the VPC and the route tables should allow outbound internet access for private subnets through this NAT gateway. Egress access For Docker Registry public.ecr.aws quay.io ghcr.io docker.io/truefoundrycloud docker.io/natsio nvcr.io registry.k8s.io This is to download docker images for TrueFoundry, ArgoCD, NATS, GPU operator, ArgoRollouts, ArgoWorkflows, Istio, Keda. DNS with SSL/TLS Set of endpoints (preferably wildcard) to point to the deployments being made. Something like *.internal.example.com, *.external.example.com. An ACM certificate with the chosen domains as SAN is required in the same region When developers deploy their services, they will need to access the endpoints of their services to test it out or call from other services. This is why we need the DNS along with TLS on the compute plane. Its better if we can make it a wildcard since then developers can deploy services like service1.internal.example.com, service2.internal.example.com ACM Certificate We need to have a certificate for the domains listed above. The certificate ARN will be passed to the Istio Ingress config. If you have a certificate from some other source, that can also work by creating a secret with the certificate in istio-system namespace. Cloud Quotas GPU If you are planning to use GPU machines, make sure you have quotas for: G and VT Spot/On-demand Instances P Spot/On-demand Instance Requests Inferentia (Optional) If you are planning to use Inferentia machines, make sure you have quota for Inferentia Spot/On-demand machines. This is to make sure that TrueFoundry can bring up the instances as requested by developers. A request needs to be raised to AWS for increasing the limits for instances in case we don't have quotas. You can check and increase your quotas at AWS EC2 service quotas User / ServiceAccount to provision the cluster sts must be enabled for the user which is being used to create the cluster. User must have the list of permissions listed below See Enabling STS in a region Existing network Requirements Description Reason for Requirement VPC Minimum 2 private subnets in different availability zone with min CIDR /24 Tags should be present on the VPC as described below NAT gateway for private subnets Minimum 1 public subnet for a public load balancer if endpoints are to be exposed to internet. Auto-assign IP address must be enabled. Min CIDR /28 DNS support and DNS hostnames must be enabled for your VPC This is needed to ensure around 250 instances and 4096 pods can be run in the Kubernetes cluster. If we expect the scale to be higher, the subnet range should be increased. A NAT Gateway must be connected in the VPC and the route tables should allow outbound internet access for private subnets through this NAT gateway. VPC Tags Your subnets must have the following tags for the Truefoundry terraform code to work with them. You can skip it if you are creating a new network in which case these will automatically be created. Resource Type Required Tags Private Subnets \"kubernetes.io/cluster/${clusterName}\": \"shared\" \"subnet\": \"private\" \"kubernetes.io/role/internal-elb\": \"1\" Public Subnets \"kubernetes.io/cluster/${clusterName}\": \"shared\" \"subnet\": \"public\" \"kubernetes.io/role/elb\": \"1\" EKS Node Security Group \"karpenter.sh/discovery\": \"${clusterName}\" This tag is required for Karpenter to discover and manage node provisioning for the cluster. Existing cluster Requirements Description Reason for Requirement Compute CPU All Standard (A, C, D, H, I, M, R, T, Z) Spot/On-demand must have min 4vCPU and 8 GB RAM At least 2 nodes should be available for system components EKS Version EKS version 1.30 or higher Required for compatibility with TrueFoundry components and latest security features. Newer versions provide better performance and stability. Storage EBS CSI Driver must be installed Installation Guide for EBS CSI Driver EFS CSI Driver (if using shared storage) Installation Guide for EFS CSI Driver Required for persistent volume provisioning and shared storage support. Load Balancer AWS Load Balancer Controller version 2.12.0 or higher Installation Guide for AWS Load Balancer Controller Appropriate IAM roles for service account (IRSA) Required for Ingress and Service type LoadBalancer support. Updated 8 days ago",
    "https://docs.truefoundry.com/docs/setting-up-dns-and-tls-in-aws": "Setting up DNS and TLS in AWS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Setting up DNS and TLS in AWS All Pages Start typing to search\u2026 Setting up DNS and TLS in AWS To host any service/model endpoints a domain has to be used to expose them to the external world or to an internal network. Below document will help you to set the same in your AWS EKS cluster. Any number of domains can be setup for your cluster. Setting up DNS There are two kind of domains that you can setup for TrueFoundry workloads Wild card domains - *.example.com , *.tfy.example.com , *.ml.example.com Non wild card domains - tfy.example.com , dev.example.com , prod.example.com Wild card domains (recommended) In wild card domains a subdomain wildcard is dedicatedly used to resolve endpoints in the EKS cluster. Some of the samples are given below where example.com is your domain. The services will be exposed like service1.tfy.example.com service2.tfy.example.com Non wild card domains In non-wild card domains a dedicated domain is used to resolve endpoints. Some of the samples for service endpoints will look like tfy.example.com/service1 tfy.example.com/service2 Load balancer IP address Once a domain name is decided a DNS record is to be mapped with the load balancer in the EKS cluster. To get the load balancer's IP address run the following command in your EKS cluster Shell kubectl get svc tfy-istio-ingress -n istio-system -ojsonpath='{.status.loadBalancer.ingress[0].hostname}' Create a DNS record in your route 53 or your DNS provider with the following details Record Type Record Name Record value CNAME *.tfy.example.com LOADBALANCER_IP_ADDRESS Setting up TLS There are three ways primarily through which we can add TLS to the load balancer in AWS: Using AWS Certificate Manager (recommended) - Through this certs get renewed automatically Using Certificate and key files - Through this pre-created certs are added to istio Using cert-manager - This allows you to automatically provision and manage TLS certificates from various issuers (like Let's Encrypt) with any DNS provider Updated 8 days ago",
    "https://docs.truefoundry.com/docs/overview-1": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on GCP Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled GKE cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VPC + New GKE cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VPC + New GKE cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing GKE cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for GCP GKE depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated 3 months ago",
    "https://docs.truefoundry.com/docs/infrastructure-requirements-gcp": "Architecture Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Architecture All Pages Start typing to search\u2026 Architecture This guide describes the architecture diagram and access policies in a compute plane in your GCP account GCP Architecture Diagram Please refer to the \"Access Policies\" section for details of each access policy. Access Policies Access Policy Role Reason RolePolicy - Blob storage - link Secret manager - link Artifact Registry - link Cluster viewer - link Cluster Autoscaler - link <cluster_name>-platform-user Roles assumed by the TrueFoundry user are added for the following reason: To create and manage blob storage buckets To create and manage secrets within secret manager to be used within the platform To pull and push images to artifact registry To enable cloud integration for GCP. This is used to surface node level details in the platform To allow viewing cluster autoscaler logs Updated 3 months ago",
    "https://docs.truefoundry.com/docs/gcp-requirements": "Requirements Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Requirements All Pages Start typing to search\u2026 Requirements Requirements for Truefoundry installation on GCP Following is the list of requirements to set up compute plane in your GCP project GCP Infra Requirements New VPC + New Cluster These are the requirements for a fresh Truefoundry installation. If you are reusing an existing network or cluster, refer to the sections further below, in addition to this one Requirements Description Reason for requirement VPC Any existing or new VPC with the following subnets should work: Minimum subnet CIDR should be /24 Additional ranges: Pods: min /20 CIDR Services: min /22 CIDR Cloud Router, Cloud NAT Port 80/443 ingress from internet Allow all ports ingress inside a subnet Port 443, 8443, 9443 and 15017 for connection to GKE master control plane This is needed to ensure around 250 instances and 4096 pods can be run in the Kubernetes cluster. If we expect the scale to be higher, the subnet range should be increased. Cloud Router and NAT are required for egress internet access. Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, GPU operator, ArgoRollouts, ArgoWorkflows, Istio, Keda. DNS with SSL/TLS Set of endpoints (preferably wildcard) to point to the deployments being made. Something like .internal.example.com, .external.example.com. An ACM certificate with the chose domains as SAN is required in the same region When developers deploy their services, they will need to access the endpoints of their services to test it out or call from other services. This is why we need the second domain name. Its better if we can make it a wildcard since then developers can deploy services like service1.internal.example.com , service2.internal.example.com . We also support path based routing which would make the endpoints internal.example.com/service1 and internal.example.com/service2 Compute - Quotas must be enabled for required CPU and GPU instance types (on-demand and preemptible / spot) This is to make sure TrueFoundry can bring up the machines as needed. User/ServiceAccount to provision the infrastructure - Compute Admin Compute Network Admin Kubernetes Engine Admin Security Admin Service Account Admin Service Account Token Creator Service Account User Storage Admin Service usage Admin These are the permissions required by the IAM user in GCP to create the entire compute-plane infrastructure. Existing Network Requirements Description Reason for requirement VPC Minimum subnet CIDR should be /24 Additional ranges: Pods: min /20 CIDR Services: min /22 CIDR Cloud Router, Cloud NAT Port 80/443 ingress from internet Allow all ports ingress inside a subnet Port 443, 8443, 9443 and 15017 for connection to GKE master control plane This is needed to ensure around 250 instances and 4096 pods can be run in the Kubernetes cluster. If we expect the scale to be higher, the subnet range should be increased. Cloud Router and NAT are required for egress internet access. Existing Cluster Requirements Description Reason for requirement GKE Version GKE version 1.30 or later (recommended) Recommended for latest security features, better performance and stability. Node Auto Provisioning (NAP) Must be enabled and configured with appropriate resource limits Recommended minimum limits: CPU: 1000 cores Memory: 4000 GB GPU quotas (if using accelerators) Required for automatic node provisioning based on workload demands. This ensures efficient resource allocation and scaling for ML workloads without manual intervention. Workload Identity Must be enabled on the cluster Configured with PROJECT_ID.svc.id.goog workload pool Node pools must have Workload Identity enabled Required for secure service account access to GCP resources. Eliminates the need for service account keys and provides fine-grained access control for Kubernetes workloads. Node Auto Provisioning Configuration Node Auto Provisioning (NAP) needs to be properly configured with resource limits and defaults. Here's how to configure NAP: To enable NAP on your existing GKE cluster: Bash gcloud container clusters update CLUSTER_NAME \\ --enable-autoprovisioning \\ --min-cpu 0 \\ --max-cpu 1000 \\ --min-memory 0 \\ --max-memory 10000 \\ --location=REGION For GPU support, add GPU resource limits: Bash gcloud container clusters update CLUSTER_NAME \\ --enable-autoprovisioning \\ --autoprovisioning-resource-limits=nvidia-tesla-t4=0:256 \\ --location=REGION You can also enable NAP through the Google Cloud Console: Go to Google Kubernetes Engine > Clusters Select your cluster and click \"Edit\" Under \"Features\" find \"Node Auto-Provisioning\" and enable it Set resource limits for CPU and memory Click \"Save\" For more details, see Google's official documentation on Node Auto-Provisioning . Enable Workload Identity Run the following commands to enable workload identity: Bash # Enable Workload Identity feature gcloud container clusters update CLUSTER_NAME \\ --workload-pool=PROJECT_ID.svc.id.goog \\ --region=REGION # Enable Workload Identity on the node pool gcloud container node-pools update NODE_POOL_NAME \\ --cluster=CLUSTER_NAME \\ --workload-identity-config=workload-pool=PROJECT_ID.svc.id.goog \\ --region=REGION Permissions required to create the infrastructure The IAM user should have the following permissions Compute Admin Compute Network Admin Kubernetes Engine Admin Security Admin Service Account Admin Service Account Token Creator Service Account User Storage Admin Service usage Admin Updated 8 days ago",
    "https://docs.truefoundry.com/docs/setting-up-dns-and-tls-in-gcp": "Setting up DNS and TLS in GCP Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Setting up DNS and TLS in GCP All Pages Start typing to search\u2026 Setting up DNS and TLS in GCP To host any service/model endpoints a domain has to be used to expose them to the external world or to an internal network. Below document will help you to set the same in your GCP GKE standard cluster. Any number of domains can be setup for your cluster. Setting up DNS There are two kind of domains that you can setup for TrueFoundry workloads Wild card domains - *.example.com , *.tfy.example.com , *.ml.example.com Non wild card domains - tfy.example.com , dev.example.com , prod.example.com Wild card domains (recommended) In wild card domains a subdomain wildcard is dedicatedly used to resolve endpoints in the GKE cluster. Some of the samples are given below where example.com is your domain. The services will be exposed like service1.tfy.example.com service2.tfy.example.com Non wild card domains In non-wild card domains a dedicated domain is used to resolve endpoints. Some of the samples for service endpoints will look like tfy.example.com/service1 tfy.example.com/service2 Load balancer IP address Once a domain name is decided a DNS record is to be mapped with the load balancer IP address in the GKE cluster. To get the load balancer's IP address run the following command Shell kubectl get svc -n istio-system tfy-istio-ingress -ojsonpath='{.status.loadBalancer.ingress[0].ip}' Create a DNS record in your cloud DNS or your DNS provider with the following details Record Type Record Name Record value A * .tfy.example.com LOADBALANCER_IP_ADDRESS Setting up TLS There are two ways primarily through we can add TLS to the load balancer in GCP Using cert-manager + GCP cloud DNS (recommended) - Through this certs get renewed automatically Using Certificate and key files - Through this pre-created certs are added to istio Updated 8 days ago",
    "https://docs.truefoundry.com/docs/overview-2": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on Azure Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled AKS cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VNet + New AKS cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VNet + New AKS cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing AKS cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for Azure AKS depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated 3 months ago",
    "https://docs.truefoundry.com/docs/infrastructure-requirements-azure": "Architecture Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Architecture All Pages Start typing to search\u2026 Architecture This guide describes the architecture diagram and access policies in a typical Truefoundry installation Azure Architecture Diagram Access Policies ACR - We use admin username and password. This is for the platform to be able to push and pull from ACR. Blob storage - We use connection string to get access. The blob storage is used to store model artifacts. Cluster Diagnostic Setting - We use the \"Monitoring Reader\" role to access the cluster autoscaler logs in Log Analytics. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/requirements-azure": "Requirements Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Requirements All Pages Start typing to search\u2026 Requirements Requirements for Truefoundry installation on Azure Following is the list of requirements to set up compute plane in your Azure subscription Azure Infra Requirements New VPC + New Cluster Following is the list of requirements to set up compute plane in your Azure account Requirements Description Reason for requirement Azure Subscription Azure subscription should have billing enabled Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, GPU operator, ArgoRollouts, ArgoWorkflows, Istio, Keda. DNS with SSL/TLS Set of endpoints (preferably wildcard) to point to the deployments being made. Something like .internal.example.com, .external.example.com. Certificate can be generated using cert-manager by creating a few DNS records. Or you can bring your own custom certificate. When developers deploy their services, they will need to access the endpoints of their services to test it out or call from other services. Its better if we can make it a wildcard since then developers can deploy services like service1..internal.example.com, service2.internal.example.com Compute Quotas Quotas need be present to bring up the CPU and GPU machines required for your use case. Viewing quotas in Azure portal Microsoft.Storage Giving access to create storage account and other resource Ensure that Microsoft.Storage resource provider is registered. Check this link Host encryption Ensure that host encryption is enabled Enable host encryption for data at rest. Check this link Existing Network Requirements Description Reason for requirement VPC The existing VNet should have the following available: Min CIDR /24 for the private subnet Pod CIDR - /16 Service CIDR - /20 Networking mode (for existing cluster) - Azure CNI or Azure CNI overlay This is needed to ensure around 250 instances and 4096 pods can be run in the Kubernetes cluster. If we expect the scale to be higher, the subnet range should be increased. Cloud Router and NAT are required for egress internet access. Existing Cluster Requirements Description Reason for requirement Compute *Kubernetes version 1.30 or higher Minimum 3 worker nodes *Each worker node: 4 vCPUs, 16GB RAM For GPU workloads: NVIDIA GPU-enabled nodes *Azure CNI or Azure CNI Overlay networking Sufficient quota for on-demand instances (minimum 50 vCPUs) Sufficient quota for spot instances if using spot node pools (recommended minimum 24 vCPUs) | Required for running core Truefoundry components and user workloads. Spot instances can help optimize costs for interruptible workloads. | Permissions required to create the infrastructure The IAM user should have the following permissions - Contributor Role to the above Subscription Role Based Access Administrator to the above subscription Either Azure AD Administrator or Azure AD Application Developer role to: Create app registrations and service principals Assign Reader role to AD application for read-only AKS cluster access Assign Monitoring Reader role to applications for cluster monitoring Refer: Azure admin permission Updated 8 days ago",
    "https://docs.truefoundry.com/docs/setting-up-dns-and-tls-in-aks": "Setting up DNS and TLS in AKS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Setting up DNS and TLS in AKS All Pages Start typing to search\u2026 Setting up DNS and TLS in AKS To host any service/model endpoints a domain has to be used to expose them to the external world or to an internal network. Below document will help you to set the same in your Azure AKS cluster. Any number of domains can be setup for your cluster. Setting up DNS There are two kind of domains that you can setup for TrueFoundry workloads Wild card domains - *.example.com , *.tfy.example.com , *.ml.example.com Non wild card domains - tfy.example.com , dev.example.com , prod.example.com Wild card domains (recommended) In wild card domains a subdomain wildcard is dedicatedly used to resolve endpoints in the GKE cluster. Some of the samples are given below where example.com is your domain. The services will be exposed like service1.tfy.example.com service2.tfy.example.com Non wild card domains In non-wild card domains a dedicated domain is used to resolve endpoints. Some of the samples for service endpoints will look like tfy.example.com/service1 tfy.example.com/service2 Load balancer IP address Once a domain name is decided a DNS record is to be mapped with the load balancer IP address in the AKS cluster. To get the load balancer's IP address run the following command Shell kubectl get svc -n istio-system tfy-istio-ingress -ojsonpath='{.status.loadBalancer.ingress[0].ip}' Create a DNS record in your cloud DNS or your DNS provider with the following details Record Type Record Name Record value A * .tfy.example.com LOADBALANCER_IP_ADDRESS Setting up TLS in Azure There are two ways primarily through we can add TLS to the load balancer in Azure Using cert-manager + Azure DNS (recommended) - Through this certs get renewed automatically Using Certificate and key files - Through this pre-created certs are added to istio Updated 8 days ago",
    "https://docs.truefoundry.com/docs/overview-3": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on a Generic cluster Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. Scenarios Following scenario is supported in the provided terraform code. You can find the requirements for the scenario here : Existing Network + Existing cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing network and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Attach Existing Cluster You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the cluster to be onboarded. Fill in with your cluster details. Click Submit when done Disable the addons which are already present in your cluster You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/deploy-control-and-compute-plane": "Deploy Control Plane Only Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Control Plane Only All Pages Start typing to search\u2026 Deploy Control Plane Only Truefoundry control-plane ships as a helm chart which can be installed on any Kubernetes cluster. It has the following dependencies: A Kubernetes Cluster on which it can be installed. A Postgres Database which can be created on Kubernetes using helm chart for testing purpose - but we recommend a managed database like RDS on production. One blob storage bucket like S3, GCS or ACS bucket. Deploying in Dev Mode To test out things quickly to see if Truefoundry can be useful for your use cases, we provide a quick dev installation which has minimal infrastructure requirements. In the dev installation, you can test most of the features except few as mentioned below. The key differences between dev and production mode are: Postgres is provisioned on Kubernetes instead of using a managed database - hence lower reliability. We do not recommend this for production usage. Code upload from CLI feature will not work. DNS mapping is not compulsory. You can test deploying from public Github repositories or your own docker image repository, hugging face model by just port-forwarding the truefoundry frontend service. To install Truefoundry in dev mode, you can directly install the control plane using helm as outlined here Installing Control Plane using Helm Chart Deploying in Production Mode To deploy in production mode, we will first create the appropriate infrastructure components before moving on to actual implementation. The guides for individual cloud providers wrt infrastructure related requirements and steps to create them are available here - Provisioning Control Plane Infrastructure on AWS Provisioning Control Plane Infrastructure on GCP Provisioning Control Plane Infrastructure on Azure Once the infra components are setup, we can go ahead and install the control plane using the helm chart - Installing Control Plane using Helm Chart Resources & Cost Resource Tier Small Medium Large Without LLM Gateway 1vCPUx4GB (~ $ 60) 6vCPUx20GB (~ $ 300) 12vCPUx30GB (~ $ 700) With LLM Gateway 2vCPUx8GB (~ $ 120) 16vCPUx50GB (~ $ 800) 24vCPUx65GB (~ $ 1400) Updated 6 days ago",
    "https://docs.truefoundry.com/docs/aws-control-plane": "AWS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account AWS All Pages Start typing to search\u2026 AWS Provisioning Control Plane Infrastructure on AWS \ud83d\udea7 There are steps in this guide where Truefoundry team will have to be involved. Please reach out to [email protected] to get the credentials Setting up Truefoundry control plane on your own cloud involves creating the infrastructure to support the platform and then installing the platform itself. Setting up Infrastructure Requirements These are the infrastructure components required to set up a production grade Truefoundry control plane. \ud83d\udcd8 If you have the below requirements already set up then skip directly to the Installation section Requirements Description Reason for Requirement Kubernetes Cluster Any Kubernetes cluster will work here - we can also choose the compute-plane cluster itself to install Truefoundry helm chart The Truefoundry helm chart will be installed here. Postgres RDS Postgres >= 13 The database is used by Truefoundry control plane to store all its metadata. S3 bucket Any S3 bucket reachable from control-plane. This is used by control-plane to store the intermediate code while building the docker image. Egress Access for TruefoundryAuth Egress access to https://auth.truefoundry.com This is needed to verify the users logging into the Truefoundry platform for licensing purposes Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, ArgoRollouts, ArgoWorkflows, Istio. DNS with TLS/SSL One endpoint to point to the control plane service (something like platform.example.com where example.com is your domain. There should also be a certificate with the domain so that the domains can be accessed over TLS. The control-plane url should be reachable from the compute-plane so that compute-plane cluster can connect to the control-plane The developers will need to access the Truefoundry UI at domain that is provided here. User/ServiceAccount to provision the infrastructure This is the set of permissions needed to provision the infrastructure for Truefoundry control-plane. Its detailed here Permissions Required We will be using OCLI (Onboarding CLI) to create the infrastructure. We will be using a locally setup AWS profile. Please make sure the user has the following permissions Shell export REGION=\"\" # us-east-1 export SHORT_REGION=\"\" #usea1 export ACCOUNT_ID=\"\" #123524493244 export NAME=\"\" JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateInstanceProfile\", \"iam:DeleteInstanceProfile\", \"rds:AddTagsToResource\", \"iam:GetInstanceProfile\", \"iam:RemoveRoleFromInstanceProfile\", \"rds:DeleteTenantDatabase\", \"iam:AddRoleToInstanceProfile\", \"rds:CreateDBInstance\", \"rds:DescribeDBInstances\", \"rds:RemoveTagsFromResource\", \"rds:CreateTenantDatabase\", \"iam:TagInstanceProfile\", \"rds:DeleteDBInstance\" ], \"Resource\": [ \"arn:aws:iam::$ACCOUNT_ID:instance-profile/*\", \"arn:aws:rds:$REGION:$ACCOUNT_ID:db:tfy-$SHORT_REGION-$NAME-*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"rds:AddTagsToResource\", \"rds:DeleteDBSubnetGroup\", \"rds:DescribeDBSubnetGroups\", \"iam:DeleteOpenIDConnectProvider\", \"iam:GetOpenIDConnectProvider\", \"rds:CreateDBSubnetGroup\", \"rds:ListTagsForResource\", \"rds:RemoveTagsFromResource\", \"iam:TagOpenIDConnectProvider\", \"iam:CreateOpenIDConnectProvider\", \"rds:CreateDBInstance\", \"rds:DeleteDBInstance\" ], \"Resource\": [ \"arn:aws:rds:$REGION:$ACCOUNT_ID:subgrp:tfy-$SHORT_REGION-$NAME-*\", \"arn:aws:iam::$ACCOUNT_ID:oidc-provider/*\" ] }, { \"Sid\": \"VisualEditor9\", \"Effect\": \"Allow\", \"Action\": [ \"rds:DescribeDBInstances\" ], \"Resource\": [ \"arn:aws:rds:$REGION:$ACCOUNT_ID:db:*\" ] }, { \"Sid\": \"VisualEditor2\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreatePolicy\", \"iam:GetPolicyVersion\", \"iam:GetPolicy\", \"iam:ListPolicyVersions\", \"iam:DeletePolicy\", \"iam:TagPolicy\" ], \"Resource\": [ \"arn:aws:iam::$ACCOUNT_ID:policy/tfy-*\", \"arn:aws:iam::$ACCOUNT_ID:policy/truefoundry-*\", \"arn:aws:iam::$ACCOUNT_ID:policy/AmazonEKS_Karpenter_Controller_Policy*\", \"arn:aws:iam::$ACCOUNT_ID:policy/AmazonEKS_CNI_Policy*\", \"arn:aws:iam::$ACCOUNT_ID:policy/AmazonEKS_AWS_Load_Balancer_Controller*\", \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess\" ] }, { \"Sid\": \"VisualEditor3\", \"Effect\": \"Allow\", \"Action\": [ \"iam:ListPolicies\", \"elasticfilesystem:*\", \"iam:GetRole\", \"s3:ListAllMyBuckets\", \"kms:*\", \"ec2:*\", \"s3:ListBucket\", \"route53:AssociateVPCWithHostedZone\", \"sts:GetCallerIdentity\", \"eks:*\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor4\", \"Effect\": \"Allow\", \"Action\": \"dynamodb:*\", \"Resource\": \"arn:aws:dynamodb:$REGION:$ACCOUNT_ID:table/$NAME-$REGION-tfy-ocli-table\" }, { \"Sid\": \"VisualEditor5\", \"Effect\": \"Allow\", \"Action\": \"iam:*\", \"Resource\": [ \"arn:aws:iam::$ACCOUNT_ID:role/tfy-*\", \"arn:aws:iam::$ACCOUNT_ID:role/initial-*\" ] }, { \"Sid\": \"VisualEditor6\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::tfy-$SHORT_REGION-$NAME-*/*\", \"arn:aws:s3:::$NAME-$REGION-tfy-ocli-bucket/*\", \"arn:aws:s3:::tfy-$SHORT_REGION-$NAME*\", \"arn:aws:s3:::$NAME-$REGION-tfy-ocli-bucket\", \"arn:aws:s3:::tfy-$SHORT_REGION-$NAME-truefoundry*\", \"arn:aws:s3:::tfy-$SHORT_REGION-$NAME-truefoundry*/*\" ] }, { \"Sid\": \"VisualEditor7\", \"Effect\": \"Allow\", \"Action\": \"events:*\", \"Resource\": \"arn:aws:events:$REGION:$ACCOUNT_ID:rule/tfy-$SHORT_REGION-$NAME*\" }, { \"Sid\": \"VisualEditor8\", \"Effect\": \"Allow\", \"Action\": \"sqs:*\", \"Resource\": \"arn:aws:sqs:$REGION:$ACCOUNT_ID:tfy-$SHORT_REGION-$NAME-karpenter\" } ] } Run Infra Provisioning using OCLI Prerequisites Install git if not already present. Install aws cli == 2.x.x and create an AWS profile locally with the specified access to the AWS account where you want to create the new cluster. Installing OCLI Download the binary using the below command. Apple silicon MacOS (arm64)) Intel MacOS (amd64) Linux (arm64) Linux (amd64) curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_amd64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_amd64\" -o ocli Make the binary executable and move it to $PATH Shell sudo chmod +x ./ocli sudo mv ocli /usr/local/bin Confirm by running the command Shell ocli --version Configuring Input Config file To create a new cluster, you would require your AWS Account ID , Region , and an AWS Profile Run the following command to fill in the inputs interactively Shell ocli infra-init For networking, there are two possible configurations: New VPC (Recommended) - This creates a new VPC for your new cluster. Existing VPC - You can enter your existing VPC and subnet IDs. Once all the inputs are filled, a config file with the name tfy-config.yaml would be generated in your current directory Modify the file to enable control plane installation by setting aws.tfy_control_plane.enabled: true . Below is the sample for the same: Existing network New network aws: account: id: \"xxxxxxxxxxxxxxxxx\" cluster: name: \"coolml\" public_access: cidrs: - 0.0.0.0/0 enabled: true version: \"1.30\" network: existing: true private_subnets_cidrs: [] private_subnets_ids: - subnet-xxxxxxxxxxxxxxxxx - subnet-xxxxxxxxxxxxxxxxx - subnet-xxxxxxxxxxxxxxxxx public_subnets_cidrs: [] public_subnets_ids: - subnet-xxxxxxxxxxxxxxxxx - subnet-xxxxxxxxxxxxxxxxx - subnet-xxxxxxxxxxxxxxxxx vpc_cidr: \"\" vpc_id: \"vpc-xxxxxxxxxxxxxxxxx\" platform_features: cluster_integration: enabled: true ecr: enabled: true enabled: true iam_role: assume_role_arns: - arn:aws:iam::416964291864:role/tfy-ctl-euwe1-production-truefoundry-deps role_enable_override: false role_override_name: \"\" iam_user: enabled: false user_enable_override: false user_override_name: \"\" parameter_store: enabled: true s3: bucket_enable_override: false bucket_override_name: \"\" enabled: true secrets_manager: enabled: false profile: name: administrator-xxxxxxxxxxxxxxxxx region: availability_zones: - us-east-1a - us-east-1b - us-east-1c - us-east-1d name: us-east-1 tags: project: coolml tfy_control_plane: enabled: true azure: null binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: aws aws: account: id: \"xxxxxxxxxxxxxxxxx\" cluster: name: \"coolml\" public_access: cidrs: - 0.0.0.0/0 enabled: true version: \"1.30\" network: existing: false private_subnets_cidrs: - 10.99.0.0/20 - 10.99.16.0/20 - 10.99.32.0/20 - 10.99.48.0/20 private_subnets_ids: [] public_subnets_cidrs: - 10.99.176.0/20 - 10.99.192.0/20 - 10.99.208.0/20 - 10.99.224.0/20 public_subnets_ids: [] vpc_cidr: 10.99.0.0/16 vpc_id: \"\" platform_features: cluster_integration: enabled: true ecr: enabled: true enabled: true iam_role: assume_role_arns: - arn:aws:iam::416964291864:role/tfy-ctl-euwe1-production-truefoundry-deps role_enable_override: false role_override_name: \"\" iam_user: enabled: false user_enable_override: false user_override_name: \"\" parameter_store: enabled: true s3: bucket_enable_override: false bucket_override_name: \"\" enabled: true secrets_manager: enabled: false profile: name: administrator-xxxxxxxxxxxxxxxxx region: availability_zones: - us-east-1a - us-east-1b - us-east-1c - us-east-1d name: us-east-1 tags: project: coolml tfy_control_plane: enabled: true azure: null binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: aws Create the cluster Run the following command to create the EKS cluster and IAM roles needed to provide access to various infrastructure components as per the inputs configured above. Shell ocli infra-create --file tfy-config.yaml This command may take around 30-45 minutes to complete. In the last step the database credentials will be printed. Make sure to note them down. Installating TrueFoundry Pre-requisites Installing helm Add the following chart repository Shell helm repo add argocd https://argoproj.github.io/argo-helm helm repo add truefoundry https://truefoundry.github.io/infra-charts/ Updating helm repo to download the latest local repository index Shell helm repo update argocd truefoundry Installing TrueFoundry helm chart Install argocd helm chart Shell helm upgrade --install argocd argocd/argo-cd -n argocd \\ --create-namespace \\ --version 7.4.4 \\ --set applicationSet.enabled=false \\ --set notifications.enabled=false \\ --set dex.enabled=false Create values.yaml for the truefoundry helm chart. You can refer to the values for more details. <terragruntOutput> from the below yaml file can be replaced with the terragrunt output of the ocli infra-create command. If you are not using ocli to create infrastructure you can chose the components to deploy and accordingly install the right components YAML ## @section Global Parameters ## @param tenantName Parameters for tenantName ## Tenant Name - This is same as the name of the organization used to sign up ## on Truefoundry ## tenantName: \"\" ## @param controlPlaneURL Parameters for controlPlaneURL ## URL of the control plane - This is the URL that can be used by workload to access the truefoundry components ## controlPlaneURL: \"\" ## @param clusterName Name of the cluster ## Name of the cluster that you have created on AWS/GCP/Azure ## clusterName: \"\" ## @param tolerations [array] Tolerations for the all chart components ## tolerations: - key: CriticalAddonsOnly value: \"true\" effect: NoSchedule operator: Equal - key: kubernetes.azure.com/scalesetpriority value: \"spot\" effect: NoSchedule operator: Equal - key: \"cloud.google.com/gke-spot\" value: \"true\" effect: NoSchedule operator: Equal ## @param affinity [object] Affinity for the all chart components ## affinity: {} ## @section AWS parameters ## AWS parameters ## aws: ## @subsection awsLoadBalancerController parameters ## @param aws.awsLoadBalancerController.enabled Flag to enable AWS Load Balancer Controller awsLoadBalancerController: enabled: true ## @param aws.awsLoadBalancerController.roleArn Role ARN for AWS Load Balancer Controller ## roleArn: \"\" ## @param aws.awsLoadBalancerController.vpcId VPC ID of AWS EKS cluster ## vpcId: \"\" ## @param aws.awsLoadBalancerController.region region of AWS EKS cluster ## region: \"\" ## @subsection karpenter parameters ## @param aws.karpenter.enabled Flag to enable Karpenter ## karpenter: enabled: true ## @param aws.karpenter.clusterEndpoint Cluster endpoint for Karpenter ## clusterEndpoint: \"\" ## @param aws.karpenter.roleArn Role ARN for Karpenter ## roleArn: \"\" ## @param aws.karpenter.instanceProfile Instance profile for Karpenter ## instanceProfile: \"\" ## @param aws.karpenter.defaultZones Default zones list for Karpenter ## defaultZones: [] ## @param aws.karpenter.interruptionQueue Interruption queue name for Karpenter ## interruptionQueue: \"\" ## @subsection awsEbsCsiDriver parameters ## @param aws.awsEbsCsiDriver.enabled Flag to enable AWS EBS CSI Driver ## awsEbsCsiDriver: enabled: true ## @param aws.awsEbsCsiDriver.roleArn Role ARN for AWS EBS CSI Driver ## roleArn: \"\" ## @subsection awsEfsCsiDriver parameters ## @param aws.awsEfsCsiDriver.enabled Flag to enable AWS EFS CSI Driver ## awsEfsCsiDriver: enabled: true ## @param aws.awsEfsCsiDriver.fileSystemId File system ID for AWS EFS CSI Driver ## fileSystemId: \"\" ## @param aws.awsEfsCsiDriver.region Region for AWS EFS CSI Driver ## region: \"\" ## @param aws.awsEfsCsiDriver.roleArn Role ARN for AWS EFS CSI Driver ## roleArn: \"\" ## @param aws.inferentia.enabled Flag to enable Inferentia inferentia: enabled: false ## @section truefoundry parameters ## @param truefoundry.enabled Flag to enable TrueFoundry ## This installs the Truefoundry control plane helm chart. You can make it true ## if you want to install Truefoundry control plane. ## truefoundry: enabled: true ## @param truefoundry.devMode.enabled Flag to enable TrueFoundry Dev mode ## devMode: enabled: false ## @section truefoundryBootstrap parameters ## @param truefoundry.truefoundryBootstrap.enabled Flag to enable bootstrap job to prep cluster for truefoundry installation truefoundryBootstrap: enabled: false ## @section database. Can be left empty if using the dev mode parameters database: ## @param truefoundry.database.host Hostname of the database host: \"\" ## @param truefoundry.database.name Name of the database name: \"\" ## @param truefoundry.database.username Username of the database username: \"\" ## @param truefoundry.database.password Password of the database password: \"\" ## @param truefoundry.tfyApiKey API Key for TrueFoundry tfyApiKey: \"\" ## @param truefoundry.truefoundryImagePullConfigJSON Json config for authenticating to the TrueFoundry registry truefoundryImagePullConfigJSON: \"\" ## @section istio parameters ## @param istio.enabled Flag to enable Istio ## istio: enabled: true ## @skip istio.gateway.annotations Annotations for Istio Gateway gateway: annotations: \"service.beta.kubernetes.io/aws-load-balancer-name\": \"<terragruntOutput.cluster.cluster_name.raw>\" \"service.beta.kubernetes.io/aws-load-balancer-type\": \"external\" \"service.beta.kubernetes.io/aws-load-balancer-scheme\": \"internet-facing\" \"service.beta.kubernetes.io/aws-load-balancer-ssl-ports\": \"https\" \"service.beta.kubernetes.io/aws-load-balancer-alpn-policy\": \"HTTP2Preferred\" \"service.beta.kubernetes.io/aws-load-balancer-backend-protocol\": \"tcp\" \"service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags\": cluster-name=<terragruntOutput.cluster.cluster_name.raw>, truefoundry.com/managed=true, owner=Truefoundry, application=tfy-istio-ingress \"service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled\": \"true\" ## @section istio discovery parameters discovery: ## @param istio.discovery.hub Hub for the istio image hub: gcr.io/istio-release ## @param istio.discovery.tag Tag for the istio image tag: 1.21.1-distroless ## @param istio.tfyGateway.httpsRedirect Flag to enable HTTPS redirect for Istio Gateway tfyGateway: httpsRedirect: true ## @section tfyAgent parameters ## @param tfyAgent.enabled Flag to enable Tfy Agent ## tfyAgent: enabled: false ## @param tfyAgent.clusterToken cluster token ## Token for cluster authentication ## clusterToken: \"\" Fill the following values tenantName - name of the tenant. If you haven't created one, please do it here controlPlaneURL - URL at which to host the platform (for e.g. https://truefoundry.example.com ) clusterName - name of the cluster in AWS console For the remaining values truefoundry.tfyApiKey - api key to given by TrueFoundry team truefoundry.truefoundryImagePullConfigJSON - Image pull config JSON to be given by TrueFoundry team truefoundry.truefoundryFrontendApp.istio.hosts[0] - control plane URL without protocol Run the following command to install the chart Shell helm upgrade --install tfy-k8s-aws-eks-inframold \\ truefoundry/tfy-k8s-aws-eks-inframold \\ -f values.yaml -n argocd Once the helm chart is installed, point the control plane URL to the load balancer's IP address. To get the IP address of the load balancer Shell kubectl get svc tfy-istio-ingress -n istio-system We will also need the TLS certificates to be passed to the load balancer (in our case istio) to terminate the TLS traffic. Add the compute plane Add the same cluster as the compute-plane from the UI and get the cluster token Add the token in the values.yaml YAML ## @section tfyAgent parameters tfyAgent: enabled: true clusterToken: \"\" The control plane URL should be reachable to from inside of the k8s cluster as the tfy-agent will use the control plane URL to initiate the connection to the control plane. Run the helm command to install the agent Shell helm upgrade --install tfy-k8s-aws-eks-inframold \\ truefoundry/tfy-k8s-aws-eks-inframold \\ -f values.yaml -n argocd Adding domain to Load balancer We need to add one more domain to the load balancer so that a separate domain can be used to host the workloads only. This domain can be a wildcard (recommended) as well. To add the domain Point the domain to the load balancer IP address Pass the TLS certificate to istio so that it can terminate the TLS traffic. Add the domain in the platform. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/gcp-control-plane": "GCP Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account GCP All Pages Start typing to search\u2026 GCP Provisioning Control Plane Infrastructure on GCP \ud83d\udea7 There are steps in this guide where Truefoundry team will have to be involved. Please reach out to [email protected] to get the credentials Setting up Truefoundry control plane on your own cloud involves creating the infrastructure to support the platform and then installing the platform itself. Setting up Infrastructure Requirements These are the infrastructure components required to set up a production grade Truefoundry control plane. \ud83d\udcd8 If you have the below requirements already set up then skip directly to the Installation section Requirements Description Reason for Requirement Kubernetes Cluster Any Kubernetes cluster will work here - we can also choose the compute-plane cluster itself to install Truefoundry helm chart. The Truefoundry helm chart will be installed here. CloudSQL Postgres Postgres >= 13 The database is used by Truefoundry control plane to store all its metadata. GCS bucket Any GCS bucket reachable from control-plane. This is used by control-plane to store the intermediate code while building the docker image. Egress Access for TruefoundryAuth Egress access to https://auth.truefoundry.com This is needed to validate the users logging into Truefoundry so that licensing can be maintained. Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, ArgoRollouts, ArgoWorkflows, Istio. DNS with TLS/SSL One endpoint to point to the control plane service (something like platform.example.com where example.com is your domain. There should also be a certificate with the domain so that the domains can be accessed over TLS. The control-plane url should be reachable from the compute-plane so that compute-plane cluster can connect to the control-plane The developers will need to access the Truefoundry UI at domain that is provided here. User/ServiceAccount to provision the infrastructure - Cloud SQL Admin Security Admin Service Account Admin Service Account Token Creator Service Account User Storage Admin These are the permissions required by the IAM user in GCP to create the entire control plane components. Permissions Required We will be using OCLI (Onboarding CLI) to create the infrastructure. We will be using a locally setup gcloud CLI. Please make sure the user or service account authenticated with GCP has the following permissions - Cloud SQL Admin - roles/cloudsql.admin Security Admin - roles/iam.securityAdmin Service Account Admin - roles/iam.serviceAccountAdmin Service Account Token Creator - roles/iam.serviceAccountTokenCreator Service Account User - roles/iam.serviceAccountUser Storage Admin - roles/storage.admin GCP Infra Architecture Run Infra Provisioning using OCLI Prerequisites Install git if not already present. Setup gcloud CLI Install gcloud cli == 474.x.x Install gke-gcloud-auth-plugin Authenticate with a user that has the above mentioned roles - gcloud auth application-default login Set the project to be used for infra creation - gcloud config set project truefoundry-devtest Installing OCLI Download the binary using the below command. Apple silicon MacOS (arm64)) Intel MacOS (amd64) Linux (arm64) Linux (amd64) curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_amd64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_amd64\" -o ocli Make the binary executable and move it to $PATH Shell sudo chmod +x ./ocli sudo mv ocli /usr/local/bin Confirm by running the command Shell ocli --version Configuring Input Config file To create a new cluster, you would require your GCP Project ID and Region Run the following command to fill in the inputs interactively Shell ocli infra-init For networking, there are two possible configurations: New VPC (Recommended) - This creates a new VPC for your new cluster. Existing VPC - You can enter your existing VPC and subnet IDs. Once all the inputs are filled, a config file with the name tfy-config.yaml would be generated in your current directory Modify the file to enable control plane installation by setting gcp.tfy_control_plane.enabled: true . Below is the sample for the same: Existing network New network aws: null azure: null binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: cluster: master_cidr_block: 172.16.0.32/28 name: <cluster_name> pod_range_name: pods service_range_name: services version: \"1.28\" network: additional_ranges: [] existing: false network_name: \"\" pod_cidr: 10.244.0.0/16 service_cidr: 10.255.0.0/16 shared_vpc: enabled: false network_name: \"\" project_id: \"\" subnet_name: \"\" subnet_cidr: 10.10.0.0/16 subnet_id: \"\" network_tags: [] project: id: <project-name> region: availability_zones: - asia-south1-b - asia-south1-a - asia-south1-c name: asia-south1 tags: {} tfy_control_plane: enabled: true provider: gcp aws: account: id: \"xxxxxxxxxxxxxxxxx\" cluster: name: coolml public_access: cidrs: - 0.0.0.0/0 enabled: true version: \"1.28\" iam_role: assume_role_arns: - arn:aws:iam::416964291864:role/tfy-ctl-euwe1-production-truefoundry-deps ecr: enabled: true enabled: true role_enable_override: false role_override_name: \"\" s3: bucket_enable_override: false bucket_override_name: \"\" enabled: true ssm: enabled: true network: existing: false private_subnets_cidrs: - 10.222.0.0/20 - 10.222.16.0/20 - 10.222.32.0/20 private_subnets_ids: [] public_subnets_cidrs: - 10.222.176.0/20 - 10.222.192.0/20 - 10.222.208.0/20 public_subnets_ids: [] vpc_cidr: 10.222.0.0/16 vpc_id: \"\" profile: name: administrator-xxxxxxxxxxxxxxxxx region: availability_zones: - us-east-2a - us-east-2b - us-east-2c name: us-east-2 tags: {} tfy_control_plane: enabled: true azure: null binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: aws Create the cluster Run the following command to create the GKE cluster and IAM roles needed to provide access to various infrastructure components as per the inputs configured above. Shell ocli infra-create --file tfy-config.yaml This command may take around 30-45 minutes to complete. In the last step the database credentials will be printed. Make sure to note them down. Installation Installation steps Make sure that kubectl context is set to the newly created cluster. Install argoCD - kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/core-install.yaml Add the truefoundry helm repo helm repo add truefoundry https://truefoundry.github.io/infra-charts/ helm repo update We will create a values.yaml for the helm chart installation - Download the values.yaml from helm chart repo - curl https://raw.githubusercontent.com/truefoundry/infra-charts/main/charts/tfy-k8s-gcp-gke-standard-inframold/values-cp.yaml > values.yaml Fill in the tenant_name , cluster_name , truefoundry_image_pull_config_json and tfy_api_key in the downloaded file. You can get these from the Truefoundry team Also fill in the database section with database creds. If you created the infrastructure using OCLI, you can get the credentials by running ocli output Apply the helm chart with the values.yaml helm install -n argocd inframold truefoundry/tfy-k8s-gcp-gke-standard-inframold --version 0.0.16 -f values.yaml Test the installation Port forward the frontend application to access the Truefoundry dashboard - kubectl port-forward svc/truefoundry-truefoundry-frontend-app -n truefoundry 5000 Access the truefoundry dashboard from a browser by opening http://localhost:5000 . You can login with the username and password provided by the Truefoundry team. Now you are ready to connect a cluster to the Truefoundry platform and get deploying. Go here for the directions. You can also onboard the same cluster as the control plane Updated 5 months ago",
    "https://docs.truefoundry.com/docs/azure-control-plane": "Azure Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Azure All Pages Start typing to search\u2026 Azure Provisioning Control Plane Infrastructure on Azure \ud83d\udea7 There are steps in this guide where Truefoundry team will have to be involved. Please reach out to [email protected] to get the credentials Setting up Truefoundry control plane on your own cloud involves creating the infrastructure to support the platform and then installing the platform itself. Setting up Infrastructure Requirements These are the infrastructure components required to set up a production grade Truefoundry control plane. \ud83d\udcd8 If you have the below requirements already set up then skip directly to the Installation section Requirements Description Reason for Requirement Kubernetes Cluster Any Kubernetes cluster will work here - we can also choose the compute-plane cluster itself to install Truefoundry helm chart. The Truefoundry helm chart will be installed here. Azure Flexible Server for PostgreSQL Postgres >= 13 The database is used by Truefoundry control plane to store all its metadata. Container in Azure Storage Account Any container bucket reachable from control-plane. This is used by control-plane to store the intermediate code while building the docker image. AzureAD application AzureAD application for a service principal having read only access to the AKS cluster This is used to read the node pools created in the AKS cluster for workloads to get deployed on them. Egress Access for TruefoundryAuth Egress access to https://auth.truefoundry.com This is needed to validate the users logging into Truefoundry so that licensing can be maintained. Egress access For Docker Registry 1 public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, ArgoRollouts, ArgoWorkflows, Istio. DNS with TLS/SSL One endpoint to point to the control plane service (something like platform.example.com where example.com is your domain. There should also be a certificate with the domain so that the domains can be accessed over TLS. The control-plane url should be reachable from the compute-plane so that compute-plane cluster can connect to the control-plane Ensure that require_secure_transport is kept OFF The developers will need to access the Truefoundry UI at domain that is provided here. User/ServiceAccount to provision the infrastructure - azure subscription with billing enabled Contributor Role to the above Subscription. Role Based Access Administrator to the above subscription These are the permissions required by the IAM user in Azure to create the entire control plane components. Run Infra Provisioning using OCLI Prerequisites Install git if not already present. Setup az CLI Install azure cli >= 2.50 Log in and set a subscription. Please ensure that the user has Contributor and RBAC admin roles in the Subscription # login az login # setting the subscription az account set --subscription $SUBSCRIPTION_ID Installing OCLI Download the binary using the below command. Apple silicon MacOS (arm64)) Intel MacOS (amd64) Linux (arm64) Linux (amd64) curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_darwin_amd64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_arm64\" -o ocli curl -H 'Cache-Control: max-age=0' -s \"https://releases.ocli.truefoundry.tech/binaries/ocli_$(curl -H 'Cache-Control: max-age=0' -s https://releases.ocli.truefoundry.tech/stable.txt)_linux_amd64\" -o ocli Make the binary executable and move it to $PATH Shell sudo chmod +x ./ocli sudo mv ocli /usr/local/bin Confirm by running the command Shell ocli --version Configuring input config file To create a new cluster, you would require your Azure Subscription , Location , Resource Group . Run the following command to fill in the inputs interactively Shell ocli infra-init For networking, there are the following possible configurations: New resource group & network (Recommended) - This will create a new resource group and a new Virtual network. Existing resource group with existing network - You can use an existing resource group and an existing Virtual network. Existing resource group with new network - You can use an existing resource group while creating a new Virtual network Once all the inputs are filled, an input config file with the name tfy-config.yaml would be generated in your current directory Modify the file to enable control plane installation by setting azure.tfy_control_plane.enabled: true . Also modify the azure.tfy_control_plane.subnet_cidr: \"\" or azure.tfy_control_plane.subnet_id: \"\" for installing control plane components. Below is the sample for the same: Existing resource-group/ Exising network Existing resource-group/ New network New resource-group/ New network aws: null azure: cluster: name: CLUSTER_NAME node_pools: cpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D2ds_v5 max_count: 2 name: cpu - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D4ds_v5 max_count: 2 name: cpu2x gpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NV6ads_A10_v5 max_count: 2 name: a10 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC4as_T4_v3 max_count: 2 name: t4 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC24ads_A100_v4 max_count: 2 name: a100 initial: instance_type: Standard_D2ds_v5 name: initial version: \"1.29\" location: eastus network: existing: true subnet_cidr: \"\" subnet_id: \"/subscriptions/xxxxx-xxxxx-xxxxx-xxxxxxxxx/resourceGroups/RESOURCE_GROUP/providers/Microsoft.Network/virtualNetworks/VNET/subnets/SUBNET\" vnet_cidr: \"\" vnet_id: \"/subscriptions/xxxxx-xxxxx-xxxxx-xxxxxxxxx/resourceGroups/RESOURCE_GROUP/providers/Microsoft.Network/virtualNetworks/VNET\" vnet_name: \"\" platform_features: blob_storage: container_enable_override: false container_override_name: \"\" enabled: true storage_account_enable_override: false storage_account_override_name: \"\" cloud_integration: azuread_application_enable_override: false azuread_application_override_name: \"\" enabled: true container_registry: container_registry_enable_override: false container_registry_override_name: \"\" enabled: true enabled: true resource_group: existing: true name: RESOURCE_GROUP state: container_name: tfy-tfstate-CLUSTER_NAME-cn-1714629250 resource_group: tfy-tfstate-CLUSTER_NAME-rg-1714629250 storage_account_name: tfytfstateCLUSTER_NAMEsa storage_account_sku: Standard_GRS subscription: id: SUBSCRIPTION_ID name: SUBSCRIPTION_NAME tags: {} tfy_control_plane: database: existing_network: true instance_class: GP_Standard_D4ds_v5 subnet_cidr: \"\" subnet_id: \"/subscriptions/xxxxx-xxxxx-xxxxx-xxxxxxxxx/resourceGroups/RESOURCE_GROUP/providers/Microsoft.Network/virtualNetworks/VNET/subnets/DB_SUBNET\" enabled: true binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: azure aws: null azure: cluster: name: CLUSTER_NAME node_pools: cpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D2ds_v5 max_count: 2 name: cpu - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D4ds_v5 max_count: 2 name: cpu2x gpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NV6ads_A10_v5 max_count: 2 name: a10 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC4as_T4_v3 max_count: 2 name: t4 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC24ads_A100_v4 max_count: 2 name: a100 initial: instance_type: Standard_D2ds_v5 name: initial version: \"1.29\" location: eastus network: existing: false subnet_cidr: 10.10.0.0/16 subnet_id: \"\" vnet_cidr: 10.0.0.0/8 vnet_id: \"\" vnet_name: \"\" platform_features: blob_storage: container_enable_override: false container_override_name: \"\" enabled: true storage_account_enable_override: false storage_account_override_name: \"\" cloud_integration: azuread_application_enable_override: false azuread_application_override_name: \"\" enabled: true container_registry: container_registry_enable_override: false container_registry_override_name: \"\" enabled: true enabled: true resource_group: existing: true name: RESOURCE_GROUP state: container_name: tfy-tfstate-CLUSTER_NAME-cn-1714629250 resource_group: tfy-tfstate-CLUSTER_NAME-rg-1714629250 storage_account_name: tfytfstateCLUSTER_NAMEsa subscription: id: SUBSCRIPTION_ID name: SUBSCRIPTION_NAME tags: {} tfy_control_plane: database: existing_network: true instance_class: GP_Standard_D4ds_v5 subnet_cidr: \"10.11.0.0/24\" subnet_id: \"/subscriptions/xxxxx-xxxxx-xxxxx-xxxxxxxxx/resourceGroups/RESOURCE_GROUP/providers/Microsoft.Network/virtualNetworks/VNET/subnets/DB_SUBNET\" enabled: true binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: azure aws: null azure: cluster: name: CLUSTER_NAME node_pools: cpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D2ds_v5 max_count: 2 name: cpu - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_D4ds_v5 max_count: 2 name: cpu2x gpu_pools: - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NV6ads_A10_v5 max_count: 2 name: a10 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC4as_T4_v3 max_count: 2 name: t4 - enable_on_demand_pool: true enable_spot_pool: true instance_type: Standard_NC24ads_A100_v4 max_count: 2 name: a100 initial: instance_type: Standard_D2ds_v5 name: initial version: \"1.29\" location: eastus network: existing: false subnet_cidr: 10.10.0.0/16 subnet_id: \"\" vnet_cidr: 10.0.0.0/8 vnet_id: \"\" vnet_name: \"\" platform_features: blob_storage: container_enable_override: false container_override_name: \"\" enabled: true storage_account_enable_override: false storage_account_override_name: \"\" cloud_integration: azuread_application_enable_override: false azuread_application_override_name: \"\" enabled: true container_registry: container_registry_enable_override: false container_registry_override_name: \"\" enabled: true enabled: true resource_group: existing: false name: RESOURCE_GROUP state: container_name: tfy-tfstate-CLUSTER_NAME-cn-1714629250 resource_group: tfy-tfstate-CLUSTER_NAME-rg-1714629250 storage_account_name: tfytfstateCLUSTER_NAMEsa subscription: id: SUBSCRIPTION_ID name: SUBSCRIPTION_NAME tags: {} tfy_control_plane: enabled: false binaries: terraform: binary_path: null terragrunt: binary_path: null gcp: null provider: azure Create the cluster Run the following command to create the GKE cluster and IAM roles needed to provide access to various infrastructure components as per the inputs configured above. Shell ocli infra-create --file tfy-config.yaml This command may take around 30-45 minutes to complete. In the last step the database credentials will be printed. Make sure to note them down. Installing TrueFoundry Pre-requisites Installing helm Add the following chart repository Shell helm repo add argocd https://argoproj.github.io/argo-helm helm repo add truefoundry https://truefoundry.github.io/infra-charts/ Updating helm repo to download the latest local repository index Shell helm repo update argocd truefoundry Installing truefoundry helm chart Installing argocd helm chart Shell helm upgrade --install argocd argocd/argo-cd -n argocd \\ --create-namespace \\ --version 7.4.4 \\ --set applicationSet.enabled=false \\ --set notifications.enabled=false \\ --set dex.enabled=false Create values.yaml for the truefoundry helm chart. You can refer to the values for more details YAML ## @param tenantName Parameters for tenantName ## Tenant Name - This is same as the name of the organization used to sign up ## on Truefoundry ## tenantName: \"\" ## @param controlPlaneURL Parameters for controlPlaneURL ## URL of the control plane - This is the URL that can be used by workload to access the truefoundry components ## controlPlaneURL: \"\" ## @param clusterName Name of the cluster ## Name of the cluster that you have created on AWS/GCP/Azure ## clusterName: \"\" ## @section notebookController parameters ## Notebook Controller is required to power notebooks in Truefoundry ## notebookController: enabled: false defaultStorageClass: \"\" ## @section tfyAgent parameters tfyAgent: enabled: false ## @param truefoundry.enabled Flag to enable TrueFoundry ## This installs the Truefoundry control plane helm chart. You can make it true ## if you want to install Truefoundry control plane. ## truefoundry: enabled: true ## @param truefoundry.devMode.enabled Flag to enable TrueFoundry Dev mode. Postgres will run ## devMode: enabled: false truefoundryBootstrap: enabled: true truefoundryFrontendApp: replicaCount: 2 istio: virtualservice: enabled: true gateways: [\"istio-system/tfy-wildcard\"] hosts: - \"\" tfyWorkflowAdmin: enabled: false database: host: \"\" name: \"\" username: \"\" password: \"\" ## @param global.tfyApiKey API key for truefoundry ## tfyApiKey: \"\" ## @param global.truefoundryImagePullConfigJSON JSON config for image pull secret ## truefoundryImagePullConfigJSON: \"\" Fill the following values tenantName - name of the tenant. If you haven't created one. please do it here controlPlaneURL - URL at which to host the platform (for e.g. https://truefoundry.example.com ) clusterName - name of the cluster For the remaining values truefoundry.tfyApiKey - api key to given by TrueFoundry team truefoundry.truefoundryImagePullConfigJSON - Image pull config JSON to be given by TrueFoundry team truefoundry.truefoundryFrontendApp.istio.hosts[0] - control plane URL without protocol Run the following command to install the chart Shell helm upgrade --install tfy-k8s-azure-aks-inframold \\ truefoundry/tfy-k8s-azure-aks-inframold \\ -f values.yaml -n argocd Once the helm chart is installed, point the control plane URL to the load balancer's IP address. To get the IP address of the load balancer Shell kubectl get svc tfy-istio-ingress -n istio-system We will also need the TLS certificates to be passed to the load balancer (in our case istio) to terminate the TLS traffic. Login in the control plane URL with the same credentials used to register the tenant. Add the compute plane Add the same cluster as the compute-plane from the UI and get the cluster token Add the token in the values.yaml YAML ## @section tfyAgent parameters tfyAgent: enabled: true clusterToken: \"\" The control plane URL should be reachable to from inside of the k8s cluster as the tfy-agent will use the control plane URL to initiate the connection to the control plane. Helm Shell helm upgrade --install tfy-k8s-azure-aks-inframold \\ truefoundry/tfy-k8s-azure-aks-inframold \\ -f values.yaml -n argocd Adding domain to Load balancer We need to add one more domain to the load balancer so that a separate domain can be used to host the workloads only. This domain can be a wildcard (recommended) as well. To add the domain Point the domain to the load balancer IP address. Pass the TLS certificate to istio so that it can terminate the TLS traffic. Add the domain in the platform. Adding integrations If you have used ocli to bootstrap your infrastructure then it creates the following additional resources alongwith AKS cluster in your selected resource group. Check the below documents to understand how to create the integrations manually, if not done through OCLI and how to add them to the platform. Container registry - How to add container registry to the platform Storage account - how to add storage account to the platform Container Service Principal having read only access to AKS cluster - how to add azure application to TF platform Updated 5 months ago",
    "https://docs.truefoundry.com/docs/installing-control-plane-using-helm-chart": "Installation with Helm Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Installation with Helm All Pages Start typing to search\u2026 Installation with Helm Installing Control Plane using Helm Chart Installation steps Create truefoundry namespace kubectl create ns truefoundry Create an imagePullSecret for the Truefoundry images in the truefoundry namespace cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: truefoundry-image-pull-secret namespace: truefoundry data: .dockerconfigjson: <image pull secret provided by truefoundry team> type: kubernetes.io/dockerconfigjson EOF The helm charts to use for the three cloud providers are as follows - AWS - tfy-k8s-aws-eks-inframold GCP - Standard - tfy-k8s-gcp-gke-standard-inframold Autopilot - tfy-k8s-gcp-gke-autopilot-inframold Azure - tfy-k8s-azure-aks-inframold Add the truefoundry helm repo helm repo add truefoundry https://truefoundry.github.io/infra-charts/ Create a values.yaml and fill in the values ## @section Global Parameters ## @param tenantName Parameters for tenantName ## Tenant Name - This is same as the name of the organization used to sign up ## on Truefoundry ## tenantName: <tenant_name> ## @param controlPlaneURL Parameters for controlPlaneURL ## URL of the control plane - Same as the URL of the Truefoundry dashboard ## controlPlaneURL: <control_plane_url> ## @param clusterName Name of the cluster ## Name of the cluster that you have created on AWS/GCP/Azure ## clusterName: <cluster_name> ## @section Parameters for argocd ## @param argocd.enabled Flag to enable ArgoCD ## ArgoCD is mandatory for Truefoundry to work. You can make it false if ArgoCD is ## already installed in your cluster. Please make sure that the configuration of ## existing ArgoCD is same as the ArgoCD configuration required by Truefoundry. argocd: enabled: true ## @section Parameters for argoWorkflows ## @param argoWorkflows.enabled Flag to enable Argo Workflows ## argoWorkflows: enabled: true ## @section Parameters for truefoundry ## @param truefoundry.enabled Flag to enable TrueFoundry ## This installs the Truefoundry control plane helm chart. You can make it true ## if you want to install Truefoundry control plane. ## truefoundry: enabled: true truefoundryBootstrap: enabled: true database: host: <host> name: <name> username: <username> password: <password> sfyApiKey: <sfy_api_key> dev: false ## @section Parameters for loki ## @param loki.enabled Flag to enable Loki. This is needed to show build logs ## loki: enabled: true ## @section Parameters for tfyAgent ## @param tfyAgent.enabled Flag to enable Tfy Agent ## tfyAgent: enabled: false Apply the helm chart with the values.yaml. Replace the chart_name with the correct one from step 3 helm install inframold truefoundry/<chart_name> --version 0.0.8 -f values.yaml Updated 5 months ago",
    "https://docs.truefoundry.com/docs/dev-mode-installation": "Dev Mode installation Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Dev Mode installation All Pages Start typing to search\u2026 Dev Mode installation Dev Mode Infrastructure Requirements Requirements Description Reason for Requirement Kubernetes Cluster Any Kubernetes cluster will work here - we can also choose the compute-plane cluster itself to install Truefoundry helm chart The Truefoundry helm chart will be installed here Egress Access for TruefoundryAuth Egress access to https://auth.truefoundry.com This is needed to verify the users logging into the Truefoundry platform for licensing purposes Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, ArgoRollouts, ArgoWorkflows, Istio Installation steps Install argoCD - kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/core-install.yaml Add the truefoundry helm repo helm repo add truefoundry https://truefoundry.github.io/infra-charts/ helm repo update We will create a values.yaml for the helm chart installation - Download the values.yaml from helm chart repo - curl https://raw.githubusercontent.com/truefoundry/infra-charts/main/charts/tfy-k8s-generic-inframold/values-cp-dev.yaml > values.yaml Fill in the tenant_name , cluster_name , truefoundry_image_pull_config_json , tfy_api_key , truefoundry_internal_jwt_jwks and INITIAL_ADMIN_OAUTH_USER_PASSWORD in the downloaded file. You can get these from the Truefoundry team Apply the helm chart with the values.yaml helm install -n argocd inframold truefoundry/tfy-k8s-generic-inframold --version 0.0.18 -f values.yaml Test the installation Port forward the frontend application to access the Truefoundry dashboard - kubectl port-forward svc/truefoundry-truefoundry-frontend-app -n truefoundry 5000 Access the truefoundry dashboard from a browser by opening http://localhost:5000 . You can login with the username and password provided by the Truefoundry team. Now you are ready to connect a cluster to the Truefoundry platform and get deploying. Go here for the directions. You can also onboard the same cluster as the control plane Updated 4 months ago",
    "https://docs.truefoundry.com/docs/generic-control-plane": "Generic Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Generic All Pages Start typing to search\u2026 Generic Provisioning control plane in Generic cluster \ud83d\udea7 There are steps in this guide where Truefoundry team will have to be involved. Please reach out to [email protected] to get the credentials Setting up Truefoundry control plane on a generic cluster involves creating the infrastructure to support the platform and then installing the platform itself. Generic cluster refers to those cluster which are not managed by any cloud provider. Setting up Infrastructure Infrastructure requirements Requirements Description Reason for Requirement Kubernetes Cluster Any Kubernetes cluster will work here - we can also choose the compute-plane cluster itself to install Truefoundry helm chart. Min 4vCPU and 8GB RAM The Truefoundry helm chart will be installed here. Postgres database Postgres >= 13 The database is used by Truefoundry control plane to store all its metadata and can be installed through the truefoundry helm chart. Volume Default storage class to support dynamic volumes Volumes are required for databases and statefulsets Egress Access for TruefoundryAuth Egress access to https://auth.truefoundry.com This is needed to verify the users logging into the Truefoundry platform for licensing purposes Egress access For Docker Registry 1. public.ecr.aws 2. quay.io 3. ghcr.io 4. docker.io/truefoundrycloud 5. docker.io/natsio 6. nvcr.io 7. registry.k8s.io This is to download docker images for Truefoundry, ArgoCD, NATS, ArgoRollouts, ArgoWorkflows, Istio. DNS with TLS/SSL One endpoint to point to the control plane service (something like platform.example.com where example.com is your domain. There should also be a certificate with the domain so that the domains can be accessed over TLS. The control-plane url should be reachable from the compute-plane so that compute-plane cluster can connect to the control-plane The developers will need to access the Truefoundry UI at domain that is provided here. Installing TrueFoundry Pre-requisites Installing helm Add the following chart repository Shell helm repo add argocd https://argoproj.github.io/argo-helm helm repo add truefoundry https://truefoundry.github.io/infra-charts/ Updating helm repo to download the latest local repository index Shell helm repo update argocd truefoundry Installing truefoundry helm chart Installing argocd helm chart Shell helm upgrade --install argocd argocd/argo-cd -n argocd \\ --create-namespace \\ --version 7.4.4 \\ --set applicationSet.enabled=false \\ --set notifications.enabled=false \\ --set dex.enabled=false Create values.yaml for the truefoundry helm chart. YAML ## @param tenantName Parameters for tenantName ## Tenant Name - This is same as the name of the organization used to sign up ## on Truefoundry ## tenantName: \"\" ## @param controlPlaneURL Parameters for controlPlaneURL ## URL of the control plane - This is the URL that can be used by workload to access the truefoundry components ## controlPlaneURL: \"\" ## @param clusterName Name of the cluster ## Name of the cluster that you have created on AWS/GCP/Azure ## clusterName: \"\" ## @section notebookController parameters ## Notebook Controller is required to power notebooks in Truefoundry ## notebookController: enabled: false defaultStorageClass: \"\" ## @section tfyAgent parameters tfyAgent: enabled: false ## @param truefoundry.enabled Flag to enable TrueFoundry ## This installs the Truefoundry control plane helm chart. You can make it true ## if you want to install Truefoundry control plane. ## truefoundry: enabled: true ## @param truefoundry.devMode.enabled Flag to enable TrueFoundry Dev mode. Postgres will run ## devMode: enabled: true truefoundryBootstrap: enabled: true tfyK8sController: enabled: false truefoundryFrontendApp: replicaCount: 2 istio: virtualservice: enabled: true gateways: [\"istio-system/tfy-wildcard\"] hosts: - \"\" tfyWorkflowAdmin: enabled: false database: host: \"\" name: \"\" username: \"\" password: \"\" ## @param global.tfyApiKey API key for truefoundry ## tfyApiKey: \"\" ## @param global.truefoundryImagePullConfigJSON JSON config for image pull secret ## truefoundryImagePullConfigJSON: \"\" Fill the following values tenantName - name of the tenant. If you haven't created one. please do it here controlPlaneURL - URL at which to host the platform (for e.g. https://truefoundry.example.com ) clusterName - name of the cluster For the remaining values truefoundry.tfyApiKey - api key to given by TrueFoundry team truefoundry.truefoundryImagePullConfigJSON - Image pull config JSON to be given by TrueFoundry team truefoundry.truefoundryFrontendApp.istio.hosts[0] - control plane URL without protocol Run the following command to install the chart Shell helm upgrade --install tfy-k8s-generic-inframold \\ truefoundry/tfy-k8s-generic-inframold \\ -f values.yaml -n argocd Once the helm chart is installed, point the control plane URL to the load balancer's IP address. To get the IP address of the load balancer Shell kubectl get svc tfy-istio-ingress -n istio-system We will also need the TLS certificates to be passed to the load balancer (in our case istio) to terminate the TLS traffic. Login in the control plane URL with the same credentials used to register the tenant. Add the compute plane Add the same cluster as the compute-plane from the UI and get the cluster token Add the token in the values.yaml and run the helm install command YAML ## @section tfyAgent parameters tfyAgent: enabled: false clusterToken: \"\" The control plane URL should be reachable to from inside of the k8s cluster as the tfy-agent will use the control plane URL to initiate the connection to the control plane. Helm Shell helm upgrade --install tfy-k8s-generic-inframold \\ truefoundry/tfy-k8s-generic-inframold \\ -f values.yaml -n argocd Configuring Node Pools for TrueFoundry To enable node pools in your cluster with TrueFoundry, follow these steps: Node Pool Label Configuration Ensure all nodes in your cluster have a node pool label. This label should follow the format <nodepool-label-key>: <nodepool-name> . Example: If your node pool label key is truefoundry.com/nodepool , each node should have a label like truefoundry.com/nodepool: <nodepool-name> . You can use any label key already in use. To configure the label key in TrueFoundry: Click on edit in the Cluster and then toggle on the Advanced Fields section. Under Node Label Keys, fill in the Nodepool Selector Label with your chosen label key. GPU Node Pool Configuration To deploy workloads on GPU nodes, assign labels to GPU node pools indicating the GPU type. Label Key: truefoundry.com/gpu_type Possible Values: A10G A10_12GB A10_24GB A10_4GB A10_8GB A100_40GB A100_80GB H100_80GB H100_94GB H200 L4 L40S P100 P4 T4 V100 Example: If you have a nodepool with A10G (24GB) nodes, label nodes of this nodepool with truefoundry.com/gpu_type: A10_24GB . This configuration helps TrueFoundry orchestrate deployments on the appropriate nodes. Adding domain to Load balancer We need to add one more domain to the load balancer so that a separate domain can be used to host the workloads only. This domain can be a wildcard (recommended) as well. To add the domain Point the domain to the load balancer IP address. Pass the TLS certificate to istio so that it can terminate the TLS traffic. Add the domain in the platform. Adding docker registry Check the integrations section to know more on this Updated 4 months ago",
    "https://docs.truefoundry.com/docs/loadbalancers": "LoadBalancer Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account LoadBalancer All Pages Start typing to search\u2026 LoadBalancer Truefoundry by default provisions a single external load-balancer for one Kubernetes cluster. This is provisioned automatically by the tfy-istio-ingress helm chart installed by Truefoundry which creates a Kubernetes service of type LoadBalancer . You can find the configuration of this service in Deployments > Helm > tfy-istio-ingress (Make sure you are filtering for the desired cluster) You can click on the three dots to understand the configuration. If you want to modify any of your load-balancer settings, you will have to edit this configuration according to the guide mentioned below. Modifying your load balancer configuration The loadbalancer can be modified using annotations on the gateway object. Below is an example of possible modifications that you can do. To get a complete list of annoations - check here AWS YAML gateway: annotations: # Denotes that this is a network load balancer. We use NLB so that TCP protocol can also be # supported. Also, NLB has lower latency and costs than ALB (Application Load Balancer) service.beta.kubernetes.io/aws-load-balancer-type: nlb # This is the default setting - It makes the loadbalancer external. If you want to create an internal # load-balancer, you can remove this annotations service.beta.kubernetes.io/aws-load-balancer-type: \"external\" service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\" # Use this only when you want to make the loadbalancer internal. Else remove this annotation service.beta.kubernetes.io/aws-load-balancer-internal: \"true\" # ACM cert arn to attach to the load balancer. If you have your own custom certificate, then you can remove this annotation. service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-2:XXXXXXX:certificate/XXXXX-XXXXX-XXXXXX # You can let these settings as it is service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https service.beta.kubernetes.io/aws-load-balancer-alpn-policy: HTTP2Preferred service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\" tfyGateway: name: tfy-wildcard spec: servers: - tls: httpsRedirect: true port: name: http-tfy-wildcard number: 80 protocol: HTTP hosts: - \"*\" - port: name: https-tfy-wildcard number: 443 protocol: HTTP hosts: - \"*\" selector: istio: tfy-istio-ingress As mentioned above, if you want the loadbalancer to be external, add the following annotation: YAML service.beta.kubernetes.io/aws-load-balancer-type: \"external\" service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing If you want it to be internal, the following annotation should be added: YAML service.beta.kubernetes.io/aws-load-balancer-scheme: \"internal\" If you convert the load balancer from internal to external or vice-versa, the loadbalancer will be recreated and you will have to remap your DNS. GCP The default configuration creates an external load-balancer: YAML gateway: tolerations: - key: \"cloud.google.com/gke-spot\" value: \"true\" effect: NoSchedule operator: Equal affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: cloud.google.com/gke-spot values: - \"true\" operator: In tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP To make the loadbalancer internal, add the annotation as follows: YAML gateway: annotations: networking.gke.io/load-balancer-type: \"Internal\" Azure The default configuration creates an external load balancer with the below configuration YAML gateway: tolerations: - key: CriticalAddonsOnly value: \"true\" effect: NoSchedule operator: Equal tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP To make the load balancer internal YAML gateway: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" Generic By default load balancer will be created without an external IP address in the case of generic cluster. Run metalLB to assign an IP address to the load balancer. Once the load balancer gets the IP address assign the certificates for it to terminate TLS traffic. For that you can use cert-manager or bring your own certificates. Cert-manager is present as an add-on in the cluster, so you can manage and edit the configurations for it vey seamlessly through the platform itself. cert-manager connects with your DNS providers and creates a secret in the k8s cluster. If you bring your own certificates, they need to be in the form of k8s secret. Check how you can create tls secret in k8s. Secret should be created in the istio-system namespace. Passing those secrets in the tfy-istio-ingress in tfyGateway.spec.servers[1].tls.credentialName YAML tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP tls: mode: SIMPLE credentialName: <> Deploy multiple load balancers Each installation of tfy-istio-ingress creates a load balancer. If you want to deploy multiple multiple load-balancers, for e.g. one internal and one external, you can clone the current tfy-istio-ingress application in the same namespace istio-system , change the tfyGateway.Name to something else other then default tfy-wildcard and update the tfyGateway.spec.Selector with the new name of the application. For e.g. if you clone the tfy-istio-ingress a new application with the name tfy-istio-ingress-1 will be created , update the tfyGateway.Name to a new name and the tfyGateway.spec.Selector to YAML tfyGateway: spec: selector: app: \"tfy-istio-ingress-1\" Once the ingress is installed, it will automatically create another loadbalancer whose IP you can get using kubectl get svc -n istio-system . Add authentication to all services behind a load balancer We can configure Istio to apply authentication at a gateway level. This will work only if you are accessing the service using the DNS provided in Istio and not access the service directly from within the cluster. This process is a bit complicated, and you should only do this if you really want to enable authentication at an istio gateway level. \ud83d\udcd8 Istio will validate if the JWT is valid. If not valid, it will return an Unauthorized Error. Create a RequestAuthentication resource to ensure that the JWT issuer and Audience are correct. Authentication will be only done if there is an Authorization header. This is pass-through if no Authorization header is present in the Request or it gets an empty string after removing the prefix. apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: tfy-oauth2 namespace: istio-system spec: selector: matchLabels: istio: tfy-istio-ingress jwtRules: - issuer: \"truefoundry.com\" fromHeaders: - name: Authorization prefix: \"Bearer \" audiences: - <tenant_name_in_truefoundry> jwksUri: https://login.truefoundry.com/.well-known/jwks.json forwardOriginalToken: true Create an AuthorizationPolicy that will reject any requests with an empty JWT. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: tfy-oauth2 namespace: istio-system spec: selector: matchLabels: istio: tfy-istio-ingress action: DENY rules: - from: - source: notRequestPrincipals: [\"*\"] to: - operation: ports: - \"443\" You can read the Istio docs: https://istio.io/latest/docs/reference/config/security/request_authentication/ for further customization or making it work with your own IdP. Updated 7 days ago",
    "https://docs.truefoundry.com/docs/add-certificate-for-tls": "Add TLS Certificates Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Add TLS Certificates All Pages Start typing to search\u2026 Add TLS Certificates Configure secure HTTPS access to your TrueFoundry deployment Adding TLS Certificates to Your TrueFoundry Deployment This guide explains how to configure TLS certificates to enable secure HTTPS access to your TrueFoundry deployment. We'll cover multiple approaches based on your cloud provider. Quick Reference Guide Cloud Provider Recommended Method Alternative Methods Reference Guide AWS AWS Certificate Manager cert-manager with DNS validation AWS DNS & TLS Setup GCP cert-manager with Cloud DNS Manual certificate files GCP DNS & TLS Setup Azure cert-manager with Azure DNS Manual certificate files Azure DNS & TLS Setup Generic Manual certificate files cert-manager with Let's Encrypt Generic Cluster Setup AWS: Using Certificate Manager When running TrueFoundry on AWS EKS, you have two options for TLS termination: Terminate TLS at the Network Load Balancer (recommended) Terminate TLS at the Istio ingress layer For production AWS deployments, terminating TLS at the Network Load Balancer using AWS Certificate Manager (ACM) is recommended for best performance and manageability. Step-by-Step Guide for AWS Certificate Manager Create a certificate in ACM : Navigate to AWS Certificate Manager in the AWS console Request a public certificate Specify your domain (e.g., *.example.com ) Choose DNS validation (recommended) Validate domain ownership : Add the CNAME records provided by ACM to your DNS provider Follow the official AWS guide for DNS validation For detailed steps on adding CNAME records, see AWS documentation on DNS validation Wait for the certificate to change to \"Active\" status (this may take 30 minutes or longer) Copy the certificate ARN for the next step (format will be like: arn:aws:acm:region:account:certificate/certificate-id ) Apply the certificate to your TrueFoundry deployment : In the TrueFoundry platform, navigate to Deployments > Helm Filter to find the helm chart for your cluster Select tfy-istio-ingress Click Edit and update the configuration: YAML gateway: annotations: service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <your-certificate-arn-here> Configure domain routing : In the same tfy-istio-ingress configuration, update the gateway configuration with your custom host: YAML tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"<EDIT HERE *.example.com>\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"<EDIT HERE *.example.com>\" port: name: https-tfy-wildcard number: 443 protocol: HTTPS GCP: Using cert-manager with Cloud DNS For GCP deployments, we recommend using cert-manager with Let's Encrypt and GCP Cloud DNS for automatic certificate issuance and renewal. \ud83d\udcd8 Prerequisites This approach requires workload identity to be enabled on your GKE cluster. For clusters created through Terraform code generated by Truefoundry, this is enabled by default. Step 1: Configure GCP Service Account and Permissions Shell # Set your variables export CLUSTER_NAME=\"<your-gke-cluster>\" export CLUSTER_LOCATION=\"<your-cluster-zone>\" #(us-central1) # Get your GCP project ID (if you don't know it) export PROJECT_ID=$(gcloud config get-value project) # Or list available projects and select one # gcloud projects list # This is a suggested role name that you can customize export DNS_ROLE_NAME=\"<cert_manager_dns_role>\" export GCP_SERVICEACCOUNT_NAME=\"cert-manager-dns\" export MAIL_ID=\"< [email protected] >\" # Get load balancer IP (once cluster is set up) export LOAD_BALANCERIP=$(kubectl get svc \\ -n istio-system tfy-istio-ingress \\ -ojsonpath='{.status.loadBalancer.ingress[0].ip}') # Verify values echo \"PROJECT_ID: ${PROJECT_ID}\" echo \"CLUSTER_NAME: ${CLUSTER_NAME}\" echo \"CLUSTER_LOCATION: ${CLUSTER_LOCATION}\" echo \"LOAD_BALANCERIP: ${LOAD_BALANCERIP}\" # Set project gcloud config set project $PROJECT_ID # Create service account gcloud iam service-accounts create $GCP_SERVICEACCOUNT_NAME \\ --display-name \"$GCP_SERVICEACCOUNT_NAME\" # Create custom role with minimal DNS permissions gcloud iam roles create custom_dns_role \\ --project=$PROJECT_ID \\ --title=$DNS_ROLE_NAME \\ --permissions=dns.resourceRecordSets.create,dns.resourceRecordSets.delete,dns.resourceRecordSets.get,dns.resourceRecordSets.list,dns.resourceRecordSets.update,dns.changes.create,dns.changes.get,dns.changes.list,dns.managedZones.list # Bind role to service account gcloud projects add-iam-policy-binding $PROJECT_ID \\ --condition=None \\ --member=serviceAccount:$GCP_SERVICEACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com \\ --role=\"projects/$PROJECT_ID/roles/custom_dns_role\" # Allow cert-manager to use the service account through workload identity gcloud iam service-accounts add-iam-policy-binding \\ --role roles/iam.workloadIdentityUser \\ --member \"serviceAccount:$PROJECT_ID.svc.id.goog[cert-manager/cert-manager]\" \\ $GCP_SERVICEACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com Step 2: Configure DNS for Your Domain Shell # Set DNS zone name export DNS_HOSTED_ZONE=\"example.com\" export DNS_ZONE_RESOURCE_GROUP=\"${PROJECT_ID}\" # Create DNS zone (if it doesn't exist) gcloud dns managed-zones create example-com \\ --description=\"DNS zone for ${DNS_HOSTED_ZONE}\" \\ --dns-name=${DNS_HOSTED_ZONE} \\ --visibility=public # Get nameservers gcloud dns managed-zones describe example-com \\ --format=\"json\" | jq -r '.nameServers[]' Add the nameservers from the output to your domain registrar (GoDaddy, Namecheap, etc.). If using an existing DNS zone, you can list all zones and find yours: Shell # List all DNS zones gcloud dns managed-zones list # Get details of your specific zone gcloud dns managed-zones describe YOUR_ZONE_NAME Step 3: Install and Configure cert-manager (Skip for clusters Created through Truefoundry-generated Terraform) Note: If you installed your cluster using Terraform code generated by the Truefoundry platform, cert-manager is already installed and configured correctly. You can skip this step and proceed to Step 4. In the TrueFoundry platform, navigate to Integrations Select your cluster Click the three dots and select Manage Applications Install cert-manager if not already installed If already installed, edit its configuration: Go to Deployments > Helm Filter for your cluster Find and edit cert-manager Ensure the configuration includes: YAML extraArgs: - --issuer-ambient-credentials serviceAccount: create: true annotations: iam.gke.io/gcp-service-account: $GCP_SERVICEACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com Replace placeholders with your actual values. Step 4: Create Certificate Issuer and Request Certificate Get your cluster credentials: Shell gcloud container clusters get-credentials $CLUSTER_NAME --zone $CLUSTER_LOCATION --project $PROJECT_ID Create an issuer for Let's Encrypt: YAML kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-issuer namespace: istio-system spec: acme: email: $MAIL_ID server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-key solvers: - dns01: cloudDNS: project: $PROJECT_ID EOF Request a certificate: YAML kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: example-com-cert namespace: istio-system spec: secretName: example-com-tls duration: 2160h # 90d renewBefore: 360h # 15d issuerRef: name: letsencrypt-issuer dnsNames: - \"example.com\" - \"*.example.com\" EOF Check certificate status: Shell kubectl get certificates -n istio-system \ud83d\udea7 Troubleshooting If the certificate remains in a non-ready state for more than 10 minutes, check the cert-manager logs: Shell kubectl logs -n cert-manager -l app=cert-manager Step 5: Configure TLS in Your Ingress Gateway In the TrueFoundry platform, navigate to Deployments > Helm Filter for your cluster and select tfy-istio-ingress Update the configuration: YAML tfyGateway: name: tfy-wildcard spec: selector: istio: tfy-istio-ingress servers: - hosts: - \"*.example.com\" - \"example.com\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*.example.com\" - \"example.com\" port: name: https-tfy-wildcard number: 443 protocol: HTTPS tls: mode: SIMPLE credentialName: example-com-tls Step 6: Update Cluster Metadata with Domain Navigate to Integrations in the TrueFoundry platform Select your cluster and click Edit Enable Show advanced fields Enable Base Domain URL Add your domain (e.g., example.com ) Azure: Using cert-manager with Azure DNS For Azure deployments, we use cert-manager with Let's Encrypt and Azure DNS for automated certificate management. Step 1: Configure Variables and Workload Identity Shell # Set your variables export CLUSTER_NAME=\"<your-aks-cluster>\" export RESOURCE_GROUP=\"<your-resource-group>\" export AZURE_SUBSCRIPTION_ID=\"<your-subscription-id>\" export SERVICE_ACCOUNT_NAME=cert-manager export SERVICE_ACCOUNT_NAMESPACE=cert-manager export MAIL_ID=\"< [email protected] >\" # Get OIDC issuer URL and load balancer IP export OIDC_ISSUER_URL=$(az aks show \\ --resource-group $RESOURCE_GROUP \\ --name $CLUSTER_NAME \\ --query \"oidcIssuerProfile.issuerUrl\" -o tsv) export LOAD_BALANCERIP=$(kubectl get svc \\ -n istio-system tfy-istio-ingress \\ -ojsonpath='{.status.loadBalancer.ingress[0].ip}') # Create and get managed identity export IDENTITY_NAME=\"$CLUSTER_NAME\" # Create managed identity and get IDs PRINCIPAL_ID=$(az identity create \\ --name \"${IDENTITY_NAME}\" \\ --resource-group \"${RESOURCE_GROUP}\" \\ --query principalId -otsv) IDENTITY_CLIENT_ID=$(az identity show \\ --name \"${IDENTITY_NAME}\" \\ --resource-group \"${RESOURCE_GROUP}\" \\ --query 'clientId' -otsv) # Verify values echo \"PRINCIPAL_ID: ${PRINCIPAL_ID}\" echo \"IDENTITY_CLIENT_ID: ${IDENTITY_CLIENT_ID}\" echo \"OIDC_ISSUER_URL: ${OIDC_ISSUER_URL}\" echo \"LOAD_BALANCERIP: ${LOAD_BALANCERIP}\" If needed, enable workload identity on your AKS cluster: Shell az aks update \\ --name ${CLUSTER_NAME} \\ --resource-group ${RESOURCE_GROUP} \\ --enable-oidc-issuer \\ --enable-workload-identity Step 2: Configure DNS Zone and Permissions Create a DNS zone (or use existing one): Shell # Set DNS zone name export DNS_HOSTED_ZONE=\"example.com\" export DNS_ZONE_RESOURCE_GROUP=\"${RESOURCE_GROUP}\" # Create DNS zone az network dns zone create \\ --name ${DNS_HOSTED_ZONE} \\ --resource-group ${DNS_ZONE_RESOURCE_GROUP} \\ --query nameServers Add the nameservers from the output to your domain registrar. If using an existing DNS zone: Shell # Get the DNS zone ID DNS_ZONE_ID=$(az network dns zone show \\ --name ${DNS_HOSTED_ZONE} \\ --resource-group ${DNS_ZONE_RESOURCE_GROUP} \\ --query id -otsv) # Assign permissions az role assignment create \\ --assignee $PRINCIPAL_ID \\ --role \"DNS Zone Contributor\" \\ --scope $DNS_ZONE_ID # Set up federated credentials az identity federated-credential create \\ --name \"cert-manager\" \\ --identity-name \"${IDENTITY_NAME}\" \\ --issuer \"${OIDC_ISSUER_URL}\" \\ --resource-group \"${RESOURCE_GROUP}\" \\ --subject \"system:serviceaccount:${SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}\" Step 3: Install and Configure cert-manager In TrueFoundry, navigate to Integrations > [Your Cluster] > Manage Applications Install cert-manager if not already installed Configure with these values: YAML installCRDs: true extraArgs: - --issuer-ambient-credentials podLabels: azure.workload.identity/use: \"true\" serviceAccount: labels: azure.workload.identity/use: \"true\" Step 4: Create Certificate Issuer and Request Certificate Get your cluster credentials: Shell az aks get-credentials --name $CLUSTER_NAME --resource-group $RESOURCE_GROUP Create an issuer: YAML kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: azure-issuer namespace: istio-system spec: acme: email: $MAIL_ID server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: azure-issuer-key solvers: - dns01: azureDNS: hostedZoneName: $DNS_HOSTED_ZONE resourceGroupName: $DNS_ZONE_RESOURCE_GROUP subscriptionID: $AZURE_SUBSCRIPTION_ID environment: AzurePublicCloud managedIdentity: clientID: $IDENTITY_CLIENT_ID EOF Request a certificate: YAML kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: example-cert namespace: istio-system spec: secretName: example-tls duration: 2160h # 90d renewBefore: 360h # 15d issuerRef: name: azure-issuer dnsNames: - \"example.com\" - \"*.example.com\" EOF Check certificate status: Shell kubectl get certificates -n istio-system Step 5: Configure TLS in Your Ingress Gateway Follow the same configuration steps as in the GCP section to set up your ingress gateway with the TLS certificate. Step 6: Update Cluster Metadata with Domain Navigate to Integrations in the TrueFoundry platform Select your cluster and click Edit Enable Show advanced fields Enable Base Domain URL Add your domain (e.g., example.com ) Generic Cluster: Using cert-manager with Let's Encrypt For generic Kubernetes clusters, you can use cert-manager with Let's Encrypt to automatically issue and manage certificates. Step 1: Install cert-manager To install cert-manager through TrueFoundry's Addon/Helm section: Navigate to Clusters in the TrueFoundry platform Select your cluster Click the three dots (\u22ee) and select Manage Addons Look for cert-manager in the list of available applications Click Install Step 2: Create a ClusterIssuer for Let's Encrypt Shell # Set your email address export EMAIL=\"< [email protected] >\" # Create HTTP01 issuer for Let's Encrypt (production) kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: ${EMAIL} privateKeySecretRef: name: letsencrypt-prod-key solvers: - http01: ingress: class: istio EOF Step 3: Request a Certificate Shell # Set your domain name export DOMAIN=\"<example.com>\" # Request a certificate (production) # Only use this after testing with staging certificate kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: example-cert-prod namespace: istio-system spec: secretName: example-cert-prod issuerRef: name: letsencrypt-prod kind: ClusterIssuer commonName: \"${DOMAIN}\" dnsNames: - \"${DOMAIN}\" - \"*.${DOMAIN}\" EOF Step 4: Configure the Ingress Gateway Once the certificate is issued, configure the Istio gateway to use it: YAML tfyGateway: name: tfy-wildcard spec: selector: istio: tfy-istio-ingress servers: - hosts: - \"*.example.com\" - \"example.com\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*.example.com\" - \"example.com\" port: name: https-tfy-wildcard number: 443 protocol: HTTPS tls: mode: SIMPLE credentialName: example-cert-prod # Use your certificate secret name Using Your Own Certificate Files If you have your own certificate files (e.g., from another certificate provider or self-signed), you can use them directly with TrueFoundry. \ud83d\udcd8 Note for Generic Kubernetes Clusters This method is particularly useful for generic Kubernetes deployments or when you want to use certificates issued by your organization's certificate authority. Option 1: Import Existing Certificate Files If you already have certificate and key files: Create a Kubernetes secret with your certificate and key: Shell # Create secret from local certificate files kubectl create secret tls my-tls-secret \\ --cert=path/to/cert/file \\ --key=path/to/key/file \\ -n istio-system Alternatively, you can create the secret using a YAML definition: YAML apiVersion: v1 kind: Secret metadata: name: my-tls-secret namespace: istio-system type: kubernetes.io/tls data: tls.crt: <base64-encoded-certificate> tls.key: <base64-encoded-private-key> \ud83d\udcd8 Tip To encode your certificate and key files in base64: Shell cat path/to/cert/file | base64 -w 0 cat path/to/key/file | base64 -w 0 Configure the ingress gateway to use your certificate: YAML tfyGateway: name: tfy-wildcard spec: selector: istio: tfy-istio-ingress servers: - hosts: - \"*.example.com\" - \"example.com\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*.example.com\" - \"example.com\" port: name: https-tfy-wildcard number: 443 protocol: HTTPS tls: mode: SIMPLE credentialName: my-tls-secret Option 2: Generate Self-Signed Certificates For testing or internal use, you can generate self-signed certificates: Shell # Generate a self-signed certificate openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout tls.key -out tls.crt \\ -subj \"/CN=*.example.com\" \\ -addext \"subjectAltName = DNS:example.com,DNS:*.example.com\" # Create a secret using the generated files kubectl create secret tls self-signed-tls \\ --cert=tls.crt \\ --key=tls.key \\ -n istio-system Then configure your gateway to use the self-signed-tls secret as shown in Option 1. \u26a0\ufe0f Warning Self-signed certificates will cause browser warnings. They should only be used for testing or internal systems. Troubleshooting Certificate not issued : Check cert-manager logs in the cert-manager namespace HTTPS not working : Verify your secret name matches the credential name in the gateway configuration DNS errors : Make sure your nameservers are correctly configured at your domain registrar \ud83d\udcd8 Need Further Help? For platform-specific TLS configuration guidance, refer to the respective guides: AWS DNS & TLS Setup GCP DNS & TLS Setup Azure DNS & TLS Setup Generic Cluster Setup Updated 8 days ago",
    "https://docs.truefoundry.com/docs/metrics": "Metrics Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Metrics All Pages Start typing to search\u2026 Metrics Coming Soon Updated 5 months ago",
    "https://docs.truefoundry.com/docs/logging": "Logging Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Logging All Pages Start typing to search\u2026 Logging Coming Soon Updated 5 months ago",
    "https://docs.truefoundry.com/docs/cost-monitoring": "Cost Monitoring Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Cost Monitoring All Pages Start typing to search\u2026 Cost Monitoring Truefoundry integrates with Kubecost to provide cost visibility for your deployments. Kubecost is one of the applications that can be installed on the cluster. Once you install the application, you will start getting cost updates on the Truefoundry portal. However, by default, Kubecost is not able to pull the spot instance prices - for this you need to setup Spot Datafeed collection on your AWS account. This guide provides a quick way to setup spot data-feed collection. AWS Spot Integration Run the following bash script. Please replace profile , region and cluster_name before running shell # Enter your profile, region, and cluster name export profile=\"default\" export region=\"\" export cluster_name=\"\" # (Do not edit) Declares some variables export bucket_name=$cluster_name-kubecost export account_id=$(aws sts get-caller-identity --query \"Account\" --output text --profile $profile) export oidc_provider=$(aws eks describe-cluster --name $cluster_name --region $region --query \"cluster.identity.oidc.issuer\" --output text --profile $profile | sed -e \"s/^https:\\/\\///\") export role_arn_name=$cluster_name-kubecost-role-arn export namespace=kubecost export service_account=kubecost-cost-analyzer # Create s3 bucket to store spot data feed aws s3api create-bucket \\ --bucket $bucket_name \\ --region $region \\ --object-ownership ObjectWriter \\ --profile $profile # Subscribe to spot data feed aws ec2 create-spot-datafeed-subscription \\ --bucket $bucket_name \\ --region $region \\ --profile $profile # Create IAM policy to access the s3 bucket ## Create policy.json cat >policy.json <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"SpotDataFeed\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\", \"s3:ListBucket\", \"s3:List*\", \"s3:Get*\" ], \"Resource\": \"arn:aws:s3:::$bucket_name*\" } ] } EOF ## Create policy aws iam create-policy \\ --policy-name SpotDataFeed \\ --policy-document file://policy.json \\ --profile $profile # Create IAM role to assume the role as service account ## Create trust-relationship.json cat >trust-relationship.json <<EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::$account_id:oidc-provider/$oidc_provider\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"$oidc_provider:aud\": \"sts.amazonaws.com\", \"$oidc_provider:sub\": \"system:serviceaccount:$namespace:$service_account\" } } } ] } EOF ## Create role aws iam create-role \\ --role-name $role_arn_name \\ --assume-role-policy-document file://trust-relationship.json \\ --profile $profile ## Attach policy to role aws iam attach-role-policy \\ --role-name $role_arn_name \\ --policy-arn=arn:aws:iam::$account_id:policy/SpotDataFeed \\ --profile $profile # Print the role ARN echo \"Role ARN: arn:aws:iam::$account_id:role/$role_arn_name\" Nagivate to Workspaces in TrueFoundry UI and edit kubecost workspace in your cluster. Switch to Show Advanced options and add Service Account with name: kubecost-cost-analyzer and role arn value as arn:aws:iam::<account_id>:role/<role-arn-name> Navigate to Deployments > Helm and edit kubecost . Then, add following section to values: values kubecostProductConfigs: projectID: \"<account_id>\" awsSpotDataBucket: <bucket-name> awsSpotDataPrefix: \"\" awsSpotDataRegion: <region> Azure Spot Integration Coming soon... GCP Spot Integration Coming soon... Updated 5 months ago",
    "https://docs.truefoundry.com/docs/notebook-controller": "Notebook Controller Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Notebook Controller All Pages Start typing to search\u2026 Notebook Controller This component is required if you want to enable the deployment of Notebooks / VS Code Server (from browser). You can install Notebook Controller for your Cluster by going to Integrations -> Clusters -> Installed Applications Configuring Base Domain Url This is a required field for the deployment of Notebook Controller. You need to specify a Base Domain (should be already configured in the cluster on truefoundry) and add it in the values: values.yaml notebookBaseDomainUrl: https://nb.ml.demo.xyz.cloud Storage Class for Notebook Every Notebook/SSH Server is backed by a Volume. So your cluster must have a storage class setup. You can define the storage class in values.yaml as shown below: values.yaml notebookBaseDomainUrl: https://nb.ml.demo.xyz.cloud defaultStorageClass: your-storage-class Note: This is an optional field, If not specified, here is the default we use for each cloud: AWS_EKS: gp2 GCP GKE: standard-rwo Azure AKS: default Generic K8S Cluster: default Civo: civo-volume Enabling OAuth in Notebooks By default there is no authentication/authorization in Notebooks. Anyone who has access to the link to the notebook can access it. To enable OAuth in notebooks, you can add the following field in the values.yaml file values.yaml notebookBaseDomainUrl: https://nb.ml.demo.xyz.cloud oauth: type: truefoundry enabled: true By default, if OAuth is enabled in notebooks, Authorization is also enabled, which restricts access to users who have at least workspace viewer permissions for the workspace where the notebook is present in. If you want to override this behavior, you can disable the truefoundry authorization by adding the following field in values.yaml file values.yaml oauth: truefoundryExternalAuthorization: enabled: false Updated 5 months ago",
    "https://docs.truefoundry.com/docs/tfy-agent": "TFY Agent Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account TFY Agent All Pages Start typing to search\u2026 TFY Agent What is TFY Agent? TFY Agent runs on the compute plane cluster and connects the cluster to the control plane. TFY Agent allows the control plane to deploy applications (Service, Job, SSH Server, etc.) and track their status. TFY Agent does not require an open endpoint (Load balancer Service or Node Port) in your cluster. TFY Agent initiates secure WebSocket connections to the control plane, which uses these connections for all activities. How is TFY Agent installed in the cluster? TFY Agent is installed in the cluster using a public Helm Chart. Architecture TFY Agent consists of two parts: TFY Agent and TFY Agent Proxy. Each runs independently and uses a secure WebSocket connection to connect to the control plane. TFY Agent TFY Agent streams the state of the compute plane cluster to the control plane. This state includes but is not limited to the status of Nodes, Pods, Argo Applications, etc., in the cluster. TFY Agent Proxy TFY Agent Proxy enables the control plane to access the compute plane cluster's Kubernetes API server and other Services . This enables the control plane to create namespace (Workspace) and deploy applications to the compute plane cluster. Authorization The TFY Agent helm chart uses Kubernetes RBAC objects to set up authorization rules for both the TFY Agent and TFY Agent Proxy. TFY Agent TFY Agent runs informers to stream kubernetes resource changes and sends it to the control plane. To run informers, the TFY Agent must be able to list and watch those resource types across all the namespaces in the cluster. The config.allowedNamespaces field on the helm chart allows you to configure a list of allowed namespaces. TFY Agent will filter out any namespaced resource's update if the resource is not part of the allowed namespaces. YAML config: tenantName: your-tenant-name controlPlaneURL: your-controlplane-url clusterTokenSecret: your-cluster-token-secret allowedNamespaces: - tfy-agent - argocd - your-namespace TFY Agent Proxy By default, the TFY Agent Proxy enables the control plane to access all resources on the compute cluster. Strict Mode YAML config: tenantName: your-tenant-name controlPlaneURL: your-controlplane-url clusterTokenSecret: your-cluster-token-secret tfyAgentProxy: clusterRole: strictMode: true If you want to give minimum authorization, please set the field tfyAgentProxy.clusterRole.strictMode to true on the helm chart. In this mode, we set up minimum required authorization rules for the control plane to function correctly. If you give a list of allowed namespaces using the config.allowedNamespaces field, we will always run on strict mode, regardless of the value of the tfyAgentProxy.clusterRole.strictMode field. In strict mode, we allow the control plane to create namespaces (Workspace) in the cluster. But if you give a list of allowed namespaces using the config.allowedNamespaces field, the control plane will not be allowed to create new namespaces. Cluster Scope resource access This file documents all the authorization rules we set for the resources for which we require cluster-scope access. Note that if you have configured a list of allowed namespaces, the control plane cannot create any new namespace in the cluster. Namespace Scope resource access This file documents all the authorization rules we set for the resources the control plane can work with namespace-scope access. If you give a list of allowed namespaces using the config.allowedNamespaces field, we setup role binding only for those namespaces. If the list of allowed namespaces is empty. We set up cluster-wide access for these namespaced resources. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/enable-self-signed-ca-certificates": "Support self signed CA certificates Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Support self signed CA certificates All Pages Start typing to search\u2026 Support self signed CA certificates For organisations that enforce using self-signed certificates to connect to external services or other services, Truefoundry platform will have to be configured accordingly. We will use kyverno to enable automated CA cert injection on pods selectively. Steps Install kyverno Shell $ kubectl create namespace kyverno $ helm repo add kyverno https://kyverno.github.io/kyverno/ $ helm install kyverno kyverno/kyverno --version 3.2.5 -n kyverno We will use tfy-kyverno-config helm chart to enable cert injection link . Create a values.yaml and fill in the values. YAML addCaCertificateVolume: enabled: true sourceNamespace: <source_namespace> sourceConfigMap: name: <name of the source configmap> subPath: <configmap data subpath> destinationConfigMap: name: <name of the cloned configmap to create in each namespace> envs: [] injectionConfigs: - label: key: <\"app.kubernetes.io/instance\"> value: <\"truefoundry\"> mountPaths: - </etc/pki/tls/certs> Install the helm chart $ helm install kyverno kyverno/kyverno --version 3.2.5 -n kyverno -f values.yaml This will install a Kyverno ClusterPolicy which will inject the ca certificates in the pods getting created along with env variables if needed. You can also add more addCaCertificateVolume.injectionConfigs with custom labels for workloads that need the CA certificate injected. The per-service labels can be configured from the truefoundry UI Updated 5 months ago",
    "https://docs.truefoundry.com/docs/enabling-git-integrations": "Enabling Git Integrations Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Enabling Git Integrations All Pages Start typing to search\u2026 Enabling Git Integrations Gitlab To enable Gitlab integration in your app, you need to create a gitlab app using your gitlab account. This gitlab app can then be authenticated to access repositories of users' account. Follow these steps to create the gitlab app and integrate it with truefoundry: Sign in to GitLab.com. On the left sidebar, select your avatar and select Edit profile. On the left sidebar, select Applications . Click on the Add new application button right side. Provide the required details for Add new application . Name: This can be anything. Consider something like <Organization> 's GitLab or <Your Name> 's GitLab or something else descriptive. Redirect URI: https://app.example-org.truefoundry.com/api/svc/v1/vcs/gitlab/callback Enable the Confidential check box. Select the following scopes: read_api, read_user, read_repository, write_repository. Select Save application . You should now see an Application ID and Secret . List of connected repositories Set environment variables in servicefoundry server : GITLAB_APP_ID: '' GITLAB_APP_SECRET: '' GITLAB_SCOPE: 'read_api read_user read_repository write_repository' Azure Repos This guide walks you through the process of enabling Azure Repo integration with TrueFoundry by creating an app registration in Microsoft Entra ID (formerly Azure AD) and providing the necessary client environment variables to TrueFoundry. This allows the TrueFoundry Control Plane to authenticate and interact with Azure Repos on behalf of the registered app. Prerequisites Ensure you have: Access to the Azure portal with the necessary permissions to create an app registration. Administrator or sufficient privileges in your Azure DevOps organization. The ability to configure environment variables in TrueFoundry. The control plane URL of your TrueFoundry deployment. Step 1: Create an App Registration in Microsoft Entra ID Log in to the Azure Portal : Go to Azure Portal and sign in with your Microsoft account. Navigate to Microsoft Entra ID (Azure Active Directory) : From the left-hand navigation pane, select Microsoft Entra ID (formerly Azure Active Directory). Register a New Application : Under Manage , select App registrations and click on New registration . Name : Provide a name for the application (e.g., TrueFoundry Integration ). Supported account types : Choose Accounts in this organizational directory only (Single tenant) if your Azure DevOps organisation is linked to the current tenant. Otherwise, choose multi-tenant option. Redirect URI : Under Redirect URI (optional) , select Web from the dropdown and enter the following URL: <control-plane-url>/api/svc/v1/vcs/azure/callback Replace <control-plane-url> with the control plane URL of your TrueFoundry deployment. It will be something like https://org.truefoundry.cloud and is typically used to access the TrueFoundry Dashboard. Click Register to create the app. Configure API Permissions : After registration, you'll be redirected to the app\u2019s overview page. Under Manage , select API permissions . Click on Add a permission . a. Azure DevOps : Select APIs my organization uses . Search for Azure DevOps and select it. Choose Delegated permissions . Select the permission vso.code_manage to allow management of code repositories. Click Add permissions . b. Microsoft Graph : Click Add a permission again. Select Microsoft Graph . Choose Delegated permissions . Select the permissions offline_access and User.Read . Click Add permissions . Click on Grant admin consent for your organization to ensure the permissions are active and granted. Generate a Client Secret : Under Manage , select Certificates & secrets . Under Client secrets , click New client secret . Provide a description (e.g., TrueFoundry Secret ) and choose an expiry duration. Click Add . Copy the Value of the client secret immediately. You won't be able to retrieve it later. Gather Required Information : Navigate back to the Overview section of the app registration. Note down the Application (client) ID . Step 2: Provide Environment Variables to TrueFoundry In TrueFoundry, you need to configure the following environment variables using the details from the Microsoft Entra ID app registration. Edit the Helm deployment of truefoundry and add the following variables in the env section under servicefoundry-server AZURE_CLIENT_ID : The Application (client) ID of your app registration. AZURE_CLIENT_SECRET : The Client Secret generated in the previous step. YAML servicefoundry-server: # other fields env: # other envs AZURE_CLIENT_ID: <client id of azure app registration> AZURE_CLIENT_SECRET: <client secret> Now you can deploy applications from your Azure Repos to TrueFoundry by linking repositories. You can link repositories by going to Integrations > Git > Azure Repos on the TrueFoundry Dashboard. Updated 27 days ago",
    "https://docs.truefoundry.com/docs/adding-pre-and-post-build-scripts": "Adding pre and post build scripts Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Adding pre and post build scripts All Pages Start typing to search\u2026 Adding pre and post build scripts For a control plane installation, it is possible to add custom scripts which can run before and after your build/push process. This is being used to give more flexibility in terms of modifying the total behaviour of the build process and determining the result of the build pipeline. Adding script in the build pipeline A script can be added before the build and post the build-and-push process. To do this edit your truefoundry helm chart and configure the section of the tfyBuild.truefoundryWorkflows.preBuild and tfyBuild.truefoundryWorkflows.postBuild Following is an example YAML tfyBuild: truefoundryWorkflows: preBuild: image: tag: latest repository: ubuntu script: | #!/bin/bash echo \"Running pre-build script...\" # your steps can come here echo \"Finishing pre-build script.\" command: - /bin/bash enabled: true postBuild: image: tag: latest repository: ubuntu script: | #!/bin/bash echo \"Running post-build script...\" # your steps can come here echo \"Finishing post-build script.\" command: - /bin/bash enabled: true \ud83d\udcd8 Running time-intensive task Each build pipeline is allowed to run for max 14400 seconds post which the build will be declared as failed. User needs to ensure that the pre-build and post-build steps don't exceed the given time. Each step in itself is allowed to run for max 5400 seconds. Each step can refer to few inputs - Build Source - {{inputs.parameters.buildSource}} Build config - {{inputs.parameters.buildConfig}} Docker registry URL - {{inputs.parameters.dockerRegistryURL}} Docker registry username - {{inputs.parameters.dockerRegistryUsername}} Docker registry password - {{inputs.parameters.dockerRegistryPassword}} Docker image repository - {{inputs.parameters.dockerRepo}} Docker image tag - {{inputs.parameters.dockerTag}} An example of using this - YAML postBuild: image: tag: latest repository: ubuntu script: > #!/bin/bash REGISTRY=\"{{inputs.parameters.dockerRegistryURL}}\" REPOSITORY=\"{{inputs.parameters.dockerRepo}}\" TAG=\"{{inputs.parameters.dockerTag}}\" IMAGE=$REGISTRY/$REPOSITORY:$TAG echo \"Registry URL is \"$REGISTRY command: - /bin/bash enabled: true \ud83d\udcd8 Docker cli isn't supported Currently docker CLI is not supported in the pre-build script as it can't connect to any daemon. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/tfy-llm-gateway-installation": "LLM Gateway Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account LLM Gateway All Pages Start typing to search\u2026 LLM Gateway Follow the steps to enable LLM Gateway in your control plane: For enabling request logging and metrics in the LLM gateway, few infra components are required like nats and clickhouse. Follow these steps to install: ( Note : password should not contain @ or : ) Create k8s secret apiVersion: v1 kind: Secret metadata: name: tfy-llm-gateway-infra-auth namespace: truefoundry stringData: CLICKHOUSE_USER_PASSWORD: xxx NATS_ADMIN_PASSWORD: xxx NATS_CLICKHOUSE_REQUEST_LOGS_READER_PASSWORD: xxx NATS_LLM_GATEWAY_REQUEST_LOGGER_PASSWORD: xxx type: Opaque Install tfy-llm-gateway-infra helm chart via truefoundry name: tfy-llm-gateway-infra type: helm source: type: helm-repo chart: tfy-llm-gateway-infra version: 0.2.2 repo_url: https://truefoundry.github.io/infra-charts/ Install tfy-llm-gateway helm chart name: tfy-llm-gateway type: helm source: type: helm-repo chart: tfy-llm-gateway version: 0.18.2 repo_url: https://truefoundry.github.io/infra-charts/ values: global: existingTruefoundryImagePullSecretName: truefoundry-image-pull-secret controlPlaneURL: <> llmGatewayInfra: enabled: true natsAdminPassword: \"${k8s-secret/tfy-llm-gateway-infra-auth/NATS_ADMIN_PASSWORD}\" # to refer k8s secret in step 1 Update the control plane (truefoundry helm chart) configuration - please add the following section under global in values values: ... global: ... llmGatewayInfra: enabled: true natsAdminPassword: \"${k8s-secret/tfy-llm-gateway-infra-auth/NATS_ADMIN_PASSWORD}\" # to refer k8s secret in step 1 clickhousePassword: \"${k8s-secret/tfy-llm-gateway-infra-auth/CLICKHOUSE_USER_PASSWORD}\" # to refer k8s secret in step 1 Updated 3 months ago",
    "https://docs.truefoundry.com/docs/customize-build-workflow": "Customize build workflow Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Customize build workflow All Pages Start typing to search\u2026 Customize build workflow This guide covers adding a custom workflow in the build workflow. Writing your custom build workflow script We support adding custom bash scripts to our build workflow. For example, one may want to scan the source code before building and pushing the image to their docker registry. Shell #!/bin/bash # Example to scan source code using sonarqube, ref: https://docs.sonarsource.com/sonarqube-server/latest/analyzing-source-code/scanners/sonarscanner/#running-from-zip-file sonar-scanner $SOURCE_CODE_DOWNLOAD_PATH -Dsonar.token=$SONARQUBE_AUTH_TOKEN Create config map Create the config map for your custom script to be able to mount that in our build workflow. You can either write a YAML spec and apply it or create it using kubectl command as follows: Terminal kubectl create configmap <configmap-name> --from-file=<script-file-name> -n truefoundry Add to Truefoundry build workflow To add the above custom scrip to our build workflow, we need to first attach the config map as volume and then execute the script in the desired step. Please make the following changes in the values of the truefoundry helm chart YAML tfyBuild: truefoundryWorkflows: extraVolumes: - name: custom-script configMap: name: <config-map-name> defaultMode: 511 extraVolumeMounts: - name: custom-script mountPath: /custom-scripts sfyBuilder: script: | download-code.sh # eg: excute before build and push # /custom-scripts/<your-custom-script-file-name> registry-login.sh wait-for-builder.sh build-and-push.sh # eg: excute after build and push # /custom-scripts/<your-custom-script-file-name> update-build.sh '{\"status\":\"SUCCEEDED\"}' Notes: Please DO NOT remove any existing step as it may cause issues in existing workflow While mounting the configmap, make sure to keep the defaultMode to 511 to make the file executable. Always execute the custom script using the absolute path as the working directory is set to /scripts by default. If you need to set any environment variables to use inside your custom script, you can add those at tfyBuild.truefoundryWorkflows.extraEnvs . Make sure you don't overwrite SOURCE_CODE_DOWNLOAD_PATH, DOCKER_REGISTRY_URL, DOCKER_REGISTRY_USERNAME , DOCKER_REGISTRY_PASSWORD, DOCKER_REPO, DOCKER_TAG, CALLBACK_URL environment variables' as these are reserved for internal usage. You may use them in your workflow if necessary. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/azure-node-pools": "Azure node pools Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Azure node pools All Pages Start typing to search\u2026 Azure node pools In this document we will understand how to better utilise the node pools in AKS for better management of your resources. This document has steps to create CPU/GPU spot as well as on-demand nodepools in your AKS cluster. Below steps are done through azure-cli but can be done through the Azure portal as well. System Node pool If you have followed the steps of Creating the AKS cluster a system node pool must have been created which works on on-demand nodes to deploy all the necessary applications that are required to power the platform. These includes Argocd Argo rollouts Istio tfy-agent It is advisable to use atleast 2 nodes with 2vCPU and 8GB RAM to successfully install all the necessary applications. \ud83d\udcd8 One on-demand node pool is always required BY default the primary node pool of AKS (system node pool) should always be on-demand. It is not possible to create a SPOT node pool initially. User based node pools User based node pools are used to run the user applications. These pools can be of type on-demand or spot . Spot node pool Spot node pools are used to host the user workloads which can tolerate interruptions. As spot instances are the machines which are used from left-overs they can bring significant cost savings in the cloud billing and because of this there are certain applications, dev workloads and un-important job runs which can be promoted to run over spot instances. Creating a SPOT CPU node pool Spot CPU node pool should be used for the cases where the applications can tolerate significant interruptions. By default TrueFoundry can tolerate interruptions on these applications which are supporting the platform Prometheus Loki cert-manager argo-workflows A right instance size can be selected from this page which can help you select the right size/price ratio for your workloads. With the below command you can create a spot instance Shell # export export RESOURCE_GROUP=\"\" export CLUSTER_NAME=\"\" # enter the instance size from the page linked above export INSTANCE_SIZE=\"\" Command to create a spot CPU pool Shell az aks nodepool add \\ --resource-group $RESOURCE_GROUP \\ --cluster-name $CLUSTER_NAME \\ --name <NODEPOOL NAME> \\ --priority Spot \\ --eviction-policy Delete \\ --spot-max-price -1 \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --min-count 2 \\ --node-count 2 \\ --max-count 10 \\ --no-wait With this command a SPOT node pool with minimum 2 nodes will spin up which can autoscale to 10. Spot node pools by default create a taint kubernetes.azure.com/scalesetpriority:spot which means all the pods which you want to get deployed on these spot instances must tolerate this taint. The toleration must happen like this Shell tolerations: - key: kubernetes.azure.com/scalesetpriority value: spot effect: NoSchedule However, this doesn't guarantee that the pods that you meant to deploy on these spot instances will always be deployed there. You have to select the spot node pool while deploying your services to force them to get deployed on spot pools only. Check Adding node pools to the platform to know more. Creating a spot GPU node pool Creating a spot GPU node pool is similar to the CPU spot node pool except for two things Select the right instance size for the GPU workload and make sure you have the required Quotas for GPU instance in your specific region Taint of nvidia.com/gpu=Present:NoSchedule . It is important to add this taint to avoid non-GPU workloads to get deployed on GPU machines Execute the below command to create a node pool in your AKS cluster. Make sure to replace the variable correctly Shell az aks nodepool add \\ --cluster-name $CLUSTER_NAME \\ --name <NODEPOOL NAME> \\ --resource-group $RESOURCE_GROUP \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --priority Spot \\ --spot-max-price -1 \\ --eviction-policy Delete \\ --node-vm-size $INSTANCE_SIZE \\ --node-taints nvidia.com/gpu=Present:NoSchedule \\ --max-count 2 \\ --min-count 1 \\ --node-count 1 \\ --mode user \\ --tags team=datascience owner=truefoundry On-demand or Regular node pools On-demand or Regular node pools are used for deploying your applications which require a dedicated machine to run. These workloads are important or nearly important to run at the required time. On-demand nodes are generally expensive to their counterparts (spot) but have SLA available on them. As a general practice, on-demand nodes don't suffer downtime and doesn't face interruptions. However, It is always to be noted that nodes are ephemeral in nature and upon excess threshold utilisation they can go down. Creating an on-demand CPU node pool Run the below command in your cluster by selecting the right instance size Shell az aks nodepool add \\ --resource-group $RESOURCE_GROUP \\ --cluster-name $CLUSTER_NAME \\ --name <NODEPOOL NAME> \\ --eviction-policy Delete \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --min-count 2 \\ --node-count 2 \\ --node-osdisk-size 100 \\ --max-count 10 \\ --no-wait You can set the count of nodes according to your needs and it is advisable to keep the autoscaling part enabled for your cluster. Creating an on-demand GPU node pool Creating an on-demand GPU node pool is similar to the CPU on-demand node pool except for two things Select the right instance size for the GPU workload and make sure you have the required Quotas for GPU instance in your specific region Taint of nvidia.com/gpu=Present:NoSchedule . It is important to add this taint to avoid non-GPU workloads to get deployed on GPU machines Execute the below command to create a node pool in your AKS cluster. Make sure to replace the variable correctly Shell az aks nodepool add \\ --cluster-name $CLUSTER_NAME \\ --name <NODEPOOL NAME> \\ --resource-group $RESOURCE_GROUP \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --node-taints nvidia.com/gpu=Present:NoSchedule \\ --max-count 2 \\ --min-count 1 \\ --node-count 1 \\ --node-osdisk-size 100 \\ --mode user \\ --tags team=datascience owner=truefoundry Adding node pools in the platform By default TrueFoundry requires an Azure AD application which has Reader access on the AKS cluster to sync all the nodepools. If this Azure AD application is already added to the platform the new nodepools will sync automatically. Understanding Cost implications of spot and on-demand (regular) node-pools A huge difference can be observed while analysing the cloud cost for both spot and on-demand where spot nodes seems to be a highly cheap option. Moreover, this brings lot of uncertainty for the node uptime leading to trade-off. Below is a sample collection of nodes running for a month to analyse costs for spot and on-demand machines. All these are pricing based out of South central US region. Priority Instance type Compute GPU Cost( per month) Spot Standard_D2s_v5 2vCPU/8 GB RAM False $ 10.19 On-demand Standard_D2s_v5 2vCPU/8 GB RAM False $ 83.95 Spot Standard_NC6 6vCPU/56 GB RAM True $ 78.84 On-demand Standard_NC6 6vCPU/56 GB RAM True $ 788.40 Updated 3 months ago",
    "https://docs.truefoundry.com/docs/customizing-cicd-templates": "Customizing CI/CD Templates Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Customizing CI/CD Templates All Pages Start typing to search\u2026 Customizing CI/CD Templates The CI/CD templates can be configured on a control plane level (available for customers using Truefoundry in separate hosted control plane only). This customization can be used to add custom image scanning workflows, security scans etc to the CI/CD pipelines or add whatever custom logic user wants to add. How do CI/CD templates get rendered? Truefoundry renders different CI/CD templates as shown in this document automatically based on the deployment. All the templates can be found here . There is a file called cicd-providers.yaml which lists the providers need to be enabled like github, bitbucket and gitlab. All other files contain templates to configure CI/CD based on different cases like: Whether image is built on TrueFoundry or built elsewhere. Whether the spec is stored as a yaml in your repository (complete GitOps) or not. These are different templates for gitlab, bitbucket and github. How to customize these templates for your Control Plane? The CI/CD templates are supplied to TrueFoundry's control plane via a config map. These config map values can be overridden by updating the values of truefoundry helm chart You will need to update the values of tfy-configs in the truefoundry helm chart. You need to define the content of cicd-providers.yaml with your provider specific details. Here is the default file. Apart from this you need to define all the the other templates of a particular provider (for e.g. github) by taking reference from the files in this folder. ( You should ideally change only the steps field in each yaml file (or maybe the description). No other field should be changed.) values.yaml (truefoundry helm chart) ... tfy-configs: ... configs: cicdTemplates: cicd-providers.yaml: | - id: github name: GitHub icon: github enabled: true - id: gitlab name: GitLab icon: gitlab enabled: false - id: bitbucket name: Bitbucket icon: bitbucket enabled: false github-actions-git-source.yaml: | ... github-actions-local-source.yaml: | ... github-actions-self-build-image.yaml: | ... github-actions-git-source-patch-application.yaml: | ... github-actions-self-build-image-patch-application.yaml: | name: Deploy docker image on TrueFoundry with out spec in Git repository cicd_provider_id: github enabled: true description: \"The application spec will be stored and maintained from TrueFoundry UI. The docker image is built in your CI pipeline and then the image uri is patched in the spec and deployed to truefoundry.\" deployment_mode: patch-application build_source: local recommended_environment: dev image_builder: self steps: - label: Generate API Key icon: null usage: Generate an API Key to authenticate and deploy applications type: generate-api-key - label: Add API Key to Github Secrets icon: null usage: null type: markdown-content args: content: | In your GitHub Repository, navigate to **Settings > Secrets and Variables > Actions**. Add a new secret called `TFY_API_KEY` and set the generated api key as value - label: Create GitHub Action icon: null usage: | Add the below workflow as `tfy-deploy.yaml` in your github workflow directory (`.github/workflows/`). Following GitHub Action will be triggered on each push to `main` branch type: markdown-content args: content: | > **Note:** Please read through the `env` section and Image Build Section and update them for your registry and repo. ```yaml name: Deploy to TrueFoundry on: push: branches: - 'main' permissions: id-token: write contents: read env: TFY_HOST: {{ TRUEFOUNDRY_TFY_HOST }} TFY_API_KEY: $\\{{ secrets.TFY_API_KEY }} APPLICATION_FQN: {{ TRUEFOUNDRY_APPLICATION_FQN }} # Update these with your Docker Registry and Repository DOCKER_REGISTRY: docker.io DOCKER_REPO_NAME: $\\{{ github.event.repository.name }} DOCKER_IMAGE_REPO: $\\{{ env.DOCKER_REGISTRY }}/$\\{{ env.DOCKER_REPO_NAME }} DOCKER_IMAGE_TAG: $\\{{ github.sha }} DOCKER_IMAGE_URI: \"$\\{{ env.DOCKER_IMAGE_REPO }}:$\\{{ env.DOCKER_IMAGE_TAG }}\" jobs: build_deploy: name: Build Image runs-on: ubuntu-latest timeout-minutes: 30 steps: - name: Checkout code uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v3 ### Image Build Section ### # Build your image, push it # Here is a sample, you can replace this with your registry specific steps. # The registry here should be also be linked in Integrations on TrueFoundry # Please see https://github.com/docker/login-action?tab=readme-ov-file#usage for examples name: Login to Docker Hub uses: docker/login-action@v3 with: registry: $\\{{ env.DOCKER_REGISTRY }} username: $\\{{ secrets.DOCKER_REGISTRY_USERNAME }} password: $\\{{ secrets.DOCKER_REGISTRY_PASSWORD }} - name: Build and push image uses: docker/build-push-action@v5 with: platforms: linux/amd64 context: . push: true tags: $\\{{ env.DOCKER_IMAGE_URI }} cache-from: type=registry,ref=$\\{{ env.DOCKER_IMAGE_REPO }}:buildcache cache-to: mode=max,image-manifest=true,type=registry,ref=$\\{{ env.DOCKER_IMAGE_REPO }}:buildcache ############################ - name: Set up Python uses: actions/setup-python@v4 with: python-version: 3.11 - name: Install dependencies run: | pip install \"truefoundry<1.0.0\" - name: Deploy to workspace run: | tfy patch-application --application-fqn $\\{{ env.APPLICATION_FQN }} --patch='{\"image\": {\"image_uri\": \"$\\{{ env.DOCKER_IMAGE_URI }}\"}}' ``` Note There must be a file named: cicd-providers.yaml in the CI/CD templates. You can use this as reference to create a CI/CD providers file: All the keys must be file names of format *.yaml You must define all the templates you need in the truefoundry values. Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/applying-custom-policies": "Applying Custom Policies Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Applying Custom Policies All Pages Start typing to search\u2026 Applying Custom Policies The Policy Engine lets users define custom validation and modification rules using TypeScript. It helps ensure compliance, security, and consistency by enforcing rules and dynamically updating manifests before deployment. Validation Policies Validation policies ensure that only compliant configurations are deployed. These policies evaluate Truefoundry manifests and prevent deployments that do not meet specific conditions. Some common use cases include: Enforcing readiness and liveness probes for all production services. Enforcing spot instance on dev environment for cost savings. Enforcing auto shutdown for all dev services. Here is a sample policy which enforces auto-shutdown for all services deployed in workspace with env name \"dev\" JavaScript import { ValidationInput, ValidationError } from '@src/types'; export function validate(validationInput: ValidationInput): void { const { manifest, context } = validationInput; const envName = context.envName; if(manifest.type !== 'service') return; if (envName === 'dev') { if (!manifest.auto_shutdown) { throw new ValidationError( 'Auto shutdown is required for the dev environment.' ); } } } Mutation Policies Mutation policies let you automatically modify Kubernetes manifests before they're applied to the cluster. You can define custom rules by writing code to change these manifests based on your needs\u2014no need to use Kustomize. Common use cases include: Setting node affinity for certain workloads like SSH servers or notebooks Adding default secrets, volume mounts, or environment variables to services and jobs Updating image prefixes to match internal repository setups These policies run one after another, in a specific order. Policies with lower order values are applied first. Here is a sample mutation policy which mutates the registry for the images to private jfrog repository. import { MutationInput, MutationOutput } from '@src/types'; export function mutate(mutationInput: MutationInput): MutationOutput { const { generatedK8sManifests } = mutationInput; if (mutationInput.context.inputManifest.type !== 'service') { return { generatedK8sManifests }; } if (generatedK8sManifests) { for (const manifest of generatedK8sManifests) { if ( manifest.kind === 'Deployment' ) { manifest.spec.template.spec.containers.forEach((container: any) => { if (container.image.startsWith('tfy.jfrog.io')) { container.image = container.image.replace( 'tfy.jfrog.io', 'private.tfy.jfrog.io' ); } }); } } } return { generatedK8sManifests }; } Both validation and mutation policies are executed in a sandbox environment before deployment to ensure secure and isolated execution. Creating a policy Policies must be written in TypeScript. To write and test your policy code, refer to this repository: https://github.com/truefoundry/tfy-typescript-policy Write your policy logic in src/policy.ts . Import required models from src/models.ts . Use type definitions from src/types.ts to ensure type-safe code. Test your policy locally using: npx ts-node local_run.ts You can find policy code examples for common use cases in this folder: https://github.com/truefoundry/tfy-typescript-policy/tree/main/examples Note: When creating/updating your policy, only provide the contents of policy.ts in the Policy Code input field. Registering Your Policy on Truefoundry After writing your policy code, you can register your policy on Truefoundry by following these steps: 1. Enter Policy Details Provide the name and description of the policy. 2. Select Policy Action Validate : Ensures manifests meet certain conditions. Mutate : Modifies manifests before applying them. Mutate policies have an order associated that determines their execution order. 3. Choose Policy Mode Audit : Logs policy executions but does not block deployments. Ideally when creating a policy, users should put it in Enforce : Blocks deployments if the policy evaluation fails. Disabled : The policy is ignored. 4. TypeScript Code: Paste the typescript code that you wrote in previous step. 5. Define Entities Specify the resource types the policy should apply to. (service, job, ssh-server etc) 6. Set Filters Use Clusters, Environments, and Workspaces to filter applicable manifests. [by default it applies to all manifests] Filters expect the name (not FQN) of workspaces, environments, and clusters. Here is how it looks on UI: You can find all Policies registered and see their specific runs: You can also check the diff for a mutation policy also: Updated 22 days ago",
    "https://docs.truefoundry.com/docs/integrations": "Integrations Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integrations All Pages Start typing to search\u2026 Integrations TrueFoundry supports integration with different cloud providers like AWS, GCP, Azure; along with other integration providers like OpenAI, DockerHub, JFrog, Github, Bitbucket, Gitlab and many more. You can find the details of each integrations on the following pages: AWS - Blob Storage (S3), Docker Registry (ECR), Secret Store (SSM), Cluster Integration (EKS) Read more GCP - Blob Storage (GCS), Docker Registry (GCR), Secret Store (GSM), Cluster Integration (GKE) Read more Azure - Blob Storage (ABS), Docker Registry (ACR), Cluster Integration (AKS) Read more Git - Github, Bitbucket, Gitlab Read more Others - DockerHub (Docker Registry), OpenAI(LLMs), JFrog (Docker Registry), TTL (Docker Registry), Quay(Docker Registry) Read more Updated 5 months ago What\u2019s Next Cloud Integration",
    "https://docs.truefoundry.com/docs/integration-provider-aws": "Integration Provider - AWS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integration Provider - AWS All Pages Start typing to search\u2026 Integration Provider - AWS TrueFoundry supports integrating with multiple AWS services like S3, ECR, SSM, EKS etc. To integrate any of the above services, you simply need to add your AWS account as a provider account and add integrations for the same as shown below: Share access with users, teams or everyone in your TrueFoundry account As shown in the previous slides, you can share access of each integration with users, teams or everyone in your TrueFoundry account. This would allow them to view and use the integration. Only tenant-admins can edit the integrations. Generate Access Key or Assumed Role You might have the IAM role for TrueFoundry already created with the name - tfy-<short-region-name>-<name>-platform-role-<xxxyyyzzz> , if not then create a new one as explained below. You can add the required permissions to that role. Please refer to the next sections for the permissions required for each integration. You can also create a user with the required permissions, generate an access key and secret key to add integration. Create an IAM role with assume role The role should have following trust policy added. json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::416964291864:role/tfy-ctl-euwe1-production-truefoundry-deps\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } To create this IAM role, you must first save this trust policy in your local system as a JSON file. You can then use the following commands to create the role. Shell aws iam create-role --role-name {{tfy-<short-region-name>-<name>-platform-role-<xxxyyyzzz>}} --assume-role-policy-document {{JSON_FILE_FULL_PATH}} Policies required for S3 Integration [Pre-requisite] Create a S3 Bucket with following config Make sure the bucket has lifecycle configuration to abort multipart upload set for 7 days. Make sure CORS is applied on the bucket with the below configuration: JSON [ { \"AllowedHeaders\": [ \"*\" ], \"AllowedMethods\": [ \"GET\", \"POST\", \"PUT\" ], \"AllowedOrigins\": [ \"*\" ], \"ExposeHeaders\": [ \"ETag\" ], \"MaxAgeSeconds\": 3000 } ] Required Policies JSON { \"Sid\": \"S3\", \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<YOUR_S3_BUCKET_NAME>\", \"arn:aws:s3:::<YOUR_S3_BUCKET_NAME>/*\" ] } Policies required for ECR Integration JSON [ { \"Sid\": \"ECR\", \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetRegistryPolicy\", \"ecr:DescribeImageScanFindings\", \"ecr:GetLifecyclePolicyPreview\", \"ecr:CreateRepository\", \"ecr:GetDownloadUrlForLayer\", \"ecr:DescribeImageReplicationStatus\", \"ecr:ListTagsForResource\", \"ecr:BatchGetRepositoryScanningConfiguration\", \"ecr:GetRegistryScanningConfiguration\", \"ecr:PutImage\", \"ecr:BatchGetImage\", \"ecr:DescribeRepositories\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetRepositoryPolicy\", \"ecr:GetLifecyclePolicy\", \"ecr:ListImages\", \"ecr:InitiateLayerUpload\", \"ecr:CompleteLayerUpload\", \"ecr:DescribeImages\", \"ecr:DeleteRepository\", \"ecr:UploadLayerPart\" ], \"Resource\": [ \"arn:aws:ecr:AWS_REGION:ACCOUNT_ID:repository/tfy-*\" ] }, { \"Sid\": \"ECR\", \"Effect\": \"Allow\", \"Action\": [ \"ecr:DescribeRegistry\", \"ecr:GetAuthorizationToken\", \"sts:GetServiceBearerToken\" ], \"Resource\": [ \"*\" ] } ] Policies required for SSM Integration JSON { \"Sid\": \"SSM\", \"Effect\": \"Allow\", \"Action\": [ \"ssm:GetParameter\", \"ssm:GetParameters\", \"ssm:PutParameter\", \"ssm:DeleteParameter\", \"ssm:DeleteParameters\", \"ssm:GetParameterHistory\" ], \"Resource\": [ \"arn:aws:ssm:AWS_REGION:ACCOUNT_ID:parameter/tfy-secret/*\" ] } Policies required for Bedrock Integration The following policy grants permission to invoke any foundation model available on Bedrock in the us-east-1 region. You can configure Resource list to control which models can be accessed through the LLM Gateway. JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Sid\": \"InvokeAllModels\", \"Action\": [ \"bedrock:InvokeModel\", \"bedrock:InvokeModelWithResponseStream\" ], \"Resource\": [ \"arn:aws:bedrock:us-east-1::foundation-model/*\" ] } ] } Updated 5 months ago",
    "https://docs.truefoundry.com/docs/integration-provider-gcp": "Integration Provider - GCP Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integration Provider - GCP All Pages Start typing to search\u2026 Integration Provider - GCP TrueFoundry supports integrating with multiple GCP services like GCS, GKE, GCR, GSM, GCP Models etc. To integrate any of the above services, you simply need to add your GCP account as a provider account and add integrations for the same as shown below: Share access with users, teams or everyone in your TrueFoundry account As shown in the previous slides, you can share access of each integration with users, teams or everyone in your TrueFoundry account. This would allow them to view and use the integration. Only tenant-admins can edit the integrations. Create a custom serviceaccount Create an IAM serviceaccount named tfy-<short-region-name>-<name>-platform (name can be anything but add a prefix tfy to differentiate it with others), if not created before. Once the IAM serviceaccount is created, make sure to create a key in JSON format. Google Cloud Storage Integration Follow the steps below to connect GCS storage to TrueFoundry: Create a GCP bucket . Make sure to add the lifecycle configurations on the bucket to delete multipart upload after 7 days. For this go to GCP bucket -> Lifecycle -> Add a rule Select Delete multi-part upload for 7 days We also need to add the CORS policy to the GCP bucket. Right now adding the CORS policy to the GCP bucket is not possible through the console so for this, we will use gsutil Create a file called cors.json using the below command Shell cat > cors.json <<EOF [ { \"origin\": [\"*\"], \"method\": [\"GET\", \"POST\", \"PUT\"], \"maxAgeSeconds\": 3600 } ] EOF Attach the above CORS policy to the service account by running the following command using gsutils Shell gsutil cors set cors.json gs://BUCKET_NAME Create a custom IAM role with the following permissions and add to the serviceaccount created above: JSON [ \"storage.objects.create\", \"storage.objects.delete\", \"storage.objects.get\", \"storage.objects.list\", \"storage.objects.update\", \"storage.buckets.create\", \"storage.buckets.get\", \"storage.buckets.list\", \"storage.buckets.create\", \"storage.buckets.update\", \"storage.multipartUploads.create\", \"storage.multipartUploads.list\", \"storage.multipartUploads.listParts\", \"storage.multipartUploads.abort\", \"resourcemanager.projects.get\" ] Add the following IAM condition - resource.name.startsWith('projects/\\_/buckets/<bucket name>}') Navigate to Integrations tab and follow the steps shown the previous demo to integrate your storage. Google Artifact registry Integration Create a custom IAM role with the following permissions and add to the serviceaccount created above: JSON [ \"artifactregistry.dockerimages.get\", \"artifactregistry.dockerimages.list\", \"artifactregistry.locations.get\", \"artifactregistry.locations.list\", \"artifactregistry.repositories.get\", \"artifactregistry.repositories.list\", \"artifactregistry.repositories.create\", \"artifactregistry.repositories.createTagBinding\", \"artifactregistry.repositories.delete\", \"artifactregistry.repositories.deleteArtifacts\", \"artifactregistry.repositories.deleteTagBinding\", \"artifactregistry.repositories.downloadArtifacts\", \"artifactregistry.repositories.get\", \"artifactregistry.repositories.getIamPolicy\", \"artifactregistry.repositories.list\", \"artifactregistry.repositories.listEffectiveTags\", \"artifactregistry.repositories.listTagBindings\", \"artifactregistry.repositories.update\", \"artifactregistry.repositories.uploadArtifacts\", \"artifactregistry.tags.get\", \"artifactregistry.tags.list\", \"artifactregistry.tags.create\", \"artifactregistry.tags.update\", \"artifactregistry.versions.get\", \"artifactregistry.versions.list\", \"artifactregistry.versions.delete\" ] Navigate to Integrations tab and follow the steps shown the previous demo to integrate your Artifact registry. Google Secrets Manager Integration Create a custom IAM role with the following permissions and add to the serviceaccount created above: JSON [ \"secretmanager.secrets.get\", \"secretmanager.secrets.list\", \"secretmanager.secrets.create\", \"secretmanager.secrets.delete\", \"secretmanager.secrets.update\", \"secretmanager.versions.access\", \"secretmanager.versions.list\", \"secretmanager.versions.get\", \"secretmanager.versions.add\", \"secretmanager.versions.destroy\", \"resourcemanager.projects.get\", ] Add the following IAM condition- resource.name.startsWith('projects/<GCP Project Number>/secrets/tfy') Navigate to Integrations tab and follow the steps shown the previous demo to integrate your secret manager. Google GKE cluster Integration Create a custom IAM role with the following permissions and add to the serviceaccount created above: JSON [ \"container.clusters.get\", \"container.clusters.list\", \"container.nodes.get\", \"container.nodes.getStatus\", \"container.nodes.list\", \"resourcemanager.projects.get\", ] Navigate to Integrations tab and follow the steps shown the previous demo to integrate your secret manager. Google Vertex Model Integration Create the GCP Provider Account as described in the demo at the top of this document. Create a custom IAM role with the following permission and add to the serviceaccount created above: JSON [ \"aiplatform.endpoints.predict\" ] Navigate to Integrations tab and edit the GCP Provider Account previously created and add the required models using their model id and they should start showing up in the LLM Gateway. Here's an example of adding gemini-1.5-flash-001 . Updated 5 months ago",
    "https://docs.truefoundry.com/docs/integration-provider-azure": "Integration Provider - Azure Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integration Provider - Azure All Pages Start typing to search\u2026 Integration Provider - Azure TrueFoundry supports integrating with multiple Azure services like ABS, ACR, File Vault, AKS, Azure OpenAI Models, Azure Repos etc. To integrate any of the above services, you simply need to add your Azure account as a provider account and add integrations for the same as shown below: Share access with users, teams or everyone in your TrueFoundry account As shown in the previous slides, you can share access of each integration with users, teams or everyone in your TrueFoundry account. This would allow them to view and use the integration. Only tenant-admins can edit the integrations. Azure Blob Storage(ABS) Integration Follow the steps below to connect your Azure blob storage to TrueFoundry: Create a Azure Storage account in your resource group Instance details - You must Geo-redundant storage to make sure your data is available through other regions in case of region unavailability. Security - Make sure DISABLE Allow enabling anonymous access on individual containers ENABLE Enable storage account key access Network access - ENABLE Allow public access from all networks Recovery - You can keep it to default for 7 days. Create an Azure container inside the above storage account. Search for CORS from the left panel and for Blob service (optional for File service Queue service and Table Service , only apply the change if you are using them) select the below options Allowed Origins - * or your control plane URL Allowed Methods - GET, POST, PUT Allowed Headers - * Exposed Headers - Etag MaxAgeSeconds - 3600 Collect the following information Standard endpoint - Endpoint of the blob storage Once the container is created we need to get the standard endpoint of the blob storage along with the container which will look something like this. Replace this with your storage account name and the container name. https://*mystorageaccount*.blob.core.windows.net/*mycontainer*/ Connection string - From the Azure portal in your storage account, head over to the Security + Networking section under Access keys which will contain the Connection String . Head over to the Integrations tab from the sidebar and follow the steps shown in the previous demo to complete the integration. Azure Container Registry(ACR) Integration To add Azure Container registry, follow the steps below: Create an Azure container registry in your Azure resource group. Connectivity access - Public from (all networks) Collect the following information Your container registry name will be in the format <name>.azurecr.io The username and password for the container registry can be copied from Access keys under Settings in the left panel of the Azure portal in your container registry resource. Enable the Admin User to copy the username and the password Head over to the Integrations tab from the sidebar and follow the steps shown in the previous demo to complete the integration. Azure AKS integration To add support of Azure AKS integration, follow the below steps Create an Azure Ad App registration Create a client secret for the Azure AD application with custom expiration set to a long time. You can do this from the left panel -> Certificates and Secrets. Head over to the AKS cluster and add Reader and Monitoring Reader role to the above Azure AD application. This can be done from Azure portal -> kubernetes services -> AKS cluster -> Access Control. Head over to the Integrations tab from the sidebar and follow the steps shown in the previous demo to complete the integration. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/integrations-git": "Integration Provider - Git Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Integration Provider - Git All Pages Start typing to search\u2026 Integration Provider - Git For integrating git services, navigate to the Integrations section of your truefoundry dashboard, and go to the Git section. In the git integrations, we currently have three types of integrations: Github GitHub is linked by installing the TrueFoundry GitHub app on your repositories. These can then be used with TrueFoundry. We can easily link our Github account in few steps as mentioned below: Click on the Link Github button present in the top right corner of the Github tab for linking your Github account as shown below: Now a pop-up window will appear asking where to install Github app as shown below: Now click on configure where you want to install the app. Then it will ask for permissions from your github account as shown below: Now, by clicking on install button, the app will get installed on your account. You will now be able to see your account on the dashboard as shown below: You can also see the connected repositories of your account by clicking on \u2699\ufe0f button on the right side of that particular account. List of connected repositories You can also unlink your account by unlink button. It can be seen in the above figure. Bitbucket Enable BitBucket Integration in Control Plane To enable Bitbucket integration in your app, you need to create a Bitbucket OAuth Consumer app using your BitBucket account. This Bitbucket app can then be authenticated to access repositories of users' account. Follow these steps to create the Bitbucket OAuth Consumer app and integrate it with truefoundry: Sign in to bitbucker.org . Navigate to Workspace Settings(preferably for your organization). Navigate to OAuth Consumers under Apps and features . Click the button to Add consumer Fill in the following details: Name: This can be anything. Consider something like <Organization>'s Bitbucket or <Your Name>'s Bitbucket or something else descriptive. Redirect URI: https://app.example-org.truefoundry.com/api/svc/v1/vcs/bitbucket/callback (replace app.example-org.truefoundry.com with your control plane URL) Select the following permissions: Account: Email, Read Projects: Read Repositories: Read, Write(to allow adding CI/CD PR) Pull requests: Read, Write(to allow adding CI/CD PR) Apply. Copy the Key and Secret from created app. Set environment variables in servicefoundryServer : BITBUCKET_CLIENT_ID=$ {Key} BITBUCKET_CLIENT_SECRET=$ {Secret} If using CI/CD for control plane deployment follow below: Add these keys and values in the truefoundry-creds k8s secret present in truefoundry namespace. BITBUCKET_CLIENT_ID=$ {Key} BITBUCKET_CLIENT_SECRET=$ {Secret} In the truefoundry application values file, use the value as ${k8s-secret/truefoundry-creds/BITBUCKET_CLIENT_ID} and ${k8s-secret/truefoundry-creds/BITBUCKET_CLIENT_SECRET} Link Bitbucket Bitbucket is linked via OAuth. This will allow use of any repository you have access to. We can easily link our Bitbucket account in few steps as mentioned below: Click on the Link Bitbucket button present in the top right corner of the Bitbucket tab for linking your Bitbucket account as shown below: Your bitbucket account which is signed in to your system will get linked. You will now be able to see your account on the dashboard as shown below: You can also see the connected repositories of your account by clicking on \u2699\ufe0f button on the right side of that particular account. Note : In this connected repositories section you will only see the repositories that you have admin access. Only these repositories can be used to make deployment on the platform. You can also unlink your account by unlink button. It can be seen in the above figure. Gitlab To enable Gitlab integration in your app, you need to create a gitlab app using your gitlab account. This gitlab app can then be authenticated to access repositories of users' account. \u2139\ufe0f Steps to follow if you have cutom gitlab domain. You have to set GITLAB\\_BASE\\_HOST env variable in the tfy build helm chart under the field tfyBuild.truefoundryWorkflows.sfyBuilder.extraEnvs YAML tfyBuild: truefoundryWorkflows: sfyBuilder: version: 0.8.0rc3 extraEnvs: - name: GITLAB_BASE_HOST value: gitlab.truefoundry.com Also you have to set the GITLAB\\_BASE\\_URL as an environment variable in the servicefoundry server, for example env: GITLAB_BASE_URL: \"https://gitlab.truefoundry.com\" Follow these steps to create the gitlab app and integrate it with truefoundry: Sign in to GitLab.com. On the left sidebar, select your avatar and select Edit profile. On the left sidebar, select Applications . Click on the Add new application button right side. Provide the required details for Add new application . Name: This can be anything. Consider something like <Organization>'s GitLab or <Your Name>'s GitLab or something else descriptive. Redirect URI: https://app.example-org.truefoundry.com/api/svc/v1/vcs/gitlab/callback Enable the Confidential check box. Select the following scopes: read_api, read_user, read_repository, write_repository. Select Save application . You should now see an Application ID and Secret . List of connected repositories Set environment variables in servicefoundry server : GITLAB_APP_ID=$ {Application ID} GITLAB_SECRET=$ {Secret} GITLAB_SCOPE=read_api read_user read_repository write_repository Updated 21 days ago",
    "https://docs.truefoundry.com/docs/integration-provider-others": "Other Integrations Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Other Integrations All Pages Start typing to search\u2026 Other Integrations TrueFoundry also supports integrating with a lot more integration providers like DockerHub, OpenAI, JFrog, TTL, Quay, Slack, Email (Custom Provider) etc. To integrate any of the above services, you simply need to add your provider account details and add the available integrations for the same. Let's try adding a Docker Registry from DockerHub to learn how this works: Share access with users, teams or everyone in your TrueFoundry account As shown in the previous slides, you can share access of each integration with users, teams or everyone in your TrueFoundry account. This would allow them to view and use the integration. Only tenant-admins can edit the integrations. Similarly you can integrate with other provider accounts. Updated about 1 month ago",
    "https://docs.truefoundry.com/docs/sso": "SSO Integration Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account SSO Integration All Pages Start typing to search\u2026 SSO Integration TrueFoundry allows your team to utilize an SSO with your Identity Provider (IdP) by leveraging OpenID Connect (OIDC) or Security Assertion Markup Language (SAML). TrueFoundry SSO integration provides a seamless way to sign in with your own IdPs and also eliminates the need for employees to enter credentials to prove their identities repeatedly. Truefoundry can integrate with the following IdP providers: GSuite AzureAD Okta Keycloak If you don't see the name of your IdP provider above, there is a high chance your IdP is also supported as long as it support OpenID Connect (OIDC) or SAML protocol. To use SSO with TrueFoundry, you will need: An Identity Provider (IdP) such Okta, OneLogin, Google Workplace, etc to facilitate SSO that supports either OpenID Connect (OIDC) or SAML protocol such as A technical point-of-contact who can provide TrueFoundry with the following SSO configuration information: For OIDC configurations: A customer\u2019s Client ID and Client Secret A customer\u2019s OIDC domain URL where the /.well-known/openid-configuration endpoint is hosted Employee email domain For SAML configurations: Identity Provider Single Sign-On URL Identity Provider Issuer X.509 Certificate (Optional) IDP metadata XML file Depending on if you use OpenID Connect (OIDC) or Security Assertion Markup Language (SAML) you can connect your SSO by following the relevant instructions. Updated 4 months ago",
    "https://docs.truefoundry.com/docs/openid-connect-with-azure-ad": "OpenID Connect with Azure AD Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account OpenID Connect with Azure AD All Pages Start typing to search\u2026 OpenID Connect with Azure AD Configure OpenID Connect with Azure Active Directory/Microsoft Entra ID Once you have completed this configuration you may enable an OpenID Connect \u201cLogin with Azure AD\u201d button for TrueFoundry dashboard. See Microsoft Entra ID - Register An App Quickstart Guide as an additional reference. Register a New Azure Active Directory Application You will first need to login to the Azure Portal . Once logged in, navigate to Azure Active Directory -> App Registrations -> New Registration to create a new Azure Active Directory Application. Here we have configured our application Redirect URI . Use redirect URL value as https://login.truefoundry.com/oauth2/callback . Once the application has been created, note the Application (client) ID and the Directory (tenant) ID . These will be used respectively as the Client Id value and to construct the Issuer value in your TrueFoundry OpenID Connect Identity Provider configuration. Device Code Flow To enable the Device Code login flow, follow the steps below: Navigate to Authentication tab under Manage section in your app registration page Scroll down to Advanced settings and enable Allow public client flows Create a New Azure Active Directory Application Secret Navigate to Azure Active Directory -> App Registrations ->[Your Application] -> Certificates & secrets -> New client secret to create a new Azure Active Directory Application Client Secret. Note the VALUE of the created client secret. This will be used as the Client secret value in your TrueFoundry OpenID Connect Identity Provider configuration. Integrate with TrueFoundry To integrate Azure AD with TrueFoundry, provide the following configuration to the truefoundry team via email: Tenant ID : Tenant or Directory ID of your Azure application. Client ID : Application ID of your Azure application Client Secret : Secret value of client secret created in the above step Updated 5 months ago",
    "https://docs.truefoundry.com/docs/openid-connect-with-okta": "OpenID Connect with Okta Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account OpenID Connect with Okta All Pages Start typing to search\u2026 OpenID Connect with Okta Configure OpenID Connect with Okta Once you have completed this configuration, you may enable an OpenID Connect \"Login with Okta\" button for TrueFoundry dashboard. Prerequisites A TrueFoundry instance running on a publicly accessible URL. An Okta Workforce Identity Cloud Account connected to a business email address. The Okta Verify app if your Okta account is not configured to send SMS messages for 2FA. Okta Configuration Log in to Okta and navigate to the Admin panel. Navigate to Applications -> Applications and click the Create App Integration button. Select OIDC - OpenID Connect and Web Application then click Next . Enter an App integration name and ensure that Authorization Code is checked. Then, supply a Sign-in redirect URI as https://login.truefoundry.com/oauth2/callback . Under the Assignments section, select Skip group assignment for now . Hit Save . Copy the Client ID and Secret into a text file for later use. Under Sign On , navigate to the section OpenID Connect ID Token and change the Issuer to use the Okta URL . Hit Save and copy this URL into a text file Navigate to Directory -> People and click on the username of the user you\u2019d like to authenticate. Then click Assign Applications and hit the Assign button next to the one you created. Then click Save and Go Back and Done . Integrate with TrueFoundry To integrate Azure AD with TrueFoundry, provide the following configuration to the truefoundry team via email: Issuer URL : Issuer of your Okta application. Client ID : Application ID of your Okta application Client Secret : The secret value of the client secret created in the above step Updated 5 months ago",
    "https://docs.truefoundry.com/docs/saml-v2-with-azure-ad": "SAML v2 with Azure AD Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account SAML v2 with Azure AD All Pages Start typing to search\u2026 SAML v2 with Azure AD Configure SAML v2 for Azure AD/Microsoft Entra ID This documentation will guide you in configuring SAML v2 IdP for Azure AD/Microsoft Entra ID. In this case, TrueFoundry will act as Service Provider (SP) to Azure AD (IdP). Functionally, the result will allow you to display a \u201cLogin with Azure AD\u201d button on your TrueFoundry login page and connect via SAML to Azure AD users/applications. Create an Application in Azure If you have already configured an Azure AD Enterprise application, skip this section. If you have not, please follow the brief steps outlined below: From the Azure account portal navigate to Enterprise Applications . At the top of the screen click on New application . Click on Create your own application . Name the application Select the third option - Integrate any other application you don't find in the gallery (Non-gallery) . Click Create Configure Your Azure Application From your application home screen, click on Single sign-on. Select the SAML option. Integrate with TrueFoundry To integrate Azure AD with TrueFoundry, provide the following configuration to truefoundry team via email: Login URL : This value can be obtained from your Azure AD Application as demonstrated below. You will want to copy the Login URL value from Azure AD into this field. Verification key : From the overview of your SAML application in Azure AD, under step three, you should find a Certificate (Base64) to download. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-truefoundry-in-an-air-gapped-environment": "Deploy Truefoundry in an Air-gapped Environment Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Truefoundry in an Air-gapped Environment All Pages Start typing to search\u2026 Deploy Truefoundry in an Air-gapped Environment Overview An air-gapped environment is isolated from internet. This means all the artifacts(helm charts, container images) needed for the platform to work have to be made available locally or in an internal network without access to the internet. We do this by pushing all the images and helm charts to a dedicated OCI compatible container registry which should be accessible from the environment where the cluster is supposed to run. We have provided a list of images and a script to pull and push those images to any other registry. You will also need to provide access to an npm registry with js-yaml package hosted from within the cluster. Requirements In order to setup Truefoundry in an air-gapped environment, the following are needed An OCI compatible registry which should be able to serve the following (you can also reuse the Truefoundry's registry at tfy.jfrog.io/tfy-images for container images and tfy.jfrog.io/tfy-helm for helm charts instead of replicating the images locally) OCI compatible images with a username password or any other authentication mechanism that is suitable OCI compatible helm charts without authentication A secure environment that has access to the internet and image push access to the target registry. This is to push the images to the registry and will be required for initial setup and all upgrades The following should be installed in the environment to run the script Python Docker Helm Kubectl If running on AWS, karpenter needs access to certain APIs which have to be enabled on private VPC endpoints. Detailed instructions are available here Uploading artifacts to a private registry A list of images and helm charts needed by a Truefoundry installation is maintained in infra-chart repo. Here is a list of Truefoundry Helm Charts that can be deployed depending on the environment you wish to deploy Truefoundry into AWS EKS (tfy-k8s-aws-eks-inframold) Azure AKS (tfy-k8s-azure-aks-inframold) Civo Talos (tfy-k8s-civo-talos-inframold) GCP GKE Standard (tfy-k8s-gcp-gke-standard-inframold) Generic Kubernetes Cluster (tfy-k8s-generic-inframold) Push images and helm charts to the Registry To start download the artifacts manifest file of the helm chart from the list above and save in a local file. This file contains all the container images and helm charts needed for a Truefoundry installation Shell wget <link_to_artifacts_manifest> You must login to the target registry using the docker CLI docker login --username <username> --password <password> <target_registry> To pull and push these images to your own registry, we have provided a sample python script here . Steps to prepare the environment - Clone the truefoundry/infra-charts repo Shell git clone https://github.com/truefoundry/infra-charts.git Create a virtual env to run the python script in Shell python3 -m venv venv source venv/bin/activate Install the requirements.txt Shell pip install -r infra-charts/scripts/upload-artifacts/requirements.txt The upload_artifact.py script takes in the following arguments artifact_type - this takes in the artifact type which can be either image or helm file_path - this is the location of the artifacts-manifest.json file that contains the details for all the container images and helm charts that are needed for the installation destination_registry - this is the registry you plan to use in your air-gapped environment Run script with the target container registry details Shell python infra-charts/scripts/upload-artifacts/upload_artifacts.py \\ <artifact_type> artifacts-manifest.json \\ <registry_url> Deploying Inframold Charts Create a namespace for jspolicy Shell kubectl create ns jspolicy If your registry needs to be authenticated before you can pull images, then you have to authenticate the pods to pull the images. Create a secret with the registry auth information Shell kubectl create secret docker-registry regcred \\ --docker-server=<registry_url> \\ --docker-username=<username> \\ --docker-password=<password> \\ -n jspolicy Create a file called jspolicy-values.yaml with the following content yaml image: <registry_url>/loftsh/jspolicy:0.2.2 replicaCount: 2 env: ## Needed if npmjs.org is not accessible npm_config_registry: \"https://tfy.jfrog.io/artifactory/api/npm/tfy-npm-registry-local\" ## only add this section if you need to authenticate against the registry imagePullSecrets: - regcred Install jspolicy helm chart bash helm install jspolicy \\ <registry_url>/loft/jspolicy -n jspolicy --create-namespace -f jspolicy-values.yaml Install the ArgoCD CRDs. These need to be installed first because the JSPolicy resources reference the ArgoCD CRD Shell kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/master/manifests/crds/application-crd.yaml Install tfy-jspolicy-config Chart to patch your images Create a file called jspolicy-config-values.yaml with the following content. Make sure to add the registry url with proper protocol for replaceArgoHelmRepo.registryReplacementMap YAML ## @section replaceImageRegistry Configuration options for replacing the image registry replaceImageRegistry: ## @param replaceImageRegistry.enabled Enable or disable replacing the image registry enabled: true ## @param replaceImageRegistry.excludeNamespaces Namespaces to exclude from replacing the image registry excludeNamespaces: - kube-system - kube-public - kube-node-lease ## @param replaceImageRegistry.includeNamespaces Namespaces to include from replacing the image registry. When non-empty, only these namespaces will be included includeNamespaces: [] ## @param replaceImageRegistry.registryReplacementMap The image registry replacement map ## registryReplacementMap: ## \"docker.io\": \"mydocker.io\" ## \"*\": \"myjrog.io\" registryReplacementMap: \"*\": <registry_url> ## @section replaceArgoHelmRepo Configuration options for replacing the Argo Helm repository replaceArgoHelmRepo: ## @param replaceArgoHelmRepo.enabled Enable or disable replacing the Argo Helm repository enabled: true ## @param replaceArgoHelmRepo.excludeNamespaces Namespaces to exclude from replacing the argo helm repo excludeNamespaces: [] ## @param replaceArgoHelmRepo.includeNamespaces Namespaces to include for replacing the argo helm repo. When non-empty, only these namespaces will be included includeNamespaces: [] ## @param replaceArgoHelmRepo.registryReplacementMap The argo helm repository replacement map. Protocol is mandatory ## registryReplacementMap: ## oci://myargohelm.io: \"oci://tfyargohelm.io\" ## http://myargohelm.io: \"http://tfyargohelm.io\" registryReplacementMap: \"*\": <registry_url> Install the tfy-jspolicy-config chart Shell helm install tfy-jspolicy-config <registry_url>/infra-charts/tfy-jspolicy-config --version 0.1.5 -n jspolicy -f jspolicy-config-values.yaml Now the cluster is ready and you can continue with the appropriate installation steps from here Updated 5 months ago",
    "https://docs.truefoundry.com/docs/migrate-pytorch-sagemaker-endpoints-to-truefoundry-platform": "Migrate Pytorch Sagemaker Endpoints to TrueFoundry Platform Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Migrate Pytorch Sagemaker Endpoints to TrueFoundry Platform All Pages Start typing to search\u2026 Migrate Pytorch Sagemaker Endpoints to TrueFoundry Platform Truefoundry provides a straightforward migration path for Pytorch endpoints already running in Sagemaker using a custom inference script to the Truefoundry platform. We will start with an inference script and follow a workflow to implement the expected changes. Existing Code A Sagemaker deployment typically contains code in the form of the following file tree - code/ \u251c\u2500\u2500 inference.py \u2514\u2500\u2500 requirements.txt inference.py - This is the inference handler that implements the Sagemaker functions like model_fn , input_fn , predict_fn , output_fn etc requirements.txt - This contains any additional Python packages needed by the inference handler Apart from these, there are also - Model artifacts - Generated model files (e.g. model.pth ). These may reside on your S3 buckets. Sagemaker deployment code (e.g. sagemaker_deploy.py ) - Code to call Sagemaker to deploy the model as an endpoint Examples: inference.py Python # defining model and loading weights to it. import json import torch class Net(torch.nn.Module): ... def model_fn(model_dir): model = Net() with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f: model.load_state_dict(torch.load(f)) model.to(device).eval() return model # data preprocessing def input_fn(request_body, request_content_type): assert request_content_type == \"application/json\" data = json.loads(request_body)[\"inputs\"] data = torch.tensor(data, dtype=torch.float32, device=device) return data # inference def predict_fn(input_object, model): with torch.no_grad(): prediction = model(input_object) return prediction # postprocess def output_fn(predictions, content_type): assert content_type == \"application/json\" res = predictions.cpu().numpy().tolist() return json.dumps(res) sagemaker_deploy.py sagemaker_deploy.py from sagemaker.pytorch import PyTorchModel from sagemaker.serializers import JSONSerializer from sagemaker.deserializers import JSONDeserializer pytorch_model = PyTorchModel( model_data = s3_model_uri, role=role, entry_point=<\"inference.py\">, framework_version = \"1.12.1\", py_version='py38', name = <\"model_name\">, ) predictor = pytorch_model.deploy( serializer = JSONSerializer(), deserializer = JSONDeserializer(), endpoint_name = <\"endpoint_name\">, instance_type = \"ml.g4dn.xlarge\", initial_instance_count = 2, ) Migration Workflow Broadly speaking these are the things we shall do - Enclose the inference handler within a Docker container containing torchserve to support pytorch-based models Upload the model artifact as a Truefoundry Artifact to make it accessible from the running container Launch a TrueFoundry deployment utilizing the above two pieces Steps Upload the Pytorch model artifacts to the Truefoundry Model Registry . For reference, your model on disk may look like this model/ \u2514\u2500\u2500 model.pth Add a upload_model.py Create an ML repo ( docs ) and provide access to that repo from the workspace we need to deploy in ( docs ). Change ml_repo arguments Change model_file_or_folder to point to your model folder The code prints a model fqn at the end which we will use in later steps. upload_model.py import mlfoundry client = mlfoundry.get_client() model_version = client.log_model( ml_repo=<\"ml_repo_name\">, name=<\"my-pytorch-model\">, model_file_or_folder=<\"model/\">, # Path to directory containing the model framework=mlfoundry.ModelFramework.PYTORCH, description=\"TorchServe + Sagemaker compatible model artifact\", ) print(\"Model Version FQN:\", model_version.fqn) We will write a Python script called main.py that could launch the torchserve process at startup - main.py import os # We are reusing sagemaker's open source toolkit to manage torchserve from sagemaker_pytorch_serving_container import serving import shutil def main(): home_dir = os.getenv(\"HOME\") # MODEL_DIR is populated by truefoundry artifacts. Model will be downloaded here artifact_model_dir = os.getenv(\"MODEL_DIR\") if not artifact_model_dir: raise ValueError(\"`MODEL_DIR` must be set in environment and point to a model directory\") # Base dir where models are supposed to be present base_dir = os.getenv(\"SAGEMAKER_BASE_DIR\", os.path.join(home_dir, \"model-store\")) model_dir = os.path.join(base_dir, \"model\") # Copying over the model artifacts to the model dir shutil.copytree(src=artifact_model_dir, dst=model_dir) # Copying over the code to model dir shutil.copytree(src=os.path.join(home_dir, \"code\"), dst=os.path.join(model_dir, \"code\")) # Launching the torchserve process serving.main() if __name__ == '__main__': main() Next, we'll write a Dockerfile that can create the Truefoundry application Dockerfile # We pick a gpu enabled torchserve image as base. Others can be found here - https://hub.docker.com/r/pytorch/torchserve/tags FROM --platform=linux/amd64 pytorch/torchserve:0.9.0-gpu ENV SAGEMAKER_BASE_DIR=/home/model-server/model-store RUN pip install -U pip setuptools wheel && \\ pip install --no-cache-dir \\ sagemaker-pytorch-inference==2.0.22 \\ scikit-learn \\ pandas \\ scipy==1.10.1 USER root RUN touch /etc/sagemaker-ts.properties && \\ chown model-server:model-server /etc/sagemaker-ts.properties USER model-server WORKDIR /home/model-server/ # Assuming code is the directory where inference code and requirements.txt is present COPY code ./code COPY main.py ./ EXPOSE 8080 Now let's go ahead and write a deploy.py script that can be used with Truefoundry to get a service deployed. Here you'll need to change the following Service Name - Name for the service we'll deploy Entrypoint Script Name (value for SAGEMAKER_PROGRAM ) - The code file name containing model_fn , input_fn , predict_fn and output_fn Model Version FQN - The FQN obtained from upload_model.py deploy.py import argparse import logging from typing import Optional from servicefoundry import ( Service, Build, LocalSource, DockerFileBuild, Port, ArtifactsDownload, TruefoundryArtifactSource, ArtifactsCacheVolume, Resources, GPUType, NvidiaGPU, ) logging.basicConfig(level=logging.INFO, format=logging.BASIC_FORMAT) def main(): service = Service( name=<\"service_name\">, image=Build( build_source=LocalSource(local_build=False), build_spec=DockerFileBuild( dockerfile_path=\"./Dockerfile\", command=\"python main.py\" ) ), ports=[Port(port=8080, host=<\"host.app.example.com\">, path=<\"/\">)], env={ # This should be the `entry_point` argument, the code file containing model_fn, predict_fn, etc \"SAGEMAKER_PROGRAM\": <\"inference.py\"> }, artifacts_download=ArtifactsDownload( artifacts=[ TruefoundryArtifactSource( # This should be the model version fqn obtained by running `upload_model.py` artifact_version_fqn=<\"model_version_fqn\"> download_path_env_variable=\"MODEL_DIR\", ) ], ), resources=Resources( cpu_request=1, cpu_limit=4, memory_request=8000, memory_limit=16000, ephemeral_storage_request=10000, ephemeral_storage_limit=16000, devices=[ NvidiaGPU(name=GPUType.T4, count=1) ] ), liveness_probe=HealthProbe( config=HttpProbe(path=\"/ping\", port=8080), initial_delay_seconds=30, period_seconds=10, timeout_seconds=1, success_threshold=1, failure_threshold=5, ), readiness_probe=HealthProbe( config=HttpProbe(path=\"/ping\", port=8080), initial_delay_seconds=30, period_seconds=10, timeout_seconds=1, success_threshold=1, failure_threshold=5, ), ) service.deploy(workspace_fqn=<\"workspace_fqn\">, wait=False) if __name__ == '__main__': main() Deploy using servicefoundry Shell $ python deploy.py Once the deployment has gone through, it can be tested using this script - test_endpoint.py if __name__ == \"__main__\": # The URL of the endpoint you're sending the request to url = 'https://<host.app.example.com>/predictions/model' # Your JSON payload data = {\"inputs\": <serialized_input>} # Send the POST request with the JSON payload response = requests.post(url, json=data) # Check the response if response.status_code == 200: print(\"Request successful.\") # Process response data if needed print(response.json()) else: print(\"Request failed.\", response.status_code) print(response.text) Updated 5 months ago",
    "https://docs.truefoundry.com/docs/create-custom-k8s-objects": "Create custom K8s objects Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Create custom K8s objects All Pages Start typing to search\u2026 Create custom K8s objects This guide covers creating custom k8s objects like service accounts, secrets, config maps, etc. in your workspace. Open New Deployment Form. Select Helm from the left navbar and then select your workspace Fill out the form with the following input: Name: a readable name for the group of objects Helm repository URL: https://truefoundry.github.io/infra-charts/ Chart name: tfy-manifests-template Version: 0.2.0 Add the objects in values as an array of manifests. E.g. manifests: - kind: ServiceAccount apiVersion: v1 metadata: name: test-sa namespace: workspace-2 Note: the k8s resource can be created in any namespace by overwriting in the spec of the k8s object. If not mentioned, it will use the selected workspace by default. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/model-deployment": "Model Deployment Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Model Deployment All Pages Start typing to search\u2026 Model Deployment For some model frameworks, Truefoundry can generate a model deployment package containing the following: Inference code wrapped around a general-purpose web framework like FastAPI or more specialized model servers like Triton. A requirements.txt. We also generate a Dockerfile for model servers like Triton, which are complex to set up on all operating systems. A README file that contains instructions on testing locally and deploying on Truefoundry. This approach gives you the flexibility to: Deploy the package as it is to Truefoundry or anywhere else. Change the inference code to add custom business logic and dependencies. Test the code locally. Maintain the generated code in your version control system (Github, Gitlab, etc.). To create the deployment package: Locate the model you want to deploy in the model registry and click the Deploy button. Select a workspace for deployment, and copy the command. Execute the command in your terminal to generate the model deployment package. shell \u276f tfy deploy-init model --name 'my-sklearn-model-1' --model-version-fqn 'model:truefoundry/my-classification-project/my-sklearn-model-1:1' --workspace-fqn 'tfy-usea1-devtest:deb-ws' --model-server 'fastapi' ... Generating application code for 'model:truefoundry/my-classification-project/my-sklearn-model-1:1' Model Server code initialized successfully! Code Location: /work/model-deployment/my-sklearn-model-1 Next Steps: - Navigate to the model server directory: cd /work/model-deployment/my-sklearn-model-1 - Refer to the README file in the directory for further instructions. \u276f cd /work/model-deployment/my-sklearn-model-1 \u276f ls README.md deploy.py infer.py requirements.txt server.py Follow the instructions present on the README.md . Updated 4 months ago",
    "https://docs.truefoundry.com/docs/scikit-learn": "Scikit Learn Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Scikit Learn All Pages Start typing to search\u2026 Scikit Learn Logging and Deploying Sklearn Models in Truefoundry We will need to know some information about the model you are logging to generate a deployment package. To load the model: The serialization format ( joblib , cloudpickle , etc.) and the model file name. To generate the inference script and wrap it around a model server: The inference method name ( predict , predict_proba , etc). The input and output schema of the inference method. To deploy and run: Python version along with pip package (numpy, scikit-learn) dependencies. Log a deployable Sklearn Model Below is an example of logging a model trained using Scikit-learn: Python from truefoundry.ml import get_client, SklearnFramework, sklearn_infer_schema import joblib import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC # Define training data X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) # Create and train the model clf = make_pipeline(StandardScaler(), SVC(gamma=\"auto\")) model = clf.fit(X, y) # Save the model joblib.dump(clf, \"sklearn-pipeline.joblib\") # Initialize the Truefoundry client client = get_client() # Infer model schema model_schema = sklearn_infer_schema( model_input=X, model=model, infer_method_name=\"predict\" ) # Log the model model_version = client.log_model( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", model_file_or_folder=\"sklearn-pipeline.joblib\", # To make the model deployable and generate the inference script, model file, and schema(with the method name) are required. framework=SklearnFramework( model_filepath=\"sklearn-pipeline.joblib\", model_schema=model_schema, ), # Auto-captures the current environment details (e.g., python_version, pip_packages) if not provided, based on the framework. ) # Output the model's Fully Qualified Name (FQN) print(f\"Model version logged successfully: {model_version.fqn}\") View and manage recently logged models in the ML Repos. Access framework details like serialization format, model schema, and inference method. Access environment details like the Python version and pip packages list required for a specific model version. Deploy the model Once the model is deployable, you can start the deployment flow directly using the CLI. Navigate to the Model Registry Locate the desired model in the list and click on the Deploy button Select the workspace for deployment, then click the copy icon to use the generated CLI command and initialize the model deployment package. Common Model Deployment Issues and Troubleshooting Guide Fix for Incomplete Model Manifest and make an existing logged model deployable \u2755 Deploying a logged model may fail due to an incomplete model manifest, causing errors like: - Model framework is not supported for deployment - Model filename not found, please save model filename while logging the model - Model schema not found, please save schema while logging the model - Serialization format not found, please save serialization format while logging the model Here\u2019s an example code snippet to resolve the Incomplete Model Manifest by adding the required fields and updating the model version: Python from truefoundry.ml import get_client, ModelVersionEnvironment, SklearnFramework, sklearn_infer_schema import joblib import numpy as np # Replace with your model version FQN model_version_fqn = \"model:truefoundry/my-classification-project/my-sklearn-model:1\" client = get_client() model_version = client.get_model_version_by_fqn(model_version_fqn) model_version.download(path=\".\") # Replace with your model file path model_file_path = \"./sklearn-pipeline.joblib\" model = joblib.load(model_file_path) # Update the model input example as per your model X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) model_schema = sklearn_infer_schema(model_input=X, model=model, infer_method_name=\"predict\") # To make the model deployable and generate the inference script, model file, and schema(with the method name) are required. model_version.framework = SklearnFramework( model_filepath=\"sklearn-pipeline.joblib\", serialization_format=\"joblib\", model_schema=model_schema, ) model_version.environment = ModelVersionEnvironment( python_version=\"3.11\", pip_packages=[ \"joblib==1.4.2\", \"numpy==1.26.4\", \"pandas==2.1.4\", \"scikit-learn==1.5.2\", ], ) model_version.update() Python version < 3.8 and > 3.12 is not supported for Triton deployment The Triton deployment depends on the nvidia-pytriton library ( https://pypi.org/project/nvidia-pytriton ), which supports Python versions >=3.8 and <=3.12. If you need to use a version outside this range, consider FastAPI as an alternative framework for serving the model. Numpy version must be specified for Triton deployment , Numpy version must be less than 2.0.0 for Triton deployment The nvidia-pytriton library does not support numpy versions >=2.0. If you need to use a version outside this range, consider FastAPI as an alternative framework for serving the model. Updated 4 months ago",
    "https://docs.truefoundry.com/docs/xgboost": "XGBoost Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account XGBoost All Pages Start typing to search\u2026 XGBoost Logging and Deploying XGBoost Models in Truefoundry We will need to know some information about the model you are logging to generate a deployment package. To load the model: The serialization format ( joblib , cloudpickle , pickle or json ) and the model file name. To generate the inference script and wrap it around a model server: The input and output schema of the inference method. NOTE For XGBoost models we only support predict inference method name as of now. To deploy and run: Python version along with pip package (numpy, xgboost) dependencies. Log a deployable XGBoost Model Below is an example of logging a model trained using XGBoost: Python from truefoundry.ml import get_client, XGBoostFramework, xgboost_infer_schema import joblib import os import numpy as np from xgboost import XGBClassifier X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([0, 0, 1, 1]) clf = XGBClassifier() clf.fit(X, y) name = \"my-xgboost-model\" LOCAL_MODEL_DIR = f\"{name}/\" model_file_name = \"xgboost-model.joblib\" model_file_path = f\"{name}/{model_file_name}\" os.makedirs(LOCAL_MODEL_DIR, exist_ok=True) joblib.dump(clf, model_file_path) client = get_client() model = joblib.load(model_file_path) model_schema = xgboost_infer_schema( model_input=X, model=model, ) model_version = client.log_model( ml_repo=\"project-classification\", name=\"my-xgboost-model\", description=\"A simple xgboost model\", model_file_or_folder=model_file_path, framework=XGBoostFramework( model_filepath=model_file_name, serialization_format=\"joblib\", model_schema=model_schema, ), ) View and manage recently logged models in the ML Repos. Access framework details like serialization format, model schema, and inference method. Access environment details like the Python version and pip packages list required for a specific model version. Deploy the model Once the model is deployable, you can start the deployment flow directly using the CLI. Navigate to the Model Registry Locate the desired model in the list and click on the Deploy button Select the workspace for deployment, then click the copy icon to use the generated CLI command and initialize the model deployment package. Common Model Deployment Issues and Troubleshooting Guide Fix for Incomplete Model Manifest and make an existing logged model deployable \u2755 Deploying a logged model may fail due to an incomplete model manifest, causing errors like: - Model framework is not supported for deployment - Model filename not found, please save model filename while logging the model - Model schema not found, please save schema while logging the model - Serialization format not found, please save serialization format while logging the model Here\u2019s an example code snippet to resolve the Incomplete Model Manifest by adding the required fields and updating the model version: Python from truefoundry.ml import get_client, ModelVersionEnvironment, XGBoostFramework, xgboost_infer_schema import joblib import numpy as np # Replace with your model version FQN model_version_fqn = \"model:truefoundry/project-classification/my-xgboost-model:1\" client = get_client() model_version = client.get_model_version_by_fqn(model_version_fqn) model_version.download(path=\".\") # Replace with your model file path model_file_path = \"./xgboost-model.joblib\" model = joblib.load(model_file_path) # Update the model input example as per your model X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) model_schema = xgboost_infer_schema(model_input=X, model=model) # To make the model deployable and generate the inference script, model file, and schema(with the method name) are required. model_version.framework = XGBoostFramework( model_filepath=\"xgboost-model.joblib\", serialization_format=\"joblib\", model_schema=model_schema, ) model_version.environment = ModelVersionEnvironment( python_version=\"3.11\", pip_packages=[ \"joblib==1.4.2\", \"numpy==1.26.4\", \"pandas==2.1.4\", \"xgboost==2.1.3\", ], ) model_version.update() Python version < 3.8 and > 3.12 is not supported for Triton deployment The Triton deployment depends on the nvidia-pytriton library ( https://pypi.org/project/nvidia-pytriton/ ) which supports Python versions >=3.8 and <=3.12. If you need to use a version outside this range, consider using FastAPI as an alternative framework for serving the model. Numpy version must be specified for Triton deployment , Numpy version must be less than 2.0.0 for Triton deployment The nvidia-pytriton library specifies in its pyproject.toml file that it does not support numpy versions >=2.0. This limitation has been confirmed through practical experience. If you need to use a version outside this range, consider using FastAPI as an alternative framework for serving the model. Updated 4 months ago",
    "https://docs.truefoundry.com/docs/setup-gitops-using-truefoundry": "Setup Gitops using Truefoundry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Setup Gitops using Truefoundry All Pages Start typing to search\u2026 Setup Gitops using Truefoundry Store deployment configuration in Git and implement a approval process for changes GitOps is a method of managing and deploying software applications using a version control system like Git. Think of it as a way to automate and streamline the process of updating and maintaining your applications. The key advantages of using Gitops - specially for production environments are the following: Source of Truth : In GitOps, Git repositories serve as the single source of truth for your infrastructure and application configurations. Everything needed to run your app is stored as code in these repositories. Version Control : Since everything is in Git, you have a complete history of changes. This makes it easy to track who made changes, revert to previous versions if needed, and collaborate with others. Automation : GitOps uses automation tools to continuously monitor your Git repositories. When changes are detected, these tools automatically update your applications and infrastructure to match the new configuration. This ensures that what you have in your Git repository is exactly what is running in your environment. Consistency and Reliability : By using Git as the source of truth and automating deployments, GitOps ensures that your environments are consistent and reduces the chance of human error, making deployments more reliable. Collaboration : Teams can collaborate more effectively since they can propose changes through pull requests, review code, and discuss potential impacts before deploying. Truefoundry provides first-class support for implementing Gitops and also provides a ready-to-use template using which you can implement Gitops using Github actions in your organization under 10 mins. This repository: https://github.com/truefoundry/truefoundry-gitops-sample-repository contains the entire template . It primarily comprises of the following: Folder structure for placing your YAML configuration The folder structure is as follows: clusters/ \u251c\u2500\u2500 cluster1/ \u2502 \u251c\u2500\u2500 cluster1.yaml \u2502 \u2514\u2500\u2500 workspaces/ \u2502 \u2514\u2500\u2500 workspace1/ \u2502 \u251c\u2500\u2500 workspace1.yaml \u2502 \u2514\u2500\u2500 applications/ \u2502 \u2514\u2500\u2500 app1.yaml \u2514\u2500\u2500 cluster2/ \u251c\u2500\u2500 cluster2.yaml \u2514\u2500\u2500 workspaces/ \u2514\u2500\u2500 workspace1/ \u251c\u2500\u2500 workspace1.yaml \u2514\u2500\u2500 applications/ \u2514\u2500\u2500 sample-app.yaml The spec for the cluster / workspace / application can be copied from the UI by clicking on Edit -> Spec. Github actions for validating and applying the configuration The github actions files are present in .github/workflows: https://github.com/truefoundry/truefoundry-gitops-sample-repository/tree/main/.github/workflows . The template primarily implements two things: dry_run_on_pr.yaml - This file runs on creation or updating of any pull request. It validates the spec for all the yaml files changed in the PR. For validation, it performs the following checks: Check if the changed file is a valid yaml file Check if the changed file name is same as the name field in the yaml file Check the spec if valid using tfy apply --dry-run apply_on_merge.yaml - This file runs tfy apply on all the files that have changed in the PR. It also runs delete on the files that have been deleted. Updated 3 months ago",
    "https://docs.truefoundry.com/docs/using-images-from-ngc-container-registry": "Using Images from NVIDIA NGC Container Registry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using Images from NVIDIA NGC Container Registry All Pages Start typing to search\u2026 Using Images from NVIDIA NGC Container Registry Nvidia Container Registry (nvcr.io) Create a NGC Personal Token Sign up at https://ngc.nvidia.com/ Generate a Personal Key from https://org.ngc.nvidia.com/setup/personal-keys Add nvcr.io as Custom Docker Registry Under Integrations Tab, Click +Add Integration Provider on top right Under Integrations, select Custom Docker Registry and enter as follows: Registry URL: nvcr.io Username: $oauthtoken Password: Enter the Personal Token you created earlier Save Use the Integration - E.g. Deploying Nvidia NIM Container \ud83d\udcd8 Save the Personal Access Token as a Secret We recommend saving the generated token as a Secret on the platform to be able to use it for other purposes We can now deploy a Nvidia NIM LLM Container for Inference. You can find the list of all Supported Models from the docs page We will pick the Llama 3.1 8B Instruct model as an example. From the list of models page, click the NGC Catalog link From the Container page, copy the image tag Next, Start a new Service deployment on TrueFoundry In the Image Section, add the Image URI we copied from NGC Page Select the nvcr Docker Registry we added earlier Enter 8000 for port Select a GPU Optionally add Environment Variables (See Configuring NIM docs page) Submit Here is the full spec for reference for 2 x Nvidia T4 truefoundry.yaml name: nim-llama31-8b-ins-v03 type: service image: type: image image_uri: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3 docker_registry: tenant:custom:nvcr:docker-registry:nvcr-truefoundry ports: - host: <your-host> port: 8000 expose: true protocol: TCP app_protocol: http env: NGC_API_KEY: tfy-secret://tenant:secret-group:NGC_API_KEY NIM_LOG_LEVEL: DEFAULT NIM_SERVER_PORT: '8000' NIM_JSONL_LOGGING: '1' NIM_MAX_MODEL_LEN: '4096' NIM_MODEL_PROFILE: vllm-bf16-tp2 NIM_LOW_MEMORY_MODE: '1' NIM_SERVED_MODEL_NAME: llm NIM_TRUST_CUSTOM_CODE: '1' NIM_ENABLE_KV_CACHE_REUSE: '1' NIM_CACHE_PATH: /opt/nim/.cache labels: tfy_model_server: vLLM tfy_openapi_path: openapi.json tfy_sticky_session_header_name: x-truefoundry-sticky-session-id replicas: 1 resources: node: type: node_selector capacity_type: on_demand devices: - name: T4 type: nvidia_gpu count: 2 cpu_limit: 8 cpu_request: 6 memory_limit: 32000 memory_request: 27200 shared_memory_size: 24000 ephemeral_storage_limit: 100000 ephemeral_storage_request: 20000 workspace_fqn: <your-workspace-fqn> readiness_probe: config: path: /v1/health/ready port: 8000 type: http period_seconds: 10 timeout_seconds: 1 failure_threshold: 3 success_threshold: 1 initial_delay_seconds: 0 allow_interception: false Once Deployed and ready, you can visit /docs route on the endpoint to try it out Model Caching using a Volume To ensure fast startup , you can Create a Read Write Many Volume in the same workspace and mount the volume at /opt/nim/.cache (the value of NIM_CACHE_PATH environment variable) to cache the model weights. Updated 2 months ago",
    "https://docs.truefoundry.com/docs/using-truefoundry-secrets-in-integration": "Using TrueFoundry Secrets in Integrations Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Using TrueFoundry Secrets in Integrations All Pages Start typing to search\u2026 Using TrueFoundry Secrets in Integrations You can securely use secrets like API keys in your integrations via TrueFoundry. Here's how to set up and use a secret for OpenAI integration 1. Create a Secret in TrueFoundry Follow the instructions in the TrueFoundry Secrets Guide to create a new secret. Once created, copy the Fully Qualified Name (FQN) of the secret (e.g., tfy-secret://truefoundry:openai-group:API_KEY ). 2. Reference the Secret in Your Integration When configuring your integration (like OpenAI), use the FQN instead of the raw key. Using Truefoundry secret FQN instead of raw value for API key 3. Deploy the Integration Deploy using UI or CLI: bash tfy apply -f openai-using-secret.yaml Example yaml configuration for the integration Updated 6 days ago",
    "https://docs.truefoundry.com/docs/service": "Service Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Service All Pages Start typing to search\u2026 Service \ud83d\udcd8 Note While using TrueFoundry python SDK type is not a required field in any of the imported classes Service Description Describes the configuration for the service Schema JSON { \"type\": \"string\", \"replicas\": 1, \"allow_interception\": false, \"rollout_strategy\": {} } Properties Name Type Required Description type string false + value=service replicas any false Replicas of service you want to run allow_interception boolean false Whether to allow intercepts to be applied for this service. This would inject an additional sidecar in each pod of the service. Not recommended on production rollout_strategy object false Strategy to dictate how a rollout should happen when a new release for this service is made Liveliness/Readiness Probe The modules below help configuring the Health Probes for the Service. Learn more about Health Probes here . HttpProbe Description Describes the Instructions for assessing container health by executing an HTTP GET request. To learn more you can go here Schema JSON { \"type\": \"string\", \"path\": \"string\", \"port\": 65535, \"host\": \"string\", \"scheme\": \"HTTP\" } Properties Name Type Required Description type string true +sort=1 +value=http path string true The endpoint, relative to the port, to which the HTTP GET request should be directed. port integer true The TCP socket within the container to which the HTTP GET request should be directed. host string false Host name to connect to, defaults to the pod IP scheme string false Scheme to use for connecting to the host HealthProbe Description Describes the configuration for the Health Probe's To learn more you can go here Schema JSON { \"config\": { \"type\": \"string\", \"path\": \"string\", \"port\": 65535, \"host\": \"string\", \"scheme\": \"HTTP\" }, \"initial_delay_seconds\": 0, \"period_seconds\": 10, \"timeout_seconds\": 1, \"success_threshold\": 1, \"failure_threshold\": 3 } Properties Name Type Required Description config HttpProbe true Describes the Instructions for assessing container health by executing an HTTP GET request. To learn more you can go here initial_delay_seconds integer false Number of seconds after the container is started before the first probe is initiated. period_seconds integer false How often, in seconds, to execute the probe. timeout_seconds integer false Number of seconds after which the probe times out. success_threshold integer false Minimum consecutive successes for the probe to be considered successful after having failed. failure_threshold integer false Number of consecutive failures required to determine the container is not alive (liveness probe) or not ready (readiness probe). Port Port Description Describes the ports the service should be exposed to. Schema JSON { \"port\": 80, \"protocol\": \"TCP\", \"expose\": true, \"app_protocol\": \"http\", \"host\": \"string\", \"path\": \"string\", \"rewrite_path_to\": \"string\", \"auth\": { \"type\": \"string\", \"username\": \"string\", \"password\": \"string\" } } Properties Name Type Required Description port integer true Port number to expose. protocol string true Protocol for the port. expose boolean true Expose the port app_protocol string false Application Protocol for the port. Select the application protocol used by your service. For most use cases, this should be http (HTTP/1.1). If you are running a gRPC server, select the grpc option. This is only applicable if expose=true . host string false Host e.g. ai.example.com, app.truefoundry.com path string false Path e.g. /v1/api/ml/, /v2/docs/ rewrite_path_to string false Rewrite the path prefix to a different path. If path is /v1/api and rewrite_path_to is /api . The URI in the HTTP request http://0.0.0.0:8080/v1/api/houses will be rewritten to http://0.0.0.0:8080/api/houses before the request is forwarded your service. Defaults to / . This is only applicable if path is given. auth BasicAuthCreds false Username and password for Basic Service Authentication Enumerate Values Property Value protocol TCP protocol UDP app_protocol http app_protocol grpc app_protocol tcp RolloutStrategy Rolling Description This strategy updates the pods in a rolling fashion such that a subset of the total pods are replaced with new version at one time. A commonly used strategy can be to have maxUnavailablePercentage close to 0 so that there is no downtime and keep the maxSurgePercentage to around 25%. If you are anyways running a large number of pods, the service can often tolerate a few pods going down - so you max maxUnavailablePercentage = 10 and maxSurgePercentage=0. You can read about it more here Schema JSON { \"type\": \"string\", \"max_unavailable_percentage\": 25, \"max_surge_percentage\": 0 } Properties Name Type Required Description type string true + value=rolling_update max_unavailable_percentage integer true Percentage of total replicas that can be brought down at one time. For a value of 25 when replicas are set to 12 this would mean minimum (25% of 12) = 3 pods might be unavailable during the deployment. Setting this to a higher value can help in speeding up the deployment process. max_surge_percentage integer true Percentage of total replicas of updated image that can be brought up over the total replicas count. For a value of 25 when replicas are set to 12 this would mean (12+(25% of 12) = 15) pods might be running at one time. Setting this to a higher value can help in speeding up the deployment process. Canary Description This strategy brings up the new release without bringing the older release down. Traffic is shifted from the older release to the newer release in a staged manner. This can help with verifying the health of the new release without shifting complete traffic. Schema JSON { \"type\": \"string\", \"steps\": [ { \"weight_percentage\": 100, \"pause_duration\": 30 } ] } Properties Name Type Required Description type string true + value=canary steps [ CanaryStep ] true These steps would be executed in order to enable shifting of traffic slowly from stable to canary version CanaryStep Description Describes a canary deployment step Schema JSON { \"weight_percentage\": 100, \"pause_duration\": 30 } Properties Name Type Required Description weight_percentage integer true Percentage of total traffic to be shifted to the canary release. The rest will continue to go to the existing deployment pause_duration integer false Duration for which to pause the release. The release process will wait for these seconds before proceeding to the next step. If this is not set, the step will pause indefinitely on this step BlueGreen Description This strategy brings up the new release completely before switching the complete load to the new release. This minimizes the time that two versions are serving traffic at the same time. Schema JSON { \"type\": \"string\", \"enable_auto_promotion\": false, \"auto_promotion_seconds\": 30 } Properties Name Type Required Description type string true + value=blue_green enable_auto_promotion boolean true Promote the new release to handle the complete traffic. A manual promotion would be needed if this is disabled auto_promotion_seconds integer false Promote the new release to handle the complete traffic after waiting for these many seconds Autoscaling ServiceAutoscaling Description Describes a Service Autoscaling configuration Schema JSON { \"metrics\": {} } Properties Name Type Required Description metrics object false Metrics to use for the autoscaler CPUUtilizationMetric Description Describes CPU utilization metric for autoscaling Schema JSON { \"type\": \"string\", \"value\": 0 } Properties Name Type Required Description type string true + value=cpu_utilization value integer true Percentage of cpu request averaged over all replicas which the autoscaler should try to maintain RPSMetric Description Describes requests per second metric for autoscaling Schema JSON { \"type\": \"string\", \"value\": 1 } Properties Name Type Required Description type string true + value=rps value integer true Average request per second averaged over all replicas that autoscaler should try to maintain CronMetric Description Describes cron metric for autoscaling Schema JSON { \"type\": \"string\", \"desired_replicas\": 1, \"start\": \"string\", \"end\": \"string\", \"timezone\": \"UTC\" } Properties Name Type Required Description type string true + value=cron desired_replicas integer false Desired number of replicas during the given interval. Default value is max_replicas. start string true Cron expression indicating the start of the cron schedule. end string true Cron expression indicating the end of the cron schedule. timezone string true Timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. https://docs.truefoundry.com/docs/list-of-supported-timezones Authentication BasicAuthCreds Description Username and password for Basic Service Authentication Schema JSON { \"type\": \"string\", \"username\": \"string\", \"password\": \"string\" } Properties Name Type Required Description type string true + value=basic_auth username string true +label=Username for service auth +message=Upto 64 lower case alphanumeric character long +sort=1 password string true +label=Password for service auth +message=Password should not be more than 64 characters +sort=2 Updated 4 months ago",
    "https://docs.truefoundry.com/docs/job": "Job Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Job All Pages Start typing to search\u2026 Job \ud83d\udcd8 Note While using TrueFoundry python SDK type is not a required field in any of the imported classes Job Description Describes the configuration for the job Schema JSON { \"type\": \"string\", \"name\": \"string\", \"image\": {}, \"trigger\": { \"type\": \"manual\" }, \"params\": [ { \"name\": \"string\", \"description\": \"string\", \"default\": \"string\", \"param_type\": \"string\" } ], \"env\": null, \"resources\": { \"cpu_request\": 0.2, \"cpu_limit\": 0.5, \"memory_request\": 200, \"memory_limit\": 500, \"ephemeral_storage_request\": 1000, \"ephemeral_storage_limit\": 2000, \"gpu_count\": 0, \"shared_memory_size\": 64, \"node\": {} }, \"retries\": 0, \"timeout\": 0, \"concurrency_limit\": 0, \"service_account\": \"string\", \"mounts\": [ {} ], \"labels\": { \"property1\": \"string\", \"property2\": \"string\" } } Properties Name Type Required Description type string true + value=job name string true Name of the job image object true Specify whether you want to deploy a Docker image or build and deploy from source code trigger object true Specify the trigger params [ Param ] false Configure params and pass it to create different job runs env object\u00a6null false Configure environment variables to be injected in the service. Docs resources Resources false Describes the resource constraints for the application so that it can be deployed accordingly on the cluster To learn more you can go here retries integer true Specify the maximum number of attempts to retry a job before it is marked as failed. timeout integer false Job timeout in seconds. concurrency_limit integer false Number of runs that can run concurrently service_account string false Service account that this workload should use mounts [object] false Configure data to be mounted to job pod(s) labels object false Add labels to service metadata Examples Triggers The modules below help configure the Triggers for the Job Manual Description Describes that we are going to manually trigger our job. Schema JSON { \"type\": \"string\" } Properties Name Type Required Description type string true + value=manual Schedule Description Describes that we are going to schedule our job to run at a schedule, making our job a cron job. Schema JSON { \"type\": \"string\", \"schedule\": \"string\", \"concurrency_policy\": \"Forbid\", \"timezone\": \"string\" } Properties Name Type Required Description type string true + value=scheduled schedule string true Specify the schedule for this job to be run periodically in cron format. Learn more concurrency_policy string true Choose whether to allow this job to run while another instance of the job is running, or to replace the currently running instance. Allow will enable multiple instances of this job to run. Forbid will keep the current instance of the job running and stop a new instance from being run. Replace will terminate any currently running instance of the job and start a new one. timezone string false Timezone against which the cron schedule will be calculated, e.g. \"Asia/Tokyo\". Default is machine's local time. https://docs.truefoundry.com/docs/list-of-supported-timezones Enumerate Values Property Value concurrency_policy Forbid concurrency_policy Allow concurrency_policy Replace Param Description Describes a parameter for configuration Schema JSON { \"name\": \"string\", \"description\": \"string\", \"default\": \"string\", \"param_type\": \"string\" } Properties Name Type Required Description name string true Name of the param description string false Description of param default string false Default value or placeholder param_type string false none Enumerate Values Property Value param_type string param_type ml_repo Updated 4 months ago",
    "https://docs.truefoundry.com/docs/asyncservice": "AsyncService Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account AsyncService All Pages Start typing to search\u2026 AsyncService \ud83d\udcd8 Note While using TrueFoundry python SDK type is not a required field in any of the imported classes AsyncService Description Describes the configuration for the async-service Schema JSON { \"type\": \"string\", \"replicas\": 1, \"rollout_strategy\": { \"type\": \"string\", \"max_unavailable_percentage\": 25, \"max_surge_percentage\": 0 }, \"worker_config\": { \"input_config\": { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"visibility_timeout\": 0, \"wait_time_seconds\": 19, \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } }, \"output_config\": { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } } } } Properties Name Type Required Description type string false + value=async-service replicas any false Replicas of service you want to run rollout_strategy Rolling false This strategy updates the pods in a rolling fashion such that a subset of the total pods are replaced with new version at one time. A commonly used strategy can be to have maxUnavailablePercentage close to 0 so that there is no downtime and keep the maxSurgePercentage to around 25%. If you are anyways running a large number of pods, the service can often tolerate a few pods going down - so you max maxUnavailablePercentage = 10 and maxSurgePercentage=0. You can read about it more here worker_config WorkerConfig false Describes the configuration for the Worker Config AsyncServiceAutoscaling Description Describes an Async Service Autoscaling configuration Schema JSON { \"metrics\": { \"type\": \"string\", \"queue_length\": 0 } } Properties Name Type Required Description metrics SQSQueueMetricConfig false Describes a SQS Queue Metric Configuration Worker-Config WorkerConfig Description Describes the configuration for the Worker Config Schema JSON { \"input_config\": { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"visibility_timeout\": 0, \"wait_time_seconds\": 19, \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } }, \"output_config\": { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } } } Properties Name Type Required Description input_config SQSInputConfig true Describes the configuration for the input SQS worker output_config SQSOutputConfig true Describes the configuration for the output SQS worker SQSInputConfig Description Describes the configuration for the input SQS worker Schema JSON { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"visibility_timeout\": 0, \"wait_time_seconds\": 19, \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } } Properties Name Type Required Description type string true + value=sqs queue_url string true AWS SQS Queue URL of Subscriber region_name string true AWS Region Name visibility_timeout integer true A period during which Amazon SQS prevents all consumers from receiving and processing the message. If one message takes 5 seconds to process, you can set this number to 7 or any number higher than 5. This will ensure that while the message is being processed, it will not be available to other replicas. For more information, see here wait_time_seconds integer true Wait timeout for long polling. For more information, see here auth AWSAccessKeyAuth true AWS Access Key based Authentication for Basic Service Authentication SQSOutputConfig Description Describes the configuration for the output SQS worker Schema JSON { \"type\": \"string\", \"queue_url\": \"string\", \"region_name\": \"string\", \"auth\": { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } } Properties Name Type Required Description type string true + value=sqs queue_url string true AWS SQS Queue URL of Publisher region_name string true AWS Region Name auth AWSAccessKeyAuth true AWS Access Key based Authentication for Basic Service Authentication SQSQueueMetricConfig Description Describes a SQS Queue Metric Configuration Schema JSON { \"type\": \"string\", \"queue_length\": 0 } Properties Name Type Required Description type string true + value=sqs queue_length integer true Upper limit of the number of backlog messages the auto-scaler will try to maintain per replica. If you set this number to 10 and have 30 messages in the queue and one replica, the auto-scaler will scale the number of replicas to 3. AWSAccessKeyAuth Description AWS Access Key based Authentication for Basic Service Authentication Schema JSON { \"aws_access_key_id\": \"string\", \"aws_secret_access_key\": \"string\", \"aws_session_token\": \"string\" } Properties Name Type Required Description aws_access_key_id string true AWS Access Key ID aws_secret_access_key string true AWS Secret Access Key for the user to authenticate with aws_session_token string false AWS Session Token, only required when using temporary credentials Updated 4 months ago",
    "https://docs.truefoundry.com/docs/helm": "Helm Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Helm All Pages Start typing to search\u2026 Helm \ud83d\udcd8 Note While using TrueFoundry python SDK type is not a required field in any of the imported classes Helm Description Configuration for Helm Schema JSON { \"type\": \"string\", \"name\": \"string\", \"source\": {}, \"values\": {} } Properties Name Type Required Description type string true + value=helm name string true Name of the Helm deployment. This will be set as the release name of the chart you are deploying. source object true +label=Source helm repository +sort=2 values object false Values file as block file Repo HelmRepo Description Configuration for Helm Repository Schema JSON { \"type\": \"string\", \"repo_url\": \"string\", \"chart\": \"string\", \"version\": \"string\" } Properties Name Type Required Description type string true + value=helm-repo repo_url string true +label=Helm repository URL +sort=1 +message=Needs to be a valid URL. chart string true The helm chart name version string true Helm chart version OCIRepo Description Configuration for OCI Repository Schema JSON { \"type\": \"string\", \"oci_chart_url\": \"string\", \"version\": \"string\" } Properties Name Type Required Description type string true + value=oci-repo oci_chart_url string true +label=OCI chart URL +message=Need to be a valid URL. version string true Helm chart version GitHelmRepo Description Configuration for Git Helm Repository Schema JSON { \"type\": \"string\", \"repo_url\": \"string\", \"revision\": \"string\", \"path\": \"string\", \"value_files\": [ \"string\" ] } Properties Name Type Required Description type string true + value=git-helm-repo repo_url string true +label=Git repository URL +sort=1 +message=Needs to be a valid URL. revision string true Branch/Commit SHA/Tag of the git repo. path string true Path to the chart. value_files [string] false Helm values files for overriding values in the helm chart. The path is relative to the Path directory defined above Updated 4 months ago",
    "https://docs.truefoundry.com/docs/api-reference-image-and-build": "Image and Build Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Image and Build All Pages Start typing to search\u2026 Image and Build \ud83d\udcd8 Note While using TrueFoundry python SDK type is not a required field in any of the imported classes For image you can either give link to a pre-existing image via Image module, or give instructions on how to build the image via the Build class Image Description Describes that we are using a pre-built image stored in a Docker Image registry Schema JSON { \"type\": \"string\", \"image_uri\": \"string\", \"docker_registry\": \"string\", \"command\": null } Properties Name Type Required Description type string true + value=image image_uri string true The image URI. Specify the name of the image and the tag. If the image is in Dockerhub, you can skip registry-url (for e.g. tensorflow/tensorflow ). You can use an image from a private registry using Advanced fields docker_registry string false FQN of the container registry. You can the FQN of your desired container registry (or add one) in the Integrations page Integrations page command any false Override the command to run when container starts. When deploying a Job, the command can be templatized by defining params and referencing them in command E.g. python main.py --learning_rate {{learning_rate}} Build Description Describes how we build our code into a Docker image. Schema JSON { \"type\": \"string\", \"docker_registry\": \"string\", \"build_source\": {}, \"build_spec\": {} } Properties Name Type Required Description type string true + value=build docker_registry string false FQN of the container registry. You can the FQN of your desired container registry (or add one) in the Integrations page Integrations page build_source object true Source code location. build_spec object true Instructions to build a container image out of the build source Build Spec The modules below help define the build specification. PythonBuild Description Describes that we are using python to build a container image with a specific python version and pip packages installed. Schema JSON { \"type\": \"string\", \"python_version\": \"3.9\", \"build_context_path\": \"./\", \"requirements_path\": \"string\", \"pip_packages\": [ \"string\" ], \"apt_packages\": [ \"string\" ], \"command\": null, \"cuda_version\": \"string\" } Properties Name Type Required Description type string true + value=tfy-python-buildpack python_version string true Python version to run your application. Should be one of the tags listed on Official Python Docker Page build_context_path string true Build path relative to project root path. requirements_path string false Path to requirements.txt relative to Path to build context pip_packages [string] false Define pip package requirements. In Python/YAML E.g. [\"fastapi>=0.90,<1.0\", \"uvicorn\"] apt_packages [string] false Debian packages to install via apt get . In Python/YAML E.g. [\"git\", \"ffmpeg\", \"htop\"] command any true Command to run when the container starts. Command will be set as the Entrypoint of the generated image. When deploying a Job, the command can be templatized by defining params and referencing them in command E.g. python main.py --learning_rate {{learning_rate}} cuda_version string false Version of CUDA Toolkit and CUDNN to install in the image These combinations are based off of publically available docker images on docker hub You can also specify a valid tag of the form {cuda_version_number}-cudnn{cudnn_version_number}-{runtime/devel}-ubuntu{ubuntu_version} Refer https://hub.docker.com/r/nvidia/cuda/tags for valid set of values Note: We use deadsnakes ubuntu ppa to add Python that currently supports only Ubuntu 18.04, 20.04 and 22.04 DockerFileBuild Description Describes that we are using a dockerfile to build our image Schema JSON { \"type\": \"string\", \"dockerfile_path\": \"./Dockerfile\", \"build_context_path\": \"./\", \"command\": null, \"build_args\": { \"property1\": \"string\", \"property2\": \"string\" } } Properties Name Type Required Description type string true + value=dockerfile dockerfile_path string true The file path of the Dockerfile relative to project root path. build_context_path string true Build context path for the Dockerfile relative to project root path. command any false Override the command to run when the container starts When deploying a Job, the command can be templatized by defining params and referencing them in command E.g. python main.py --learning_rate {{learning_rate}} build_args object false Build arguments to pass to docker build . Build Source The modules below help define the build specification. LocalSource Description Describes that we are using code stored in a local developement environment to build our image Schema JSON { \"type\": \"string\", \"project_root_path\": \"./\", \"local_build\": true } Properties Name Type Required Description type string true + value=local project_root_path string true Local project root path. local_build boolean true run docker build locally GitSource Description Describes that we are using code stored in a git repository to build our image Schema JSON { \"type\": \"string\", \"repo_url\": \"string\", \"ref\": \"string\", \"branch_name\": \"string\" } Properties Name Type Required Description type string true + value=git repo_url string true The repository URL. ref string true The commit SHA. branch_name string false Selecting branch will select latest commit SHA of the branch. RemoteSource Description Describes that we are using code stored in a remote respository to build our image Schema JSON { \"type\": \"string\", \"remote_uri\": \"string\" } Properties Name Type Required Description type string true + value=remote remote_uri string true Remote repository URI Updated 4 months ago",
    "https://docs.truefoundry.com/docs/deployment-additional-configuration": "Deployment Additional Configuration Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deployment Additional Configuration All Pages Start typing to search\u2026 Deployment Additional Configuration \ud83d\udcd8 Note While using Truefoundry python SDK type is not a required field in any of the imported classes For deployments we can use the modules below to add necessary functionalities Resources Resources Description Describes the resource constraints for the application so that it can be deployed accordingly on the cluster To learn more you can go here Schema JSON { \"cpu_request\": 0.2, \"cpu_limit\": 0.5, \"memory_request\": 200, \"memory_limit\": 500, \"ephemeral_storage_request\": 1000, \"ephemeral_storage_limit\": 2000, \"gpu_count\": 0, \"shared_memory_size\": 64, \"node\": {} } Properties Name Type Required Description cpu_request number true Requested CPU which determines the minimum cost incurred. The CPU usage can exceed the requested amount, but not the value specified in the limit. 1 CPU means 1 CPU core. Fractional CPU can be requested like 0.5 or 0.05 cpu_limit number true CPU limit beyond which the usage cannot be exceeded. 1 CPU means 1 CPU core. Fractional CPU can be requested like 0.5 . CPU limit should be >= cpu request. memory_request integer true Requested memory which determines the minimum cost incurred. The unit of memory is in megabytes(MB). So 1 means 1 MB and 2000 means 2GB. memory_limit integer true Memory limit after which the application will be killed with an OOM error. The unit of memory is in megabytes(MB). So 1 means 1 MB and 2000 means 2GB. MemoryLimit should be greater than memory request. ephemeral_storage_request integer true Requested disk storage. The unit of memory is in megabytes(MB). This is ephemeral storage and will be wiped out on pod restarts or eviction ephemeral_storage_limit integer true Disk storage limit. The unit of memory is in megabytes(MB). Exceeding this limit will result in eviction. It should be greater than the request. This is ephemeral storage and will be wiped out on pod restarts or eviction gpu_count integer true Count of GPUs to provide to the application Note the exact count and max count available for a given GPU type depends on cloud provider and cluster type. shared_memory_size integer false Define the shared memory requirements for your workload. Machine learning libraries like Pytorch can use Shared Memory for inter-process communication. If you use this, we will mount a tmpfs backed volume at the /dev/shm directory. Any usage will also count against the workload's memory limit ( resources.memory_limit ) along with your workload's memory usage. If the overall usage goes above resources.memory_limit the user process may get killed. Shared Memory Size cannot be more than the defined Memory Limit for the workload. node object false This field determines how the underlying node resource is to be utilized NodeSelector Description Constraints to select a Node - Specific GPU / Instance Families, On-Demand/Spot. Schema JSON { \"type\": \"string\", \"gpu_type\": \"string\", \"instance_families\": [ \"string\" ], \"capacity_type\": \"spot_fallback_on_demand\" } Properties Name Type Required Description type string true + value=node_selector gpu_type string false Name of the Nvidia GPU. One of [K80, P4, P100, V100, T4, A10G, A100_40GB, A100_80GB][K80, P4, P100, V100, T4, A10G, A100_40GB, A100_80GB] One instance of the card contains the following amount of memory - K80: 12 GB, P4: 8 GB, P100: 16 GB, V100: 16 GB, T4: 16 GB, A10G: 24 GB, A100_40GB: 40GB, A100_80GB: 80 GB instance_families [string] false Instance family of the underlying machine to use. Multiple instance families can be supplied. The workload is guaranteed to be scheduled on one of them. capacity_type string false Configure what type of nodes to run the app. By default no placement logic is applied. \"spot_fallback_on_demand\" will try to place the application on spot nodes but will fallback to on-demand when spot nodes are not available. \"spot\" will strictly place the application on spot nodes. \"on_demand\" will strictly place the application on on-demand nodes. Enumerate Values Property Value capacity_type spot_fallback_on_demand capacity_type spot capacity_type on_demand NodepoolSelector Description Specify one or more nodepools to run your application on. Schema JSON { \"type\": \"string\", \"nodepools\": [ \"string\" ] } Properties Name Type Required Description type string true + value=nodepool_selector nodepools [string] false Nodepools where you want to run your workload. Multiple nodepools can be selected. The workload is guaranteed to be scheduled on one of the nodepool Mounts VolumeMount Description Describes a volume mount Schema JSON { \"type\": \"string\", \"mount_path\": \"string\", \"volume_fqn\": \"string\" } Properties Name Type Required Description type string true + value=volume mount_path string true Absolute file path where the volume will be mounted. volume_fqn string true The Truefoundry volume that needs to be mounted. SecretMount Description Describes a secret mount Schema JSON { \"type\": \"string\", \"mount_path\": \"string\", \"secret_fqn\": \"string\" } Properties Name Type Required Description type string true + value=secret mount_path string true Absolute file path where the file will be created. secret_fqn string true The Truefoundry secret whose value will be the file content. StringDataMount Description Describes a string data mount Schema JSON { \"type\": \"string\", \"mount_path\": \"string\", \"data\": \"string\" } Properties Name Type Required Description type string true + value=string mount_path string true Absolute file path where the file will be created. data string true The file content. Updated 4 months ago",
    "https://docs.truefoundry.com/docs/guide-to-inference-model-on-truefoundry": "Guide to inference model on truefoundry Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Guide to inference model on truefoundry All Pages Start typing to search\u2026 Guide to inference model on truefoundry In this guide we will show you how to infer various types of models on the truefoundry platform, we have provided the Python snippet of all the model tasks supported on truefoundry. Image Classification Python import requests from urllib.parse import urljoin def send_image(image_path, url): # Open the image file in binary mode with open(image_path, 'rb') as image_file: # Send the POST request headers = { \"accept\": \"application/json\", \"Content-Type\": \"image/png\" } response = requests.post(url, headers=headers, data=image_file) # Check the response if response.status_code == 200: print(\"Success:\", response.json()) else: print(\"Failed:\", response.status_code, response.text) # Example usage image_path = \"<Enter path to the image on your local>\" # Replace with your image file name endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) send_image(image_path, url) In this, the response will be the array of objects that will have the confidence percentage of the prediction and the prediction category. JSON [ { \"score\": 0.9990900754928589, \"label\": \"...\" }, ] Text Classification Python import requests from urllib.parse import urljoin #endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) # if input is plain text or just json then use this code def classify(): #The text string which you want to get classification for. payload = {\"inputs\": \"I am very happy today\"} response = requests.post(url=url, json=payload) return response.json() print(classify()) The output will be an array of classes with their score. JSON [ { \"label\": \"POSITIVE\", \"score\": 0.9998797178268433 } ] Zero-Shot Classification Python import requests from urllib.parse import urljoin # endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) # if input is plain text or just json then use this code def classify(): # The text string which you want to get classification for. payload = { \"inputs\": \"I have a problem with my iphone that needs to be resolved asap!!\", # The candidate labels which you want to classify the text into. \"parameters\": { \"candidate_labels\": \"urgent, not urgent, phone, tablet, computer\", # If there are more than two classes, set this to True. \"multi_class\": True, }, } response = requests.post(url=url, json=payload) return response.json() print(classify()) The output will look something like this. JSON { \"sequence\": \"I have a problem with my iphone that needs to be resolved asap!!\", \"labels\": [ \"urgent\", \"phone\", ... ], \"scores\": [ 0.998576283454895, 0.9949977993965149, ... ] } Token Classification Python import requests from urllib.parse import urljoin # endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) def classify(): # The text string which you want to get classification for. payload = { \"inputs\": \"My name is Sarah and I live in London\" } response = requests.post(url=url, json=payload) return response.json() print(classify()) The output will be subject to change based on the mode, for NER classification it would look like this. JSON [ { \"entity\": \"B-PER\", \"score\": 0.9985477328300476, \"index\": 4, \"word\": \"Sarah\", \"start\": 11, \"end\": 16 }, ... ] Fill Mask Python import requests from urllib.parse import urljoin # endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) def classify(): # The text string which you want to get classification for. payload = { \"inputs\": \"The goal of life is [MASK].\" } response = requests.post(url=url, json=payload) return response.json() print(classify()) The output will be the array of words with the percentage of how likely a work can fit and replace [MASK] JSON [ { \"score\": 0.1093330830335617, \"token\": 2166, \"token_str\": \"life\", \"sequence\": \"the goal of life is life.\" }, ... ] Text to Image Python import requests url = \"<Enter service endpoint>\" # if input is plain text or just json then use this code def generate_image(prompt: str): payload = prompt response = requests.post(url=url, json=payload) if response.status_code == 200: with open(\"image.png\", \"wb\") as f: f.write(response.content) else: print(\"Error: \", response.text) with open(\"image.png\", \"wb\") as f: f.write(response.content) generate_image(\"A panda drinking water from a bottle.\") The above code will call the api and then it will save the image as image.png in root directory. Image to text Python import requests from urllib.parse import urljoin def send_image(image_path, url): # Open the image file in binary mode with open(image_path, 'rb') as image_file: # Send the POST request headers = { \"accept\": \"application/json\", \"Content-Type\": \"image/png\" } response = requests.post(url, headers=headers, data=image_file) # Check the response if response.status_code == 200: print(response.json()) else: print(\"Failed:\", response.status_code, response.text) # Example usage image_path = \"<Enter path to the image on your local>\" # Replace with your image file name endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) send_image(image_path, url) The response will be something like this JSON [ { \"generated_text\": \"string\" } ] Translation Python import requests from urllib.parse import urljoin # endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) # if input is plain text or just json then use this code def classify(): # The text string which you want to get translated payload = { \"inputs\": \"string.\" } response = requests.post(url=url, json=payload) return response.json() print(classify()) The ouput will containt the translated text JSON [ { \"translation_text\": \"Was ist das?\" } ] Object Detection Python import requests from urllib.parse import urljoin def send_image(image_path, url): # Open the image file in binary mode with open(image_path, 'rb') as image_file: # Send the POST request headers = { \"accept\": \"application/json\", \"Content-Type\": \"image/png\" } response = requests.post(url, headers=headers, data=image_file) # Check the response if response.status_code == 200: print(response.json()) else: print(\"Failed:\", response.status_code, response.text) # Example usage image_path = \"<Enter path to the image on your local>\" # Replace with your image file name endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) send_image(image_path, url) The output will contain the labels along with the co-ordinates of box JSON [ { \"score\": 0.9769997596740723, \"label\": \"string\", \"box\": { \"xmin\": 90, \"ymin\": 200, \"xmax\": 99, \"ymax\": 212 } } ] Text Generation(Chat Completion) Python import requests from urllib.parse import urljoin headers = { 'accept': 'application/json', 'content-type': 'application/json', } json_data = { 'model': 'model-name', 'messages': [ {'role': 'user', 'content': 'You are a helpful assistant'}, {'role': 'assistant', 'content': 'Hello there!'}, {'role': 'user', 'content': 'What is 2 + 2?'}, ], 'temperature': 0.8, } endpoint = \"<Enter service endpoint>\" generation_path = \"v1/chat/completions\" url = urljoin(endpoint, predict_path) response = requests.post( url, headers=headers, json=json_data, ) print(response.json()) This will return the generated text based on the the chat or messages JSON { ... \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"The answer to 2 + 2 is 4. Is there anything else I can help you with?\", \"tool_calls\": [] }, \"logprobs\": null, \"finish_reason\": \"stop\", \"stop_reason\": null } ], ... } Text Generation(Completion) Python import requests from urllib.parse import urljoin headers = { 'accept': 'application/json', 'content-type': 'application/json', } json_data = { 'model': 'model-name', 'prompt': [ 'hi what does this do?', ], 'temperature': 0.8, } endpoint = \"<Enter service endpoint>\" generation_path = \"v1/completions\" url = urljoin(endpoint, predict_path) response = requests.post( url, headers=headers, json=json_data, ) The model will try to complete the prompt JSON { ... \"choices\": [ { \"index\": 0, \"text\": \" the Order of the Red Hand.\\nThey had heard the call, the few scattered members of the Order of the Red Hand moved to respond. The world had become a darker place and the Order of the Red Hand saw it as their duty to bring order to the chaos.\\nAs the members began to gather the air grew thick with anticipation. The Order of the Red Hand was a force of justice, a force that struck fear into the hearts of those who would do harm. They were the protector of the innocent and the stronghold against darkness.\\nThe leader of the Order, a woman known only as the Hand rose to her feet. Her face was a map of scars and her eyes burned with determination. \\\" Brothers and sisters of the Order,\\\" she began. \\\"The time for action has come. The world is in chaos and the innocent are suffering. We have been called to duty and we will not falter.\\\"\\nThe members of the Order nodded in agreement, their faces set with determination. They knew the task that lay before them, they knew the dangers that they would face, but they were undaunted. For they were the Order of the Red Hand and they would not rest until justice was served.\\nTogether, the members of the Order began to move out, their footsteps echoing\", \"logprobs\": null, \"finish_reason\": \"length\", \"stop_reason\": null, \"prompt_logprobs\": null } ], ... } Summarization Python import requests from urllib.parse import urljoin # endpoint of the model server endpoint = \"<Enter service endpoint>\" predict_path = \"/predictions/model\" url = urljoin(endpoint, predict_path) def summarize(): # The text string which you want to get Summarization for. payload = { \"inputs\": \"\"\" TrueFoundry is a Cloud-native PaaS for Machine learning teams to build, deploy and ship ML/LLM Applications on their own cloud/on-prem Infra in a faster, scalable, cost efficient way with the right governance controls, allowing them to achieve 90% faster time to value than other teams. TrueFoundry abstracts out the engineering required and offers GenAI accelerators - LLM PlayGround, LLM Gateway, LLM Deploy, LLM Finetune, RAG Playground and Application Templates that can enable an organisation to speed up the layout of their overall GenAI/LLMOps framework. Enterprises can plug and play these accelerators with their internal systems as well as build on top of our accelerators to enable a LLMOps platform of their choice to the GenAI developers. TrueFoundry is modular and completely API driven, has native integration with popular tools in the market like LangChain, VectorDBs, GuardRails, etc. \"\"\" } response = requests.post(url=url, json=payload) return response.json() print(summarize()) The output will be summarized text. JSON [ { \"summary_text\": \"TrueFoundry is a Cloud-native PaaS for Machine learning teams to build, deploy and ship ML/LLM Applications on their own cloud/on-prem Infra in a faster, scalable, cost efficient way . Enterprises can plug and play these accelerators with their internal systems as well as build on top of our accelerators to enable a LLMOps platform of their choice to the GenAI developers .\" } ] Updated 5 months ago",
    "https://docs.truefoundry.com/docs/aws-1": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on AWS TrueFoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled EKS cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VPC + New EKS cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VPC + New EKS cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing EKS cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for AWS EKS depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue after making sure you have all the requirements satisfied. A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/azure-1": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on Azure Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled AKS cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VNet + New AKS cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VNet + New AKS cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing AKS cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for Azure AKS depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated 3 months ago",
    "https://docs.truefoundry.com/docs/environments": "Environments Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Environments All Pages Start typing to search\u2026 Environments As your organization uses the platform more and more, your Deployments dashboard will start filling up, especially if you are a Tenant Admin or a Cluster Admin. To navigate through all the applications, effectively filtering applications through meaningful tags and differentiating them based on relevant criteria becomes crucial. This is where Environments come into the picture. You can create custom environments (e.g., development, staging, production) and tag workspaces accordingly. This automatically applies the environment tag to all applications within that workspace. This approach offers two key benefits: Environment-based filtering : You can easily filter applications by their assigned environment, focusing on specific sets like production services. Descriptive environment names : Environment names provide immediate context about an application's purpose and stage. Creating Environments To create an environment in TrueFoundry, follow these steps: How to tag a workspace with an environment To tag a workspace with an environment, first, the Cluster where the workspace resides needs to have those environments added. For this, you will have to add all environments relevant to your cluster (one cluster can have multiple environments) in the cluster, using the instructions provided below. Now all your applications deployed within that specific workspace will have the environment of the workspace show up beside them. Filtering Applications You can use environments to filter your workspaces and applications. For example, you could filter your workspaces to only show those that are tagged with the development environment. You could also filter your applications to only show those that are tagged with the production environment. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/deploy-your-first-service": "Deploy Service from a public Github repository Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy Service from a public Github repository All Pages Start typing to search\u2026 Deploy Service from a public Github repository In this guide, we'll deploy a FastAPI service for solving the Iris classification problem. This problem involves predicting the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already created a FastAPI service for the Iris classification problem, and you can find the code in our GitHub Repository . Please visit the repository to familiarize yourself with the code you'll be deploying. Project Structure The project files are organized as follows: Text . \u251c\u2500\u2500 app.py - Contains FastAPI code for inference. \u251c\u2500\u2500 iris_classifier.joblib - The model file. \u2514\u2500\u2500 requirements.txt - Lists dependencies. All these files are located in the same directory. Prerequisties Before you proceed with the guide, please ensure that you have setup a Workspace . To deploy your service, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Initiating Deployment via UI Use these configs for the deployment form: Repo URL : https://github.com/truefoundry/getting-started-examples Path to build context : ./deploy-ml-model/ Command : uvicorn app:app --host 0.0.0.0 --port 8000 Port : 8000 \ud83d\udcd8 What we did above: In the example above, we only had Python code and a requirements.txt. We didn't have a prewritten docker file - so we chose the Python Code option - to let Truefoundry templatize a Dockerfile from the details provided about the application and build the Docker image for us. We give these details in the Build context field, where we specify the directory in the GitHub repository where our service code resides ( ./deploy-ml-model/ ). We also specify the command that we need to use to run our service ( uvicorn app:app --host 0.0.0.0 --port 8000 ). Finally, we specify the port that we want our service to listen on ( 8000 ). View your deployed service Once you click Submit , your deployment will be successful in a few seconds, and your service will be displayed as active (green), indicating that it's up and running. Congratulations! You've successfully deployed your FastAPI service. To learn how to use your service-specific dashboard and send requests to your service, check out this guide: Interacting with your Service Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configuring-your-service": "Dockerize your code Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Dockerize your code All Pages Start typing to search\u2026 Dockerize your code Its a good practice to always dockerize your code while deploying so that it is guaranteed to run anywhere. Truefoundry helps you deploy your code on Kubernetes for which you will first need to create a docker image for your codebase. Truefoundry can help you deploy in all the three use cases: You already have a docker image built that you want to deploy - You can integrate the docker registry with Truefoundry and deploy the image. You don't have an image, but you have written a Dockerfile to build the code - Truefoundry can help build the image and deploy the image. You just have the code and the requirements.txt and don't have a Dockerfile - Truefoundry can automatically generate a Dockerfile and build and deploy the image. Regardless of any of the above scenarios, we will finally have a docker image which Truefoundry will then deploy on Kubernetes. \ud83d\udcd8 Private Docker Registry If you are using a Private Docker Registry, you will need to also integrate it with TrueFoundry. Check the Integrations page to connect your docker registry. Deploying your application when you have a Pre-built Image Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Image, Port service = Service( name=\"your-service\", + image = Image( + image_uri=\"your_image_uri\", # You can get this following this guide: https://docs.truefoundry.com/docs/integrations-docker-registry + docker_registry=\"your_docker_registry_fqn\", + command=\"command override\", + ) ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Deploying your application when you have written a Dockerfile and haven't built the image Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Build, DockerFileBuild, Port service = Service( name=\"my-service\", + image=Build( + build_spec=DockerFileBuild( + dockerfile_path=\"Dockerfile\" + build_context_path=\"./\", + build_args={ + \"param\": \"value\", + \"param1\": \"value1\", + }, + ), + ) ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") \ud83d\udcd8 Docker Build Args In DockerFileBuild , use build_args to set arguments for docker build --build-arg . It involves key-value pairs of string datatype. For instance: Python from truefoundry.deploy import DockerFileBuild build = DockerFileBuild( build_args={\"FOO\": \"Hello\", \"BAR\": \"World!\"} ) The ARG instruction in Dockerfiles defines variables set at build-time with --build-arg during docker build . Here's a concise Dockerfile example: Dockerfile FROM ubuntu ARG FOO ARG BAR ENV FOO_ENV=$FOO ENV BAR_ENV=$BAR RUN echo $FOO_ENV && echo $BAR_ENV Deploying your application when you haven't written a Dockerfile Through User Interface Through Python SDK In your deployment code deploy.py , include the following: Diff from truefoundry.deploy import Service, Build, PythonBuild, Port service = Service( name=\"your-service\", + image=Build( + build_spec=PythonBuild( + python_version=\"3.9\", + build_context_path=\"./\", + requirements_path=\"my-requirements.txt\", + pip_packages=[\"fastapi==0.82.0\", \"uvicorn\"], + command=\"uvicorn main:app --port 8000 --host 0.0.0.0\" + ), + ), ports=[ Port( host=\"your_host\", port=8501 ) ], ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") If you run the deploy.py , Truefoundry will start building the image on your local machine if docker is installed. Building locally can be quite fast specially if you are iterating rapidly. Truefoundry can also build the image remotely using a remote build server. It will fallback to the remote build server if you don't have docker installed locally. If you always want the build to fallback to the remote server, you can set local_build as False. as described below. This can be the preferred option if its taking a long time to upload the docker image to the registry from your local machine because of slow internet. Python from truefoundry.deploy import LocalSource, Build ... service = Service( ... image=Build( build_source=LocalSource( ... local_build=False ... ), ) ... ) ... Updated 1 day ago",
    "https://docs.truefoundry.com/docs/understanding-azure-node-pools": "Understanding Azure Node Pools Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Understanding Azure Node Pools All Pages Start typing to search\u2026 Understanding Azure Node Pools Node pools in Azure AKS enable the creation and management of distinct worker node groups within a single cluster. They offer the flexibility to allocate resources based on workload requirements and cost considerations. By utilizing different node pools, organizations can optimize resource allocation, achieve scalability, and take advantage of cost savings through the use of spot instances for non-critical workloads or development environments. Additionally, node pools enable efficient application packing and isolation, ensuring performance guarantees and fault tolerance. They also facilitate the utilisation of GPU resources by dedicating on-demand node pools for high-performance production workloads and spot node pools for cost-effective development workloads. In this document we will understand how to better utilise the node pools in AKS for better management of your resources. System Node pool If you have followed the steps of Creating the AKS cluster a system node pool must have been created which works on on-demand nodes to deploy all the necessary applications that are required to power the platform. These includes Argocd Argo rollouts Istio tfy-agent It is advisable to use atleast 2 nodes with 2vCPU and 8GB RAM to successfully install all the necessary applications. \ud83d\udcd8 One on-demand node pool is always required BY default the primary node pool of AKS (system node pool) should always be on-demand. It is not possible to create a SPOT node pool initially. User based node pools User based node pools are used to run the user applications. These pools can be of type on-demand or spot . Spot node pool Spot node pools are used to host the user workloads which can tolerate interruptions. As spot instances are the machines which are used from left-overs they can bring significant cost savings in the cloud billing and because of this there are certain applications, dev workloads and un-important job runs which can be promoted to run over spot instances. Creating a SPOT CPU node pool Spot CPU node pool should be used for the cases where the applications can tolerate significant interruptions. By default TrueFoundry can tolerate interruptions on these applications which are supporting the platform Prometheus Loki cert-manager argo-workflows A right instance size can be selected from this page which can help you select the right size/price ratio for your workloads. With the below command you can create a spot instance Shell # export export RESOURCE_GROUP=\"\" export CLUSTER_NAME=\"\" # enter the instance size from the page linked above export INSTANCE_SIZE=\"\" Command to create a spot CPU pool Shell az aks nodepool add \\ --resource-group $RESOURCE_GROUP \\ --cluster-name $CLUSTER_NAME \\ --name spotnodepool \\ --priority Spot \\ --eviction-policy Delete \\ --spot-max-price -1 \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --min-count 2 \\ --node-count 2 \\ --max-count 10 \\ --no-wait With this command a SPOT node pool with minimum 2 nodes will spin up which can autoscale to 10. Spot node pools by default create a taint kubernetes.azure.com/scalesetpriority:spot which means all the pods which you want to get deployed on these spot instances must tolerate this taint. The toleration must happen like this Shell tolerations: - key: kubernetes.azure.com/scalesetpriority value: spot effect: NoSchedule However, this doesn't guarantee that the pods that you meant to deploy on these spot instances will always be deployed there. You have to select the spot node pool while deploying your services to force them to get deployed on spot pools only. Check Adding node pools to the platform to know more. Creating a spot GPU node pool Creating a spot GPU node pool is similar to the CPU spot node pool except for two things Select the right instance size for the GPU workload and make sure you have the required Quotas for GPU instance in your specific region Taint of nvidia.com/gpu=Present:NoSchedule . It is important to add this taint to avoid non-GPU workloads to get deployed on GPU machines Execute the below command to create a node pool in your AKS cluster. Make sure to replace the variable correctly Shell az aks nodepool add \\ --cluster-name $CLUSTER_NAME \\ --name gpuspotpool \\ --resource-group $RESOURCE_GROUP \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --priority Spot \\ --spot-max-price -1 \\ --eviction-policy Delete \\ --node-vm-size $INSTANCE_SIZE \\ --node-taints nvidia.com/gpu=Present:NoSchedule \\ --max-count 2 \\ --min-count 1 \\ --node-count 1 \\ --mode user \\ --tags team=datascience owner=truefoundry On-demand or Regular node pools On-demand or Regular node pools are used for deploying your applications which require a dedicated machine to run. These workloads are important or nearly important to run at the required time. On-demand nodes are generally expensive to their counterparts (spot) but have SLA available on them. As a general practice, on-demand nodes don't suffer downtime and doesn't face interruptions. However, It is always to be noted that nodes are ephemeral in nature and upon excess threshold utilisation they can go down. Creating an on-demand CPU node pool Run the below command in your cluster by selecting the right instance size Shell az aks nodepool add \\ --resource-group $RESOURCE_GROUP \\ --cluster-name $CLUSTER_NAME \\ --name odnodepool \\ --eviction-policy Delete \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --min-count 2 \\ --node-count 2 \\ --node-osdisk-size 100 \\ --max-count 10 \\ --no-wait You can set the count of nodes according to your needs and it is advisable to keep the autoscaling part enabled for your cluster. Creating an on-demand GPU node pool Creating an on-demand GPU node pool is similar to the CPU on-demand node pool except for two things Select the right instance size for the GPU workload and make sure you have the required Quotas for GPU instance in your specific region Taint of nvidia.com/gpu=Present:NoSchedule . It is important to add this taint to avoid non-GPU workloads to get deployed on GPU machines Execute the below command to create a node pool in your AKS cluster. Make sure to replace the variable correctly Shell az aks nodepool add \\ --cluster-name $CLUSTER_NAME \\ --name odgpupool \\ --resource-group $RESOURCE_GROUP \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size $INSTANCE_SIZE \\ --node-taints nvidia.com/gpu=Present:NoSchedule \\ --max-count 2 \\ --min-count 1 \\ --node-count 1 \\ --node-osdisk-size 100 \\ --mode user \\ --tags team=datascience owner=truefoundry Adding node pools in the platform By default TrueFoundry requires an Azure AD application which has Reader access on the AKS cluster to sync all the nodepools. If this Azure AD application is already added to the platform the new nodepools will sync automatically. Understanding Cost implications of spot and on-demand (regular) node-pools A huge difference can be observed while analysing the cloud cost for both spot and on-demand where spot nodes seems to be a highly cheap option. Moreover, this brings lot of uncertainty for the node uptime leading to trade-off. Below is a sample collection of nodes running for a month to analyse costs for spot and on-demand machines. All these are pricing based out of South central US region. Priority Instance type Compute GPU Cost( per month) Spot Standard_D2s_v5 2vCPU/8 GB RAM False $ 10.19 On-demand Standard_D2s_v5 2vCPU/8 GB RAM False $ 83.95 Spot Standard_NC6 6vCPU/56 GB RAM True $ 78.84 On-demand Standard_NC6 6vCPU/56 GB RAM True $ 788.40 Updated 3 months ago",
    "https://docs.truefoundry.com/docs/monitoring-your-service": "View logs, metrics and events Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account View logs, metrics and events All Pages Start typing to search\u2026 View logs, metrics and events TrueFoundry provides you with Logs, Metrics and Events to monitor your deployments to identify and debug issues. Browse Logs using UI Logs are records of events that occur in your deployment, such as requests to your services, errors that occur, and system messages. TrueFoundry provides you with logs at 2 levels: Deployment Level Logs : This will show the combined logs from all the pods in the deployment. Pod Level Logs: This allows you to view logs for an individual pod in the deployment. Metrics Dashboard Metrics dashboard provides a visual representation of the metrics collected from your application like CPU Usage, GPU Usage, Network Usage etc. This allows you quick and easy way to identify fluctuations and potential performance bottlenecks. TrueFoundry provides you with metrics dashboards at two levels: Deployment Level Metrics : This will show the combined metrics from all the pods in the deployment. Pod Level Metrics: This allows you to view metrics for an individual pod in the deployment. View Events List Events are the occurrences that happen in your TrueFoundry deployment. They can be triggered by a variety of things, a few examples of which are: Deploying a new service version Starting or stopping a pod A pod crashing Events can be used to track the progress of your deployment and to troubleshoot any problems that may occur. These are the standard Kubernetes events - you probably don't need to look at them often unless you are debugging your pod not starting. TrueFoundry provides you with events at 2 levels: Deployment Level Events : This will show the combined events from all the pods in the deployment. Pod Level Events: This allows you to view events for an individual pod in the deployment. Set up Monitoring through Grafana Grafana is a popular open-source dashboarding tool that can plot the different metrics in Prometheus. Since Truefoundry services metrics are present in Prometheus, they can be viewed in Grafana also. Exporting the metrics dashboard to Grafana will enable the following scenarios: Customizable dashboards : Grafana allows you to create custom dashboards that display the metrics that are most important to you. Alerting : You can set up alerts in Grafana to notify you when certain metrics reach specific thresholds. Integration with other tools : Grafana integrates with a variety of other tools, such as Prometheus and Elasticsearch, so you can consolidate your monitoring data into a single platform. To get the Grafana dashboard, you can use Export Metrics Dashboard to Grafana feature as described below. Prerequisites Before you can start exporting your Service's Metrics Dashboard to Grafana, you need to complete the following prerequisites: Install Grafana on your Cluster : Follow the instructions provided below to install Grafana on your cluster. Obtain Grafana Credentials : Once Grafana is installed, follow the instructions below and obtain the necessary credentials. These credentials will be used later in the process of accessing your Grafana Dashboard Exporting Metrics Dashboard To export the Metrics Dashboard to Grafana you can follow the instruction given below: Integrate with your own monitoring and logging solution You can integrate any of your existing monitoring and logging solutions like Datadog, Newrelic, Prometheus, Cloudwatch, Splunk, etc for the applications deployed on Truefoundry. Since Truefoundry applications run on a Kubernetes cluster deployed on your account, you can install the respective agents of the monitoring solutions on the cluster and get all the logs and metrics show up on your monitoring dashboard. Updated 2 months ago",
    "https://docs.truefoundry.com/docs/deploy-via-python-sdk-copy": "Deploy and Run Job from a public GitHub repository Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Deploy and Run Job from a public GitHub repository All Pages Start typing to search\u2026 Deploy and Run Job from a public GitHub repository In this guide, we'll deploy a Job to train a machine learning model. The model will learn to predict the species of an iris flower based on its sepal length, sepal width, petal length, and petal width. There are three species: Iris setosa, Iris versicolor, and Iris virginica. Project Setup We've already prepared the training script that trains a model on the Iris dataset, and you can find the code in our GitHub Repository . Please visit the repository to familiarise yourself with the code you'll be deploying. Project Structure The project files are organised as follows: Text . \u251c\u2500\u2500 train.py - Contains the training script code. \u2514\u2500\u2500 requirements.txt - Contains the list of all dependencies. All these files are located in the same directory. Prerequisites Before you proceed with the guide, please ensure that you have setup a Workspace . To deploy your job, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Initiating Deployment via UI Use these configs for the deployment form: Repo URL : https://github.com/truefoundry/getting-started-examples Path to build context : ./train-model/ Command : python train.py \ud83d\udcd8 What we did above: In the example above, we only had Python code and a requirements.txt file. The source code is in a Github repo and we don't have a prewritten docker file or a DockerImage - so we chose the Python Code option. Once we submit the form, TrueFoundry templatised a Dockerfile from the details provided and build the Docker image for us. In the Build context field we specified the directory( ./train-model/ ) where our job code resides in the GitHub repository. We also specified the command( python train.py ) that we need to use to run our training script. View your deployed job After you click Submit , your job will be deployed in a few seconds. On successful deployment your Job will be displayed as Suspended (yellow) indicating that your Job has been deployed but it will not run automatically. Run your job To run your Job you will have to trigger it manually. Congratulations! You have successfully deployed and run your training Job. To learn how to interact with your job check out this guide: Interacting with your Job Updated 5 months ago",
    "https://docs.truefoundry.com/docs/configuring-your-job": "Configure Job Trigger Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Job Trigger All Pages Start typing to search\u2026 Configure Job Trigger In many cases, tasks need to be executed either on demand or at regular intervals. For instance, sending a welcome email to a new user upon registration requires immediate execution, while data scraping from a website every few hours demands a recurring schedule. TrueFoundry provides two trigger types for your Jobs: manual triggers and schedule triggers . These trigger types allow you to configure how and when your jobs should be executed. Manual Triggers Manual triggers are ideal for tasks that require on-demand execution. This trigger type allows users to manually initiate job runs programmatically or through the User Interface. This is suitable for tasks such as: Experimenting with different hyperparameters for a machine learning model. Sending a welcome email to new users immediately upon registration. Triggering a data cleaning process when new data is available. Setting up Manual Trigger By default, any job deployed on TrueFoundry is set up as a manual trigger job. You can initiate the job execution either manually through the user interface or programmatically using the Python SDK. To initiate job execution, either follow this guide for manual execution or refer to this guide for programmatic execution. Once you Run your job, Your job will start getting executed and enter the Running state. At this point, the platform will create and launch a pod associated with the Job Run to execute the job. Upon successful completion of your job, the Job Run will transition to the Finished status. Simultaneously, the pod that was created to execute the job will be automatically released, along with the resources they utilised. Schedule Triggers Schedule triggers are designed for tasks that demand recurring execution at specific intervals or on specific dates. This is suitable for tasks such as: Scrape data from a website every 3 hours to maintain an updated dataset. Retrain a machine learning model every month to improve its performance. Generate weekly reports every Monday at 9:30 AM. Setting up Schedule Trigger To set up a schedule trigger, you will need to specify the frequency and timing of job execution. TrueFoundry provides you with an option to configure this time interval using a cron string as described below. These types of jobs also called as cron jobs. \ud83d\udcd8 Note: A Cron Job can still be triggered manually using the UI or programmatically if required Specify the schedule for a cron job You need to use the cron string format to specify job schedule. The cron expression consists of five fields representing the time to execute a specified command. * * * * * | | | | | | | | | |___ day of week (0-6) (Sunday is 0) | | | |_____ month (1-12) | | |_______ day of month (1-31) | |_________ hour (0-23) |___________ minute (0-59) For example, 0 11 1 * * represents \"first day of every month at 11:00 AM\", or 30 9 * * 1 represents \"every Monday at 9:30 AM\" We can use a site like https://crontab.guru/ to tryout cron expression and get a human-readable description of the same. What if a job is still running until the next scheduled run? Say for example, you schedule a daily job which runs at midnight to process and store some data in an S3 bucket but for some reason it is still running even at next midnight. So it is possible that the previous run of the job hasn't been completed while it is already time for the job to run again as per schedule. What should happen in this case? Should we: Skip the new run and continue the current run Stop the ongoing run and start the new one, or Run both in parallel On TrueFoundry you can select the behaviour using something called concurrency policy based on your requirements and use case. The possible options are: Forbid : This is the default . Do not allow concurrent runs. Allow : Allow jobs to run concurrently. Replace : Replace the current job with the new one. Concurrency doesn't apply to manually triggered jobs. In that case, it always creates a new job run. Let's schedule your Job Again, we can do this using two different methods: Through the User Interface (UI) Using the Python SDK Scheduling your Job through the User Interface (UI) Scheduling your Job using the Python SDK deploy.py from truefoundry.deploy import Build, Job, PythonBuild, Schedule job = Job( image=Build( build_spec=PythonBuild( command=\"python train.py\", requirements_path=\"requirements.txt\" ) ), + trigger=Schedule( + schedule=\"0 8 1 * *\", + concurrency_policy=\"Forbid\" # Any one of [\"Forbid\", \"Allow\", \"Replace\"] + ) ) job.deploy(workspace_fqn=\"your-workspace-fqn) Once successfully deployed your cron job will run at the schedule you set for it. You can view the schedule of your job in the job-specific dashboard Updated 5 months ago",
    "https://docs.truefoundry.com/cdn-cgi/l/email-protection": "Email Protection | Cloudflare Please enable cookies. Email Protection You are unable to access this email address docs.truefoundry.com The website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots. You must enable Javascript in your browser in order to decode the e-mail address . If you have a website and are interested in protecting it in a similar way, you can sign up for Cloudflare . How does Cloudflare protect email addresses on website from spammers? Can I sign up for Cloudflare? Cloudflare Ray ID: 934b6ccfa9a27ee8 \u2022 Your IP: Click to reveal 14.194.94.198 \u2022 Performance & security by Cloudflare",
    "https://docs.truefoundry.com/docs/api-reference-1": "truefoundry.ml Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account truefoundry.ml All Pages Start typing to search\u2026 truefoundry.ml module TrueFoundry.ml Global Variables TRACKING_HOST_GLOBAL function get_client Initializes and returns the truefoundry client. Args: disable_analytics (bool, optional): To turn off usage analytics collection, pass True . By default, this is set to False . Returns: MLFoundry : Instance of MLFoundry class which represents a run . Examples: Get client Python import truefoundry.ml as tfm client = tfm.get_client() class MlFoundry MlFoundry. function create_data_directory Create DataDirectory to Upload the files Args: ml_repo (str): Name of the ML Repo in which you want to create data_directory name (str): Name of the DataDirectory to be created. description (str): Description of the Datset metadata (Dict <str> : Any): Metadata about the data_directory in Dictionary form. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.create_data_directory(name=\"<data_directory-name>\", ml_repo=\"<repo-name>\") print(data_directory.fqn) function create_ml_repo Creates an ML Repository. Args: name (str, optional): The name of the Repository you want to create. if not given, it creates a name by itself. storage_integration_fqn (str): The storage integration FQN to use for the experiment for saving artifacts. Examples: Create Repository Python import truefoundry.ml as tfm client = tfm.get_client() ml_repo = client.create_ml_repo(name=\"my-repo\", storage_integration_fqn=\"truefoundry:example:name:blob-storage:blob\") function create_run Initialize a run . In a machine learning experiment run represents a single experiment conducted under a project. Args: ml_repo (str): The name of the project under which the run will be created. ml_repo should only contain alphanumerics (a-z,A-Z,0-9) or hyphen (-). The user must have ADMIN or WRITE access to this project. run_name (Optional[str], optional): The name of the run. If not passed, a randomly generated name is assigned to the run. Under a project, all runs should have a unique name. If the passed run_name is already used under a project, the run_name will be de-duplicated by adding a suffix. run name should only contain alphanumerics (a-z,A-Z,0-9) or hyphen (-). tags (Optional[Dict[str, Any]], optional): Optional tags to attach with this run. Tags are key-value pairs. kwargs: Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: Create a run under current user. Python import truefoundry.ml as tfm client = tfm.get_client() tags = {\"model_type\": \"svm\"} run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\", tags=tags ) run.end() Creating a run using context manager. Python import truefoundry.ml as tfm client = tfm.get_client() with client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\" ) as run: # ... # Model training code ... # `run` will be automatically marked as `FINISHED` or `FAILED`. Create a run in a project owned by a different user. Python import truefoundry.ml as tfm client = tfm.get_client() tags = {\"model_type\": \"svm\"} run = client.create_run( ml_repo=\"my-classification-project\", run_name=\"svm-with-rbf-kernel\", tags=tags, ) run.end() function get_all_runs Returns all the run name and id present under a project. The user must have READ access to the project. Args: ml_repo (str): Name of the project. Returns: pd.DataFrame : dataframe with two columns- run_id and run_name Examples: get all the runs from a ml_repo Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_all_runs(ml_repo='my-repo') function get_artifact_version Get the model version to download contents or load it in memory Args: ml_repo (str): ML Repo to which artifact is logged artifact_name (str): Artifact Name artifact_type (str): The type of artifact to fetch (acceptable values: \"artifact\", \"model\", \"plot\", \"image\") version (str | int): Artifact Version to fetch (default is the latest version) Returns: ArtifactVersion : An ArtifactVersion instance of the artifact Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_artifact_version(ml_repo=\"ml-repo-name\", name=\"artifact-name\", version=1) # load the model into memory clf = model_version.load() # download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info) function get_artifact_version_by_fqn Get the artifact version to download contents Args: fqn (str): Fully qualified name of the artifact version. Returns: ArtifactVersion : An ArtifactVersion instance of the artifact Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() artifact_version = client.get_artifact_version_by_fqn( fqn=\"artifact:truefoundry/my-classification-project/sklearn-artifact:1\" ) # download the artifact to disk temp = tempfile.TemporaryDirectory() download_info = artifact_version.download(path=temp.name) print(download_info) function get_data_directory Get an existing data_directory by name . Args: ml_repo (str): name of an the project of which the data-directory is part of. name (str): the name of the data-directory Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory(ml_repo='my-repo', name=\"<data-directory-name>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_data_directory_by_fqn Get the DataDirectory by DataDirectory FQN Args: fqn (str): Fully qualified name of the artifact version. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_fqn(fqn=\"<data-dir-fqn>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_data_directory_by_id Get the DataDirectory From the DataDirectory ID Args: id (uuid.UUID): Id of the data_directory. Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directory = client.get_data_directory_by_id(id=\"<data_directory-id>\") with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") data_directory.add_files( artifact_paths=[tfm.DataDirectoryPath('artifact.txt', 'a/b/')] ) # print the path of files and folder in the data_directory for file in data_directory.list_files(): print(file.path) function get_model_version Get the model version to download contents or load it in memory Args: ml_repo (str): ML Repo to which model is logged name (str): Model Name version (str | int): Model Version to fetch (default is the latest version) Returns: ModelVersion : The ModelVersion instance of the model. Examples: Sklearn Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import tempfile import joblib import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", version=1 ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir, download_info.model_filename) # Deserialize and Load model = joblib.load( os.path.join(download_info.model_dir, download_info.model_filename) ) Huggingface Transformers Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import torch from transformers import pipeline import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version( ml_repo=\"my-llm-project\", name=\"my-transformers-model\", version=1 ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load pln = pipeline(\"text-generation\", model=download_info.model_dir, torch_dtype=torch.float16) function get_model_version_by_fqn Get the model version to download contents or load it in memory Args: fqn (str): Fully qualified name of the model version. Returns: ModelVersion : The ModelVersion instance of the model. Examples: Sklearn Python # See `truefoundry.ml.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import tempfile import joblib import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-classification-project/my-sklearn-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir, download_info.model_filename) # Deserialize and Load model = joblib.load( os.path.join(download_info.model_dir, download_info.model_filename) ) Huggingface Transformers Python # See `mlfoundry.mlfoundry_api.MlFoundry.log_model` examples to understand model logging import torch from transformers import pipeline import truefoundry.ml as tfm client = tfm.get_client() model_version = client.get_model_version_by_fqn( fqn=\"model:truefoundry/my-llm-project/my-transformers-model:1\" ) # Download the model to disk temp = tempfile.TemporaryDirectory() download_info = model_version.download(path=temp.name) print(download_info.model_dir) # Deserialize and Load pln = pipeline(\"text-generation\", model=download_info.model_dir, torch_dtype=torch.float16) function get_run_by_fqn Get an existing run by fqn . fqn stands for Fully Qualified Name. A run fqn has the following pattern: tenant_name/ml_repo/run_name If a run svm under the project cat-classifier in truefoundry tenant, the fqn will be truefoundry/cat-classifier/svm . Args: run_fqn (str): fqn of an existing run. Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: get run by run fqn Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_fqn(run_fqn='truefoundry/my-repo/svm') function get_run_by_id Get an existing run by the run_id . Args: run_id (str): run_id or fqn of an existing run . Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: Get run by the run id Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_id(run_id='a8f6dafd70aa4baf9437a33c52d7ee90') function get_run_by_name Get an existing run by run_name . Args: ml_repo (str): name of the ml_repo of which the run is part of. run_name (str): the name of the run required Returns: MlFoundryRun : An instance of MlFoundryRun class which represents a run . Examples: get run by name Python import truefoundry.ml as tfm client = tfm.get_client() run = client.get_run_by_name(run_name='svm', ml_repo='my-repo') function get_tracking_uri Get the current tracking URI. Returns: The tracking URI. Examples: Python import tempfile import truefoundry.ml as tfm client = tfm.get_client() tracking_uri = client.get_tracking_uri() print(\"Current tracking uri: {}\".format(tracking_uri)) function list_artifact_versions Get all the version of na artifact to download contents or load them in memory Args: ml_repo (str): Repository in which the model is stored. name (str): Name of the artifact whose version is required artifact_type (ArtifactType): Type of artifact you want for example model, image, etc. Returns: Iterator[ArtifactVersion] : An iterator that yields non deleted artifact-versions of an artifact under a given ml_repo sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() artifact_versions = client.list_artifact_versions(ml_repo=\"my-repo\", name=\"artifact-name\") for artifact_version in artifact_versions: print(artifact_version) function list_artifact_versions_by_fqn List versions for a given artifact Args: artifact_fqn : FQN of the Artifact to list versions for. An artifact_fqn looks like {artifact_type}` : {org}/{user}/{project}/{artifact_name}` or {artifact_type}` : {user}/{project}/{artifact_name}` where artifact_type can be on of (\"model\", \"image\", \"plot\") Returns: Iterator[ArtifactVersion] : An iterator that yields non deleted artifact versions under the given artifact_fqn sorted reverse by the version number Yields: ArtifactVersion : An instance of mlfoundry.ArtifactVersion Examples: Python import truefoundry.ml as tfm tfm.login(tracking_uri=https://your.truefoundry.site.com\") client = tfm.get_client() artifact_fqn = \"artifact:org/my-project/my-artifact\" for av in client.list_artifact_versions(artifact_fqn=artifact_fqn): print(av.name, av.version, av.description) function list_data_directories Get the list of DataDirectory in a ml_repo Args: ml_repo (str): Name of the ML Repository max_results (int): Maximum number of Data Directory to list offset (int): Skip these number of instance of DataDirectory and then give the result from these number onwards Returns: DataDirectory : An instance of class DataDirectory Examples: Python import truefoundry.ml as tfm client = tfm.get_client() data_directories = client.list_data_directories(ml_repo=\"<ml-repo-nam>\") for data_directory in data_directories: print(data_directory.name) function list_ml_repos Returns a list of names of ML Repos accessible by the current user. Returns: List[str] : A list of names of ML Repos function list_model_versions Get all the version of a model to download contents or load them in memory Args: ml_repo (str): Repository in which the model is stored. name (str): Name of the model whose version is required Returns: Iterator[ModelVersion] : An iterator that yields non deleted model versions of a model under a given ml_repo sorted reverse by the version number Examples: Python import truefoundry.ml as tfm client = tfm.get_client() model_versions = client.list_model_version(ml_repo=\"my-repo\", name=\"svm\") for model_version in model_versions: print(model_version) function list_model_versions_by_fqn List versions for a given model Args: model_fqn : FQN of the Model to list versions for. A model_fqn looks like model` : {org}/{user}/{project}/{artifact_name}` or model` : {user}/{project}/{artifact_name}` Returns: Iterator[ModelVersion] : An iterator that yields non deleted model versions under the given model_fqn sorted reverse by the version number Yields: ModelVersion : An instance of mlfoundry.ModelVersion Examples: Python import truefoundry.ml as tfm mlfoundry.login(tracking_uri=\"https://your.truefoundry.site.com\") client = tfm.get_client() model_fqn = \"model:org/my-project/my-model\" for mv in client.list_model_versions(model_fqn=model_fqn): print(mv.name, mv.version, mv.description) function log_artifact Logs an artifact for the current ml_repo . An artifact is a list of local files and directories. This function packs the mentioned files and directories in artifact_paths and uploads them to remote storage linked to the ml_repo Args: ml_repo (str): Name of the ML Repo to which an artifact is to be logged. name (str): Name of the Artifact. If an artifact with this name already exists under the current ml_repo, the logged artifact will be added as a new version under that name . If no artifact exist with the given name , the given artifact will be logged as version 1. artifact_paths (List[truefoundry.ml.ArtifactPath], optional): A list of pairs of (source path, destination path) to add files and folders to the artifact version contents. The first member of the pair should be a file or directory path and the second member should be the path inside the artifact contents to upload to. E.g. >>> client.log_artifact( ... ml_repo=\"sample-repo\", ... name=\"xyz\", ... artifact_paths=[ tfm.ArtifactPath(\"foo.txt\", \"foo/bar/foo.txt\"), tfm.ArtifactPath(\"tokenizer/\", \"foo/tokenizer/\"), tfm.ArtifactPath('bar.text'), ('bar.txt', ), ('foo.txt', 'a/foo.txt') ] ... ) would result in . \u2514\u2500\u2500 foo/ \u251c\u2500\u2500 bar/ \u2502 \u2514\u2500\u2500 foo.txt \u2514\u2500\u2500 tokenizer/ \u2514\u2500\u2500 # contents of tokenizer/ directory will be uploaded here description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None Returns: truefoundry.ml.ArtifactVersion : an instance of ArtifactVersion that can be used to download the files, or update attributes like description, metadata. Examples: Python import os import truefoundry.ml as tfm with open(\"artifact.txt\", \"w\") as f: f.write(\"hello-world\") client = tfm.get_client() ml_repo = \"sample-repo\" client.create_ml_repo(ml_repo=ml_repo) client.log_artifact( ml_repo=ml_repo, name=\"hello-world-file\", artifact_paths=[tfm.ArtifactPath('artifact.txt', 'a/b/')] ) function log_model Serialize and log a versioned model under the current ml_repo. Each logged model generates a new version associated with the given name and linked to the current run. Multiple versions of the model can be logged as separate versions under the same name . Args: ml_repo (str): Name of the ML Repo to which an artifact is to be logged. name (str): Name of the model. If a model with this name already exists under the current ML Repo, the logged model will be added as a new version under that name . If no models exist with the given name , the given model will be logged as version 1. model_file_or_folder (str): Path to either a single file or a folder containing model files. This folder is usually created using serialization methods of libraries or frameworks e.g. joblib.dump , model.save_pretrained(...) , torch.save(...) , model.save(...) framework (Union[enums.ModelFramework, str]): Model Framework. Ex:- pytorch, sklearn, tensorflow etc. The full list of supported frameworks can be found in mlfoundry.enums.ModelFramework . Can also be None when model is None . description (Optional[str], optional): arbitrary text upto 1024 characters to store as description. This field can be updated at any time after logging. Defaults to None metadata (Optional[Dict[str, Any]], optional): arbitrary json serializable dictionary to store metadata. For example, you can use this to store metrics, params, notes. This field can be updated at any time after logging. Defaults to None Returns: truefoundry.ml.ModelVersion : an instance of ModelVersion that can be used to download the files, load the model, or update attributes like description, metadata, schema. Examples: Sklearn Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import joblib import numpy as np from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) y = np.array([1, 1, 2, 2]) clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) clf.fit(X, y) joblib.dump(clf, \"sklearn-pipeline.joblib\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-classification-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) model_version = client.log_model( ml_repo=\"my-classification-project\", name=\"my-sklearn-model\", model_file_or_folder=\"sklearn-pipeline.joblib\", framework=ModelFramework.SKLEARN, metadata={\"accuracy\": 0.99, \"f1\": 0.80}, step=1, # step number, useful when using iterative algorithms like SGD ) print(model_version.fqn) Huggingface Transformers Python import truefoundry.ml as tfm from truefoundry.ml import ModelFramework import torch from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM pln = pipeline( \"text-generation\", model_file_or_folder=\"EleutherAI/pythia-70m\", tokenizer=\"EleutherAI/pythia-70m\", torch_dtype=torch.float16 ) pln.model.save_pretrained(\"my-transformers-model\") pln.tokenizer.save_pretrained(\"my-transformers-model\") client = tfm.get_client() client.create_ml_repo( # This is only required once ml_repo=\"my-llm-project\", # This controls which bucket is used. # You can get this from Integrations > Blob Storage. `None` picks the default storage_integration_fqn=None ) model_version = client.log_model( ml_repo=\"my-llm-project\", name=\"my-transformers-model\", model_file_or_folder=\"my-transformers-model/\", framework=ModelFramework.TRANSFORMERS ) print(model_version.fqn) function search_runs The user must have READ access to the project. Returns an iterator that returns a MLFoundryRun on each next call. All the runs under a project which matches the filter string and the run_view_type are returned. Args: ml_repo (str): Name of the project. filter_string (str, optional): Filter query string, defaults to searching all runs. Identifier required in the LHS of a search expression. Signifies an entity to compare against. An identifier has two parts separated by a period : the type of the entity and the name of the entity. The type of the entity is metrics, params, attributes, or tags. The entity name can contain alphanumeric characters and special characters. You can search using two run attributes : status and artifact_uri. Both attributes have string values. When a metric, parameter, or tag name contains a special character like hyphen, space, period, and so on, enclose the entity name in double quotes or backticks, params.\"model-type\" or params. model-type run_view_type (str, optional): one of the following values \"ACTIVE_ONLY\", \"DELETED_ONLY\", or \"ALL\" runs. order_by (List[str], optional): List of columns to order by (e.g., \"metrics.rmse\"). Currently supported values are metric.key, parameter.key, tag.key, attribute.key. The order_by column can contain an optional DESC or ASC value. The default is ASC . The default ordering is to sort by start_time DESC . job_run_name (str): Name of the job which are associated with the runs to get that runs max_results (int): max_results on the total numbers of run yielded through filter Returns: Iterator[MlFoundryRun] : MLFoundryRuns matching the search query. Examples: Python import truefoundry.ml as tfm client = tfm.get_client() with client.create_run(ml_repo=\"my-project\", run_name=\"run-1\") as run1: run1.log_metrics(metric_dict={\"accuracy\": 0.74, \"loss\": 0.6}) run1.log_params({\"model\": \"LogisticRegression\", \"lambda\": \"0.001\"}) with client.create_run(ml_repo=\"my-project\", run_name=\"run-2\") as run2: run2.log_metrics(metric_dict={\"accuracy\": 0.8, \"loss\": 0.4}) run2.log_params({\"model\": \"SVM\"}) # Search for the subset of runs with logged accuracy metric greater than 0.75 filter_string = \"metrics.accuracy > 0.75\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 filter_string = \"metrics.accuracy > 0.7\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 and model=\"LogisticRegression\" filter_string = \"metrics.accuracy > 0.7 and params.model = 'LogisticRegression'\" runs = client.search_runs(ml_repo=\"my-project\", filter_string=filter_string) # Search for the subset of runs with logged accuracy metric greater than 0.7 and # order by accuracy in Descending order filter_string = \"metrics.accuracy > 0.7\" order_by = [\"metric.accuracy DESC\"] runs = client.search_runs( ml_repo=\"my-project\", filter_string=filter_string, order_by=order_by ) filter_string = \"metrics.accuracy > 0.7\" runs = client.search_runs( ml_repo=\"transformers\", order_by=order_by ,job_run_name='job_run_name', filter_string=filter_string ) order_by = [\"metric.accuracy DESC\"] runs = client.search_runs( ml_repo=\"my-project\", filter_string=filter_string, order_by=order_by, max_results=10 ) Updated 4 months ago",
    "https://docs.truefoundry.com/docs/guides": "Creating a workflow with different container images Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating a workflow with different container images All Pages Start typing to search\u2026 Creating a workflow with different container images This guide helps you to create a workflow with tasks that have different task configs and different container applications. This can have variety of applications like keeping different configuration (resources and container image) for steps requiring GPUs and steps not requiring GPUs In this example, We will write a simple workflow to get the latest price of Bitcoin, Ethereum, and Litecoin, the main aim of this example is to understand the different types of configs. we will create two configs, one for GPU and one for CPU like this. Prerequsite Before you proceed with the guide, make sure you have the following: Truefoundry CLI : Set up and configure the TrueFoundry CLI tool on your local machine by following the Setup for CLI guide. Workspace : To deploy your workflow, you'll need a workspace. If you don't have one, you can create it using this guide: Creating a Workspace or seek assistance from your cluster administrator. Creating the workflow Create a workflow.py where we will write the code for our workflow and place in the project root folder. (With other dependent files and requirements.txt) workflow.py workflow.py from truefoundry.workflow import ( task, workflow, PythonTaskConfig, TaskPythonBuild, ) from truefoundry.deploy import Resources, NvidiaGPU import requests import json cpu_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.8\"], # requirements_path=\"requirements.txt\" ), resources=Resources(cpu_request=0.5) ) gpu_config = PythonTaskConfig( image=TaskPythonBuild( python_version=\"3.9\", pip_packages=[\"truefoundry[workflow]==0.4.8\", \"pynvml==11.5.0\"], # requirements_path=\"requirements.txt\", cuda_version=\"11.5-cudnn8\", ), env={ \"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility\", \"NVIDIA_VISIBLE_DEVICES\": \"all\", }, resources=Resources(cpu_request=1.5, devices=[NvidiaGPU(name=\"T4\", count=1)]) ) # Python Task: Fetch real-time prices for multiple cryptocurrencies from the CoinGecko API @task(task_config=cpu_config) def fetch_crypto_data() -> str: response = requests.get( \"https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum,litecoin&vs_currencies=usd\" ) return response.text # Python Task: Process the data to extract prices @task(task_config=gpu_config) def extract_prices(data: str) -> dict: from pynvml import nvmlDeviceGetCount, nvmlInit nvmlInit() assert nvmlDeviceGetCount() > 0 json_data = json.loads(data) prices = { \"bitcoin\": json_data[\"bitcoin\"][\"usd\"], \"ethereum\": json_data[\"ethereum\"][\"usd\"], \"litecoin\": json_data[\"litecoin\"][\"usd\"], } return prices # Workflow: Combine all tasks @workflow def crypto_workflow() -> dict: data = fetch_crypto_data() prices = extract_prices(data=data) return prices As you can see for the fetch_crypto_data function we have defined the resource which does not use GPU whereas task extract_prices uses GPU and hence we have defined the env variable and we have also used the pynvml package to check whether GPU is present or not by asserting the condition that assert nvmlDeviceGetCount() > 0 . Now run the below command in the terminal to deploy your workflow, replace <workfspace-fqn> with the workspace fqn which you can find on the UI. Shell tfy deploy workflow \\ --name multi-image-workflow \\ --file workflow.py \\ --workspace_fqn \"Paste your workspace FQN here\" Updated 5 months ago",
    "https://docs.truefoundry.com/docs/adding-models-to-llm-gateway": "OpenAI, Anthropic, DeepSeek and More Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account OpenAI, Anthropic, DeepSeek and More All Pages Start typing to search\u2026 OpenAI, Anthropic, DeepSeek and More TrueFoundry offers a secure and efficient gateway to seamlessly integrate various Large Language Models (LLMs) into your applications, including models hosted on OpenAI, DeepSeek, Anthropic, AWS Bedrock, Google Vertex, Azure OpenAI and many more. View Full List Providers and Models Before we start integrating LLMs, let's quickly understand the concept of a Provider and a Model: Provider - A provider is an organization or company that supplies access to various AI language models. These providers offer multiple models, each varying in performance, features, and pricing. Prominent examples of such providers include OpenAI and Anthropic, which deliver a range of models designed to meet different needs. Model - A model refers to a specific implementation of an AI language model made available by a provider. Each model is tailored with unique features and trained on distinct datasets, which influences its performance and suitability for various tasks. For example, OpenAI's GPT-3 and GPT-4 are different models, each offering unique capabilities to cater to diverse applications. Steps to Add OpenAI Models to Gateway This section outlines the process of adding OpenAI models and configuring the necessary access controls. Access Control In the example above, all users have access to every model. However, access control can be configured at the individual model level. Adding access control through the UI When adding a model, you have the option to configure which users and teams within the organization are permitted to access that model. This ensures that only authorized personnel can use specific models based on your access control settings. You can also give model access to a Virtual Account Read more here Tryout your Integrated Models Once you've added the providers and models through the integrations, you're ready to test them out. Try in the Playground To test your models in the playground, simply visit the playground interface and select the model from the available models dropdown. You can search for your models by the provider account name or model name, as shown below: Try Programatically TrueFoundry provides several options for testing models programmatically, including Curl, REST APIs, and platform libraries such as OpenAI, Langchain, and more. To access code snippets for integration: Visit Gateway. Select a model. Click the \"View Code\" button to see auto-generated code snippets for different libraries, REST APIs, or simple Curl requests, as shown below. To make a request, you'll need the BASE_URL and API_KEY . The BASE_URL is pre-filled in the auto-generated code snippets. For instructions on generating a new API_KEY , refer to the API Key Documentation . Add LLMs from Anthropic, DeepSeek, Ollama and more The process is pretty much the same. Follow these steps - Navigate to Integrations. Click on \"Add New Integration\". Select the provider from the list. Enter the required API keys and access credentials. Once completed, your models will be ready for use in both the playground and your code. Updated 26 days ago",
    "https://docs.truefoundry.com/docs/llm-playground": "LLM Playgound Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account LLM Playgound All Pages Start typing to search\u2026 LLM Playgound The LLM Playground is a UI for the LLM Gateway where you can tryout different models you've added from across providers like OpenAI, Mistral, Cohere etc. Below is an overview of the features: Support for multiple model types Chat Models Embedding Models Rerank Models Realtime Models Image Upload : Upload images for image captioning or visual question answering. This is only available for models that support images such as GPT-4o. Model Comparison : Compare responses from different completion models to evaluate their performance. System Prompts: Use predefined system prompts to guide model behaviour. System prompt inform how the model should respond. Sample system prompt - Be clear, concise, and polite in your responses. Avoid sharing any sensitive or personal information. Updated 20 days ago",
    "https://docs.truefoundry.com/docs/configure-gateway": "Access Control Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Access Control All Pages Start typing to search\u2026 Access Control Control access of models among teams, users and applications You can add models to LLM Gateway by adding provider accounts like OpenAI, Anthropic, Bedrock etc through the Integrations page. Each model provider can have multiple models within and you can configure access control at the model level. Manage access to models for users and teams You can grant access to users and teams via the integration form as shown in the following demo: To get access to models, users can generate a personal access tokens from the Settings page and use it in the API code. \ud83d\udcd8 When you provide access to a user, all their Private Access Tokens(PATs) get access to the model. Manage access to models for Virtual Accounts When applications are using LLMs via the gateway, its better to get the keys via virtual accounts. Virtual accounts are not tied to a user and hence remain valid even if the employee leaves the company. You can grant access to virtual account via the virtual account form as shown in the following demo: Updated 26 days ago",
    "https://docs.truefoundry.com/docs/observability": "Tracing Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Tracing All Pages Start typing to search\u2026 Tracing TrueFoundry offers robust monitoring for applications by collecting traces, metrics, and logs to deliver real-time insights into performance, efficiency, and cost. At its core, TrueFoundry has built its own tracing solution based on the OpenTelemetry (OTel) standard, ensuring compatibility and flexibility. While TrueFoundry does not rely on Traceloop, we recommend using the Traceloop client SDK to push traces, as it supports a wide range of modes and provides a seamless developer experience for AI and LLM-based applications. Key Concepts Trace A trace would represent the complete lifecycle of a request or task as it flows through the various services and components that interact with the LLM. This could involve multiple stages, such as receiving input from the user, sending the input to the model for inference, processing the model\u2019s output, and delivering the result back to the user. The trace provides a holistic view of how the LLM application processes the request and where any delays or issues may arise in the overall flow, whether it's the input, the model's computation, or the output handling. Span A span represents an individual unit of work or operation that occurs within the trace. In the context of an LLM application, spans are used to capture each distinct task or action that is performed during the lifecycle of a request. Each span provides granular insights into specific stages of the request lifecycle, and together they allow you to understand not only how the LLM system performs overall but also where performance optimizations or troubleshooting may be needed in individual components or operations. For Example: Imagine a user queries an LLM for a recommendation. The trace would look something like this: Trace: Tracks the entire user request from input to response. Span 1: Input preprocessing (parsing and tokenizing the user query). Span 2: Model inference (running the LLM to generate a recommendation). Span 3: Post-processing (formatting the recommendation output). Span 4: Output delivery (returning the recommendation to the user). By using traces and spans in LLM applications, you gain end-to-end visibility of how well your LLM-based system is performing, where the bottlenecks might be, and how to improve the efficiency and responsiveness of the system. TrueFoundry's Tracing UI Getting Started [Supported Providers & Frameworks] If you are using any of the supported Providers and Frameworks then adding tracing is pretty simple. You just need to install, import and initialise the Traceloop SDK. It will automatically log traces your requests. Say for example you are using OpenAI client the follow these steps: lnstall dependencies: First, you need to install the following pip install traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here OpenAI Completion Example Python from openai import OpenAI #Import Traceloop SDK from traceloop.sdk import Traceloop api_key = \"Enter your API Key here\" #Initialise the Traceloop SDK Traceloop.init( app_name=\"openai-example\", disable_batch=True, #This is recommended, only if you are trying this in local machine. This helps to push the traces immediately, without waiting for batching ) client = OpenAI(api_key=api_key, base_url=\"https://internal.devtest.truefoundry.tech/api/llm/api/inference/openai\") stream = client.chat.completions.create( messages = [ {\"role\": \"system\", \"content\": \"You are an AI bot.\"}, {\"role\": \"user\", \"content\": \"Enter your prompt here\"}, ], model= \"openai-main/gpt-4o\", stream=True, temperature=0.7, max_tokens=256, top_p=0.8, frequency_penalty=0, presence_penalty=0, stop=[\"</s>\"], extra_headers={ \"X-TFY-METADATA\": '{\"tfy_log_request\":\"true\"}' } ) for chunk in stream: if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Logged trace on TrueFoundry UI: If you want are using any of the following frameworks then follow these: CrewAI LangGraph Agno Getting Started [Non-Supported Providers & Frameworks] Can you still log traces on TrueFoundry even if you are not using any of the supported provider or framework? YES! Follow these steps and get instant monitoring: lnstall dependencies: First, you need to install the following pip install traceloop-sdk==0.38.12 Setup environment variables: Add the necessary environment variables to enable tracing OPENAI_API_KEY=sk-proj-* TRACELOOP_BASE_URL=<<control-plane-url>>/api/otel TRACELOOP_HEADERS=\"Authorization=Bearer <<api-key>> Generate API key from here Initialise Traceloop In your LLM app, initialize the Traceloop tracer like this: Python from traceloop.sdk import Traceloop Traceloop.init() Traceloop.init(disable_batch=True) #This is recommended, only if you are trying this in local machine. This helps to push the traces immediately, without waiting for batching Annotate your Workflows, Agents and Tools For complex workflows or chains, annotating them can help you gain better insights into their operations. With TrueFoundry, you can view the entire trace of your workflow for a comprehensive understanding. Traceloop offers a set of decorators to simplify this process. For instance, if you have a function that renders a prompt and calls an LLM, you can easily add the @workflow decorator to track and annotate the workflow. Let's say your workflow calls more functions you can annotate them as @task Python from openai import OpenAI from traceloop.sdk.decorators import workflow, task client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) @task(name=\"joke_creation\") def create_joke(): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Tell me a joke about opentelemetry\"}], ) return completion.choices[0].message.content @task(name=\"signature_generation\") def generate_signature(joke: str): completion = openai.Completion.create( model=\"davinci-002\",[] prompt=\"add a signature to the joke:\\n\\n\" + joke, ) return completion.choices[0].text @workflow(name=\"pirate_joke_generator\") def joke_workflow(): eng_joke = create_joke() pirate_joke = translate_joke_to_pirate(eng_joke) signature = generate_signature(pirate_joke) print(pirate_joke + \"\\n\\n\" + signature) Similarly, when working with autonomous agents, you can use the @agent decorator to trace the agent as a single unit. Additionally, each individual tool within the agent should be annotated with the @tool decorator. Python from openai import OpenAI from traceloop.sdk.decorators import agent, tool client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) @agent(name=\"joke_translation\") def translate_joke_to_pirate(joke: str): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"Translate the below joke to pirate-like english:\\n\\n{joke}\"}], ) history_jokes_tool() return completion.choices[0].message.content @tool(name=\"history_jokes\") def history_jokes_tool(): completion = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": f\"get some history jokes\"}], ) return completion.choices[0].message.content Updated 16 days ago",
    "https://docs.truefoundry.com/docs/configure-async-service": "Configure Ports Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Configure Ports All Pages Start typing to search\u2026 Configure Ports If you are using Async Service via the library route, you need not have any ports exposed in your service. However, if you are using HTTP service along with a sidecar, you need to provide a port on your HTTP service on which the sidecar will send the request. You need not expose this port externally since the sidecar calls the HTTP service calls it via the localhost url. Updated 5 months ago",
    "https://docs.truefoundry.com/docs/creating-a-queue": "AWS SQS Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account AWS SQS All Pages Start typing to search\u2026 AWS SQS Provisioning the queue Creating an AWS Simple Queue Service (SQS) queue is a straightforward process that can be accomplished using the AWS Management Console or AWS Command Line Interface (CLI). Here's a step-by-step guide for creating an AWS SQS queue through the AWS Management Console: \u2757\ufe0f Note: The visibility timeout has a significant impact on the behavior of asynchronous services. It determines how long a message remains hidden from consumers after it's fetched from the queue, playing a vital role in preventing multiple consumers from processing the same message concurrently. For example, if your worker process typically takes around 5 seconds to complete a task, it's advisable to set the Visibility Timeout to at least twice that duration, which in this case would be 10 seconds. If the Visibility Timeout is set too low, there's a risk of multiple consumers attempting to process the same message simultaneously, potentially leading to conflicts and errors in your system. It's essential to strike the right balance to ensure efficient and orderly message processing. Once you click on Create queue , you'll receive a confirmation message indicating the successful creation of the queue. Configuring Truefoundry Async Service with AWS SQS You will have to specify these configurations for AWS SQS Input Worker: Configuring Autoscaling for AWS SQS Queue AWS SQS Average Backlog is defined as the AWS SQS pending queue length averaged over all replicas that the autoscale is designed to maintain. The pending queue length refers to the number of messages that are currently in the queue but have not yet been processed. These are messages waiting to be consumed and processed by the relevant workers or services. The average backlog is the average or mean value of the pending queue length across multiple replicas. In a distributed and auto-scaling system, there can be multiple instances or replicas of your service, each with its queue. The average backlog provides a way to measure the workload across all replicas. This Average Backlog is a valuable metric for determining how to scale your application efficiently. \ud83d\udcd8 Note: This metric is only available in case you are using AWS SQS for your input queue Parameters for SQS Average Backlog Queue lag threshold : This is the maximum number of messages each replica should handle. If there are more messages than the threshold, the auto-scaler adds replicas to share the workload. Configuring AWS SQS Average Backlog Through the User Interface (UI) Via the Python SDK In your Service deployment code deploy.py , include the following: Diff from truefoundry.deploy import AsynceService, Build, DockerFileBuild, Port, AsyncServiceAutoscaling,SQSQueueMetricConfig service = AsyncService( name=\"my-async-service\", image=Build(build_spec=DockerFileBuild()), ports=[ Port( host=\"your_host\", port=8501 ) ] + replicas = AsyncServiceAutoscaling( + min_replicas=1, + max_replicas=3, + metrics=SQSQueueMetricConfig( + queue_length=30 + ), + cooldown_period=300, + polling_interval=30 + ) ) service.deploy(workspace_fqn=\"YOUR_WORKSPACE_FQN\") Updated 5 months ago",
    "https://docs.truefoundry.com/docs/gcp": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on GCP Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. You can find an architectural overview for a Truefoundry enabled GKE cluster here Scenarios Following scenarios are supported in the provided terraform code. You can find the requirements for each scenario here : New VPC + New GKE cluster - This is the simplest setup. The Truefoundry terraform code takes care of spinning up and setting up everything. Make sure your cloud account is ready with the requirements mentioned here Existing VPC + New GKE cluster - In this setup, you come with your own VPC and truefoundry terraform code takes care of creating the cluster in the same VPC. Do make sure to adhere to the existing VPC related requirements mentioned here Existing GKE cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing VPC and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Create New Cluster or Attach Existing Cluster for GCP GKE depending on your use case You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the new cluster to be created. Fill in with your cluster details. Click Submit when done (For existing cluster) Disable the addons that are already installed on your cluster so that truefoundry installation does not interfere with your existing workloads You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated 3 months ago",
    "https://docs.truefoundry.com/docs/generic-1": "Overview Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Overview All Pages Start typing to search\u2026 Overview This page provides an overview of the architecture overview, requirements and steps to setup a Truefoundry compute plane cluster on a Generic cluster Truefoundry installation can be performed from the UI using a script that can be downloaded directly from your control plane cluster page after tenant registration or from your own control plane. This script leverages terraform internally to setup your cloud for Truefoundry. Scenarios Following scenario is supported in the provided terraform code. You can find the requirements for the scenario here : Existing Network + Existing cluster - In this setup, the Truefoundry terraform code reuses the cluster created by you to setup all the integrations needed for the platform to work. Do make sure to adhere to the existing network and existing cluster related requirements mentioned here Steps Go to the clusters page on the control plane and click on Attach Existing Cluster You will see the requirements needed to be setup in order to execute the following steps. Click on Continue A form will be presented with the details for the cluster to be onboarded. Fill in with your cluster details. Click Submit when done Disable the addons which are already present in your cluster You will be presented with a curl command to download and execute the script. The script will take care of installing the pre-requisites, downloading terraform code and running it on your local machine to set things up for Truefoundry. Once you are done with the installation, you can setup DNS and TLS for deploying workloads to your cluster by following here Now you are ready to start deploying workloads on your cluster. You can start by going here Updated about 2 months ago",
    "https://docs.truefoundry.com/docs/advanced-configuration": "LoadBalancer Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account LoadBalancer All Pages Start typing to search\u2026 LoadBalancer Truefoundry by default provisions a single external load-balancer for one Kubernetes cluster. This is provisioned automatically by the tfy-istio-ingress helm chart installed by Truefoundry which creates a Kubernetes service of type LoadBalancer . You can find the configuration of this service in Deployments > Helm > tfy-istio-ingress (Make sure you are filtering for the desired cluster) You can click on the three dots to understand the configuration. If you want to modify any of your load-balancer settings, you will have to edit this configuration according to the guide mentioned below. Modifying your load balancer configuration The loadbalancer can be modified using annotations on the gateway object. Below is an example of possible modifications that you can do. To get a complete list of annoations - check here AWS YAML gateway: annotations: # Denotes that this is a network load balancer. We use NLB so that TCP protocol can also be # supported. Also, NLB has lower latency and costs than ALB (Application Load Balancer) service.beta.kubernetes.io/aws-load-balancer-type: nlb # This is the default setting - It makes the loadbalancer external. If you want to create an internal # load-balancer, you can remove this annotations service.beta.kubernetes.io/aws-load-balancer-type: \"external\" service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\" # Use this only when you want to make the loadbalancer internal. Else remove this annotation service.beta.kubernetes.io/aws-load-balancer-internal: \"true\" # ACM cert arn to attach to the load balancer. If you have your own custom certificate, then you can remove this annotation. service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-2:XXXXXXX:certificate/XXXXX-XXXXX-XXXXXX # You can let these settings as it is service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https service.beta.kubernetes.io/aws-load-balancer-alpn-policy: HTTP2Preferred service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\" tfyGateway: name: tfy-wildcard spec: servers: - tls: httpsRedirect: true port: name: http-tfy-wildcard number: 80 protocol: HTTP hosts: - \"*\" - port: name: https-tfy-wildcard number: 443 protocol: HTTP hosts: - \"*\" selector: istio: tfy-istio-ingress As mentioned above, if you want the loadbalancer to be external, add the following annotation: YAML service.beta.kubernetes.io/aws-load-balancer-type: \"external\" service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing If you want it to be internal, the following annotation should be added: YAML service.beta.kubernetes.io/aws-load-balancer-scheme: \"internal\" If you convert the load balancer from internal to external or vice-versa, the loadbalancer will be recreated and you will have to remap your DNS. GCP The default configuration creates an external load-balancer: YAML gateway: tolerations: - key: \"cloud.google.com/gke-spot\" value: \"true\" effect: NoSchedule operator: Equal affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: cloud.google.com/gke-spot values: - \"true\" operator: In tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP To make the loadbalancer internal, add the annotation as follows: YAML gateway: annotations: networking.gke.io/load-balancer-type: \"Internal\" Azure The default configuration creates an external load balancer with the below configuration YAML gateway: tolerations: - key: CriticalAddonsOnly value: \"true\" effect: NoSchedule operator: Equal tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP To make the load balancer internal YAML gateway: annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" Generic By default load balancer will be created without an external IP address in the case of generic cluster. Run metalLB to assign an IP address to the load balancer. Once the load balancer gets the IP address assign the certificates for it to terminate TLS traffic. For that you can use cert-manager or bring your own certificates. Cert-manager is present as an add-on in the cluster, so you can manage and edit the configurations for it vey seamlessly through the platform itself. cert-manager connects with your DNS providers and creates a secret in the k8s cluster. If you bring your own certificates, they need to be in the form of k8s secret. Check how you can create tls secret in k8s. Secret should be created in the istio-system namespace. Passing those secrets in the tfy-istio-ingress in tfyGateway.spec.servers[1].tls.credentialName YAML tfyGateway: name: 'tfy-wildcard' spec: selector: istio: 'tfy-istio-ingress' servers: - hosts: - \"*\" port: name: http-tfy-wildcard number: 80 protocol: HTTP tls: httpsRedirect: true - hosts: - \"*\" port: name: https-tfy-wildcard number: 443 protocol: HTTP tls: mode: SIMPLE credentialName: <> Deploy multiple load balancers Each installation of tfy-istio-ingress creates a load balancer. If you want to deploy multiple multiple load-balancers, for e.g. one internal and one external, you can clone the current tfy-istio-ingress application in the same namespace istio-system , change the tfyGateway.Name to something else other then default tfy-wildcard and update the tfyGateway.spec.Selector with the new name of the application. For e.g. if you clone the tfy-istio-ingress a new application with the name tfy-istio-ingress-1 will be created , update the tfyGateway.Name to a new name and the tfyGateway.spec.Selector to YAML tfyGateway: spec: selector: app: \"tfy-istio-ingress-1\" Once the ingress is installed, it will automatically create another loadbalancer whose IP you can get using kubectl get svc -n istio-system . Add authentication to all services behind a load balancer We can configure Istio to apply authentication at a gateway level. This will work only if you are accessing the service using the DNS provided in Istio and not access the service directly from within the cluster. This process is a bit complicated, and you should only do this if you really want to enable authentication at an istio gateway level. \ud83d\udcd8 Istio will validate if the JWT is valid. If not valid, it will return an Unauthorized Error. Create a RequestAuthentication resource to ensure that the JWT issuer and Audience are correct. Authentication will be only done if there is an Authorization header. This is pass-through if no Authorization header is present in the Request or it gets an empty string after removing the prefix. apiVersion: security.istio.io/v1beta1 kind: RequestAuthentication metadata: name: tfy-oauth2 namespace: istio-system spec: selector: matchLabels: istio: tfy-istio-ingress jwtRules: - issuer: \"truefoundry.com\" fromHeaders: - name: Authorization prefix: \"Bearer \" audiences: - <tenant_name_in_truefoundry> jwksUri: https://login.truefoundry.com/.well-known/jwks.json forwardOriginalToken: true Create an AuthorizationPolicy that will reject any requests with an empty JWT. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: tfy-oauth2 namespace: istio-system spec: selector: matchLabels: istio: tfy-istio-ingress action: DENY rules: - from: - source: notRequestPrincipals: [\"*\"] to: - operation: ports: - \"443\" You can read the Istio docs: https://istio.io/latest/docs/reference/config/security/request_authentication/ for further customization or making it work with your own IdP. Updated 7 days ago",
    "https://docs.truefoundry.com/docs/azure-aks": "Creating an AKS cluster using azure-cli Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Creating an AKS cluster using azure-cli All Pages Start typing to search\u2026 Creating an AKS cluster using azure-cli Creation of kubernetes cluster on Azure Using Azure CLI 1. Install Azure CLI You can check Installation of Azure CLI from here for your preferred workstation - https://learn.microsoft.com/en-us/cli/azure/install-azure-cli . Example for MacOS Shell brew update && brew install azure-cli # confirm the CLI version az version { \"azure-cli\": \"2.50.0\", \"azure-cli-core\": \"2.50.0\", \"azure-cli-telemetry\": \"1.0.8\", \"extensions\": {} } 2. Log In to Azure You can use multiple methods to login through the Azure CLI Shell # browser based login az login # with username and password az login --user <user> --password <pass> # login with the tenant az login --tenant <tenantID> # check Azure login help for other methods to log in az login --help 3. Get the necessary details for the next steps Subscription details - Check if the current subscription you want is correct or not Shell az account show Region - Check the region where you want to deploy your k8s cluster. Also make sure that GPU workloads are available in your region. You can check types of GPU instances and Availability of GPU instances in your specific region. To get a list of all regions Shell az account list-locations -o table Make sure to use the second column as the region name in the above output. Resource group - A new resource group is recommended. However an existing resource group can also be used. To create a new resource with some tags, execute the command in step 5. 4. Export all the necessary details into a variable Shell ## subscription details export SUBSCRIPTION_ID=\"\" export SUBSCRIPTION_NAME=\"\" # location export LOCATION=\"\" # resource group export RESOURCE_GROUP=\"\" # name of the user assigned identity (step 6) export USER_ASSIGNED_IDENTITY=\"\" # name of the cluster export CLUSTER_NAME=\"\" Set the subscription ID for all the below steps and add the aks-preview extension. Shell az account set --subscription $SUBSCRIPTION_ID az extension add --name aks-preview az extension update --name aks-preview 5. Creating Resource group All the Azure resources (mostly) are deployed in some resource group. For our AKS cluster we will create a resource group. We are naming it as tfy-datascience but feel free to name it according to your preferred naming conventions. We are creating two tags team=datascience and owner=truefoundry Shell RESOURCE_GROUP_ID=$(az group create --name $RESOURCE_GROUP \\ --location $LOCATION \\ --tags team=datascience owner=truefoundry \\ --query 'id' --output tsv) 6. Create user assigned identity To authenticate to AKS cluster post-creation we need to create a user-assigned identity. Managed Identity is the way to authenticate to Azure resource (AKS here) using Azure AD. There are two kinds of managed identities and we will use user-assigned identities among them. Copy the unique ID of the user assigned identity from the below steps Shell USER_ASSIGNED_IDENTITY_ID=$(az identity create \\ --resource-group $RESOURCE_GROUP \\ --name $USER_ASSIGNED_IDENTITY \\ --query 'id' --output tsv) 7. Creating AKS Cluster We can create AKS cluster in mostly two ways. You can chose any one of the following ways. A. Creating AKS cluster without specifying network requirements In this we can skip the network requirements during AKS creation as it is handled automatically by Azure. We are using tfy-aks-cluster as the cluster name and node pool size will autoscale from 2 to 4 nodes. You need to pass the user assigned identity through the argument --assign-identity Shell az aks create \\ --name $CLUSTER_NAME \\ --resource-group $RESOURCE_GROUP \\ --enable-workload-identity \\ --enable-managed-identity \\ --assign-identity $USER_ASSIGNED_IDENTITY_ID \\ --enable-oidc-issuer \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --kubernetes-version 1.26 \\ --location $LOCATION \\ --min-count 1 \\ --max-count 2 \\ --node-count 1 \\ --network-plugin kubenet \\ --node-vm-size Standard_D2s_v5 \\ --node-osdisk-size 100 \\ --nodepool-labels class.truefoundry.io=initial \\ --nodepool-taints CriticalAddonsOnly=true:NoSchedule \\ --enable-node-restriction \\ --generate-ssh-keys \\ --tags team=datascience owner=truefoundry Get the kubeconfig file for the AKS cluster Shell az aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME B. Creating AKS cluster with specific network requirements Exporting the address and the vnet name You can export the below command Shell export VNET_NAME=\"\" export VNET_ADDRESS_PREFIX=\"\" export SUBNET_ADDRESS_PREFIX=\"\" You can use this as an example for the default prefixes Shell export VNET_NAME=\"tfy-virtual-net\" export VNET_ADDRESS_PREFIX=\"192.168.0.0/16\" export SUBNET_ADDRESS_PREFIX=\"192.168.1.0/24\" Creating a virtual network tfy-virtual-net . Make sure to copy the unique ID of the Virtual network created. All the nodes will be part of this virtual network. Shell VNET_SUBNET_ID=$(az network vnet create \\ --resource-group $RESOURCE_GROUP \\ --name $VNET_NAME \\ --address-prefix $VNET_ADDRESS_PREFIX \\ --location $LOCATION \\ --subnet-name $VNET_NAME-subnet \\ --subnet-prefixes $SUBNET_ADDRESS_PREFIX \\ --query 'newVNet.subnets[0].id' --output tsv) Create an AKS cluster tfy-aks-cluster-with-vnet with the above network. We are using the user assigned identity we created above along with the unique ID of the virtual network. We are again setting the node pool size to autoscale from 2 to 4 nodes. Shell az aks create \\ --name $CLUSTER_NAME \\ --resource-group $RESOURCE_GROUP \\ --enable-workload-identity \\ --enable-managed-identity \\ --assign-identity $USER_ASSIGNED_IDENTITY_ID \\ --network-plugin kubenet \\ --enable-oidc-issuer \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --kubernetes-version 1.26 \\ --location $LOCATION \\ --min-count 1 \\ --max-count 2 \\ --node-count 1 \\ --node-vm-size Standard_D2s_v5 \\ --node-osdisk-size 100 \\ --nodepool-labels class.truefoundry.io=initial \\ --nodepool-taints CriticalAddonsOnly=true:NoSchedule \\ --enable-node-restriction \\ --vnet-subnet-id $VNET_SUBNET_ID \\ --service-cidr 10.0.0.0/16 \\ --dns-service-ip 10.0.0.10 \\ --pod-cidr 10.244.0.0/16 \\ --docker-bridge-address 172.17.0.1/16 \\ --generate-ssh-keys \\ --tags team=datascience owner=truefoundry Getting the kubeconfig file for the AKS cluster Shell az aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME \u2757\ufe0f az aks create commanad Error: unrecognized arguments: --enable-node-restriction, --enable-workload-identity While creation of aks if you face this error it is because of Attaching a user based spot node pool It is advised to attach a user spot node pool in AKS to schedule your workloads that can handle interruptions. There are two kinds of node pools available in Azure system and user . System is used to assign AKS related applications and workloads. User is only used to assign workloads. Moreover each of these node pools can also contains instances which are of type on-demand or spot . The first node pool we created was of type on-demand and now we will create one of type spot. Shell az aks nodepool add \\ --resource-group $RESOURCE_GROUP \\ --cluster-name $CLUSTER_NAME \\ --name spotnodepool \\ --priority Spot \\ --eviction-policy Delete \\ --spot-max-price -1 \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size Standard_D4s_v5 \\ --min-count 1 \\ --node-count 1 \\ --max-count 10 \\ --mode User \\ --node-osdisk-size 100 \\ --tags team=datascience owner=truefoundry \\ --no-wait Update autoscaling configurations Run the below command to update the default cluster-autoscaler configurations Shell az aks update \\ --resource-group $RESOURCE_GROUP \\ --name $CLUSTER_NAME \\ --cluster-autoscaler-profile expander=random scan-interval=30s max-graceful-termination-sec=180 max-node-provision-time=5m ok-total-unready-count=0 scale-down-delay-after-add=2m scale-down-delay-after-delete=30s scale-down-unneeded-time=1m scale-down-unready-time=2m scale-down-utilization-threshold=0.3 skip-nodes-with-local-storage=true skip-nodes-with-system-pods=true Attaching a user based GPU node pool (on-demand) [GPU] As we deploy machine learning models we might want to deploy the GPU node pool so that we can bring the GPU instances. You can check types of GPU instances and Availability of GPU instances in your specific region. In the below command we have assumed we will use NC6 for the node pools. Make sure to set the min, max and the required count according to the specific needs. Shell az aks nodepool add \\ --cluster-name $CLUSTER_NAME \\ --name gpupoolnc6 \\ --resource-group $RESOURCE_GROUP \\ --enable-cluster-autoscaler \\ --enable-encryption-at-host \\ --node-vm-size Standard_NC6 \\ --node-taints nvidia.com/gpu=Present:NoSchedule \\ --max-count 2 \\ --min-count 1 \\ --node-count 1 \\ --node-osdisk-size 100 \\ --mode user \\ --tags team=datascience owner=truefoundry Read Understanding Azure Node Pools for more details on how to configure your node pools Updated 5 months ago",
    "https://docs.truefoundry.com/docs/cloud-integration": "Cloud Integration Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Cloud Integration All Pages Start typing to search\u2026 Cloud Integration Add your Amazon Web Services account It is recommended that you create a new IAM role to integrate with TrueFoundry: Open your AWS IAM console, create IAM role with assume role access with the following permissions. You can also create a user and provide an AWS Access key and secret but this is not recommended. JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:ListNodegroups\", \"eks:DescribeFargateProfile\", \"eks:ListTagsForResource\", \"eks:DescribeInsight\", \"eks:ListAddons\", \"eks:DescribeAddon\", \"eks:DescribePodIdentityAssociation\", \"eks:ListInsights\", \"eks:ListPodIdentityAssociations\", \"eks:ListFargateProfiles\", \"eks:DescribeNodegroup\", \"eks:ListUpdates\", \"eks:DescribeUpdate\", \"eks:AccessKubernetesApi\", \"eks:DescribeCluster\", ], \"Resource\": [ \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:fargateprofile/CLUSTER_NAME/*/*\", \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:addon/CLUSTER_NAME/*/*\", \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:nodegroup/CLUSTER_NAME/*/*\", \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:podidentityassociation/CLUSTER_NAME/*\", \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:identityproviderconfig/CLUSTER_NAME/*/*/*\", \"arn:aws:eks:AWS_REGION:AWS_ACCOUNT_ID:cluster/CLUSTER_NAME\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"eks:DescribeAddonConfiguration\", \"eks:ListClusters\", \"eks:DescribeAddonVersions\", \"ec2:DescribeRegions\" ], \"Resource\": [ \"*\" ] } ] } Navigate to your TrueFoundry Integrations page and open the Cloud Accounts tab. Click on New Cloud Account and select AWS as the provider. Enter the IAM role arn you created into the TrueFoundry cloud account form. \ud83d\udcd8 Note: You can edit the integration at any time to update the secrets if required. If the new secrets do not have permission to manage existing clusters, you will be unable to manage those clusters from TrueFoundry. Navigate to Clusters tab and click on Configure cloud account for your cluster. Select the AWS EKS cluster name along with the added cloud account and submit. Sync your cluster by clicking on Sync button for your cloud account. This will sync cluster details and all its node pools. Add your Google Cloud Platform account We recommend you create a new service account to integrate with TrueFoundry: Open your GCP console and navigate to IAM and admin in your project and open the service accounts page Create a new service account: Add a name and description, click Create, and continue Add roles with the required permissions: the standard Google roles service account user and Kubernetes engine admin contain all the required permissions. Select the new service account and go to the keys page. Create a new key and download the keyfile.json Navigate to your TrueFoundry Integrations page and open the Cloud Accounts tab. Click on New Cloud Account and select Google Cloud Platform(GCP) as the provider. Copy and paste the contents of your keyfile.json and Google project ID . Submit to add the Cloud Account. \ud83d\udcd8 Note: You can edit the integration at any time to update the keyfile.json and Google project ID, if required. If you change the Google project while there are still TrueFoundry clusters on it, you will be unable to manage those clusters from TrueFoundry. Navigate to Clusters tab and click on Configure cloud account for your cluster. Select the GKE cluster name along with the added cloud account and submit. Sync your cluster by clicking on Sync button for your cloud account. This will sync cluster details and all its node pools. Add your Microsoft Azure account It is recommended that you create a new Azure Active Directory application to integrate with TrueFoundry: Open Azure Portal and navigate to Azure Active Directory Register a new application with Azure AD from the add menu, or from the app registrations page. Copy the directory (tenant) ID and the application (client) ID to the TrueFoundry form. Go back to your application overview and open the certificates and secrets page. Create a new client secret, and copy the secret value (not the secret ID) to TrueFoundry. Navigate to the Kubernetes service and select your AKS cluster. Open access control (IAM) and add a new role assignment to the subscription. Select the Reader role from Job function roles , and then add your Active Directory application as a member. Navigate to your TrueFoundry Integrations page and open the Cloud Accounts tab. Click on New Cloud Account and select Azure as the provider. Enter the Tenant ID , Client ID , Client secret and Subscription ID into the TrueFoundry cloud account form. \ud83d\udcd8 Note: You can edit the integration at any time to update the secrets if required. If the new secrets do not have permission to manage existing clusters, you will be unable to manage those clusters from TrueFoundry. Navigate to Clusters tab and click on Configure cloud account for your cluster. Select the Azure AKS cluster name along with the added cloud account and submit. Sync your cluster by clicking on Sync button for your cloud account. This will sync cluster details and all its node pools. Updated 5 months ago What\u2019s Next Docker Registry",
    "https://docs.truefoundry.com/docs/list-of-supported-timezones": "List of Supported Timezones Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account List of Supported Timezones All Pages Start typing to search\u2026 List of Supported Timezones Africa/Abidjan Africa/Accra Africa/Addis_Ababa Africa/Algiers Africa/Asmara Africa/Asmera Africa/Bamako Africa/Bangui Africa/Banjul Africa/Bissau Africa/Blantyre Africa/Brazzaville Africa/Bujumbura Africa/Cairo Africa/Casablanca Africa/Ceuta Africa/Conakry Africa/Dakar Africa/Dar_es_Salaam Africa/Djibouti Africa/Douala Africa/El_Aaiun Africa/Freetown Africa/Gaborone Africa/Harare Africa/Johannesburg Africa/Juba Africa/Kampala Africa/Khartoum Africa/Kigali Africa/Kinshasa Africa/Lagos Africa/Libreville Africa/Lome Africa/Luanda Africa/Lubumbashi Africa/Lusaka Africa/Malabo Africa/Maputo Africa/Maseru Africa/Mbabane Africa/Mogadishu Africa/Monrovia Africa/Nairobi Africa/Ndjamena Africa/Niamey Africa/Nouakchott Africa/Ouagadougou Africa/Porto-Novo Africa/Sao_Tome Africa/Timbuktu Africa/Tripoli Africa/Tunis Africa/Windhoek America/Adak America/Anchorage America/Anguilla America/Antigua America/Araguaina America/Argentina/Buenos_Aires America/Argentina/Catamarca America/Argentina/ComodRivadavia America/Argentina/Cordoba America/Argentina/Jujuy America/Argentina/La_Rioja America/Argentina/Mendoza America/Argentina/Rio_Gallegos America/Argentina/Salta America/Argentina/San_Juan America/Argentina/San_Luis America/Argentina/Tucuman America/Argentina/Ushuaia America/Aruba America/Asuncion America/Atikokan America/Atka America/Bahia America/Bahia_Banderas America/Barbados America/Belem America/Belize America/Blanc-Sablon America/Boa_Vista America/Bogota America/Boise America/Buenos_Aires America/Cambridge_Bay America/Campo_Grande America/Cancun America/Caracas America/Catamarca America/Cayenne America/Cayman America/Chicago America/Chihuahua America/Coral_Harbour America/Cordoba America/Costa_Rica America/Creston America/Cuiaba America/Curacao America/Danmarkshavn America/Dawson America/Dawson_Creek America/Denver America/Detroit America/Dominica America/Edmonton America/Eirunepe America/El_Salvador America/Ensenada America/Fort_Nelson America/Fort_Wayne America/Fortaleza America/Glace_Bay America/Godthab America/Goose_Bay America/Grand_Turk America/Grenada America/Guadeloupe America/Guatemala America/Guayaquil America/Guyana America/Halifax America/Havana America/Hermosillo America/Indiana/Indianapolis America/Indiana/Knox America/Indiana/Marengo America/Indiana/Petersburg America/Indiana/Tell_City America/Indiana/Vevay America/Indiana/Vincennes America/Indiana/Winamac America/Indianapolis America/Inuvik America/Iqaluit America/Jamaica America/Jujuy America/Juneau America/Kentucky/Louisville America/Kentucky/Monticello America/Knox_IN America/Kralendijk America/La_Paz America/Lima America/Los_Angeles America/Louisville America/Lower_Princes America/Maceio America/Managua America/Manaus America/Marigot America/Martinique America/Matamoros America/Mazatlan America/Mendoza America/Menominee America/Merida America/Metlakatla America/Mexico_City America/Miquelon America/Moncton America/Monterrey America/Montevideo America/Montreal America/Montserrat America/Nassau America/New_York America/Nipigon America/Nome America/Noronha America/North_Dakota/Beulah America/North_Dakota/Center America/North_Dakota/New_Salem America/Nuuk America/Ojinaga America/Panama America/Pangnirtung America/Paramaribo America/Phoenix America/Port-au-Prince America/Port_of_Spain America/Porto_Acre America/Porto_Velho America/Puerto_Rico America/Punta_Arenas America/Rainy_River America/Rankin_Inlet America/Recife America/Regina America/Resolute America/Rio_Branco America/Rosario America/Santa_Isabel America/Santarem America/Santiago America/Santo_Domingo America/Sao_Paulo America/Scoresbysund America/Shiprock America/Sitka America/St_Barthelemy America/St_Johns America/St_Kitts America/St_Lucia America/St_Thomas America/St_Vincent America/Swift_Current America/Tegucigalpa America/Thule America/Thunder_Bay America/Tijuana America/Toronto America/Tortola America/Vancouver America/Virgin America/Whitehorse America/Winnipeg America/Yakutat America/Yellowknife Antarctica/Casey Antarctica/Davis Antarctica/DumontDUrville Antarctica/Macquarie Antarctica/Mawson Antarctica/McMurdo Antarctica/Palmer Antarctica/Rothera Antarctica/South_Pole Antarctica/Syowa Antarctica/Troll Antarctica/Vostok Arctic/Longyearbyen Asia/Aden Asia/Almaty Asia/Amman Asia/Anadyr Asia/Aqtau Asia/Aqtobe Asia/Ashgabat Asia/Ashkhabad Asia/Atyrau Asia/Baghdad Asia/Bahrain Asia/Baku Asia/Bangkok Asia/Barnaul Asia/Beirut Asia/Bishkek Asia/Brunei Asia/Calcutta Asia/Chita Asia/Choibalsan Asia/Chongqing Asia/Chungking Asia/Colombo Asia/Dacca Asia/Damascus Asia/Dhaka Asia/Dili Asia/Dubai Asia/Dushanbe Asia/Famagusta Asia/Gaza Asia/Harbin Asia/Hebron Asia/Ho_Chi_Minh Asia/Hong_Kong Asia/Hovd Asia/Irkutsk Asia/Istanbul Asia/Jakarta Asia/Jayapura Asia/Jerusalem Asia/Kabul Asia/Kamchatka Asia/Karachi Asia/Kashgar Asia/Kathmandu Asia/Katmandu Asia/Khandyga Asia/Kolkata Asia/Krasnoyarsk Asia/Kuala_Lumpur Asia/Kuching Asia/Kuwait Asia/Macao Asia/Macau Asia/Magadan Asia/Makassar Asia/Manila Asia/Muscat Asia/Nicosia Asia/Novokuznetsk Asia/Novosibirsk Asia/Omsk Asia/Oral Asia/Phnom_Penh Asia/Pontianak Asia/Pyongyang Asia/Qatar Asia/Qostanay Asia/Qyzylorda Asia/Rangoon Asia/Riyadh Asia/Saigon Asia/Sakhalin Asia/Samarkand Asia/Seoul Asia/Shanghai Asia/Singapore Asia/Srednekolymsk Asia/Taipei Asia/Tashkent Asia/Tbilisi Asia/Tehran Asia/Tel_Aviv Asia/Thimbu Asia/Thimphu Asia/Tokyo Asia/Tomsk Asia/Ujung_Pandang Asia/Ulaanbaatar Asia/Ulan_Bator Asia/Urumqi Asia/Ust-Nera Asia/Vientiane Asia/Vladivostok Asia/Yakutsk Asia/Yangon Asia/Yekaterinburg Asia/Yerevan Atlantic/Azores Atlantic/Bermuda Atlantic/Canary Atlantic/Cape_Verde Atlantic/Faeroe Atlantic/Faroe Atlantic/Jan_Mayen Atlantic/Madeira Atlantic/Reykjavik Atlantic/South_Georgia Atlantic/St_Helena Atlantic/Stanley Australia/ACT Australia/Adelaide Australia/Brisbane Australia/Broken_Hill Australia/Canberra Australia/Currie Australia/Darwin Australia/Eucla Australia/Hobart Australia/LHI Australia/Lindeman Australia/Lord_Howe Australia/Melbourne Australia/NSW Australia/North Australia/Perth Australia/Queensland Australia/South Australia/Sydney Australia/Tasmania Australia/Victoria Australia/West Australia/Yancowinna Brazil/Acre Brazil/DeNoronha Brazil/East Brazil/West CET CST6CDT Canada/Atlantic Canada/Central Canada/Eastern Canada/Mountain Canada/Newfoundland Canada/Pacific Canada/Saskatchewan Canada/Yukon Chile/Continental Chile/EasterIsland Cuba EET EST EST5EDT Egypt Eire Etc/GMT Etc/GMT+0 Etc/GMT+1 Etc/GMT+10 Etc/GMT+11 Etc/GMT+12 Etc/GMT+2 Etc/GMT+3 Etc/GMT+4 Etc/GMT+5 Etc/GMT+6 Etc/GMT+7 Etc/GMT+8 Etc/GMT+9 Etc/GMT-0 Etc/GMT-1 Etc/GMT-10 Etc/GMT-11 Etc/GMT-12 Etc/GMT-13 Etc/GMT-14 Etc/GMT-2 Etc/GMT-3 Etc/GMT-4 Etc/GMT-5 Etc/GMT-6 Etc/GMT-7 Etc/GMT-8 Etc/GMT-9 Etc/GMT0 Etc/Greenwich Etc/UCT Etc/UTC Etc/Universal Etc/Zulu Europe/Amsterdam Europe/Andorra Europe/Astrakhan Europe/Athens Europe/Belfast Europe/Belgrade Europe/Berlin Europe/Bratislava Europe/Brussels Europe/Bucharest Europe/Budapest Europe/Busingen Europe/Chisinau Europe/Copenhagen Europe/Dublin Europe/Gibraltar Europe/Guernsey Europe/Helsinki Europe/Isle_of_Man Europe/Istanbul Europe/Jersey Europe/Kaliningrad Europe/Kiev Europe/Kirov Europe/Lisbon Europe/Ljubljana Europe/London Europe/Luxembourg Europe/Madrid Europe/Malta Europe/Mariehamn Europe/Minsk Europe/Monaco Europe/Moscow Europe/Nicosia Europe/Oslo Europe/Paris Europe/Podgorica Europe/Prague Europe/Riga Europe/Rome Europe/Samara Europe/San_Marino Europe/Sarajevo Europe/Saratov Europe/Simferopol Europe/Skopje Europe/Sofia Europe/Stockholm Europe/Tallinn Europe/Tirane Europe/Tiraspol Europe/Ulyanovsk Europe/Uzhgorod Europe/Vaduz Europe/Vatican Europe/Vienna Europe/Vilnius Europe/Volgograd Europe/Warsaw Europe/Zagreb Europe/Zaporozhye Europe/Zurich Factory GB GB-Eire GMT GMT+0 GMT-0 GMT0 Greenwich HST Hongkong Iceland Indian/Antananarivo Indian/Chagos Indian/Christmas Indian/Cocos Indian/Comoro Indian/Kerguelen Indian/Mahe Indian/Maldives Indian/Mauritius Indian/Mayotte Indian/Reunion Iran Israel Jamaica Japan Kwajalein Libya MET MST MST7MDT Mexico/BajaNorte Mexico/BajaSur Mexico/General NZ NZ-CHAT Navajo PRC PST8PDT Pacific/Apia Pacific/Auckland Pacific/Bougainville Pacific/Chatham Pacific/Chuuk Pacific/Easter Pacific/Efate Pacific/Enderbury Pacific/Fakaofo Pacific/Fiji Pacific/Funafuti Pacific/Galapagos Pacific/Gambier Pacific/Guadalcanal Pacific/Guam Pacific/Honolulu Pacific/Johnston Pacific/Kanton Pacific/Kiritimati Pacific/Kosrae Pacific/Kwajalein Pacific/Majuro Pacific/Marquesas Pacific/Midway Pacific/Nauru Pacific/Niue Pacific/Norfolk Pacific/Noumea Pacific/Pago_Pago Pacific/Palau Pacific/Pitcairn Pacific/Pohnpei Pacific/Ponape Pacific/Port_Moresby Pacific/Rarotonga Pacific/Saipan Pacific/Samoa Pacific/Tahiti Pacific/Tarawa Pacific/Tongatapu Pacific/Truk Pacific/Wake Pacific/Wallis Pacific/Yap Poland Portugal ROC ROK Singapore Turkey UCT US/Alaska US/Aleutian US/Arizona US/Central US/East-Indiana US/Eastern US/Hawaii US/Indiana-Starke US/Michigan US/Mountain US/Pacific US/Samoa UTC Universal W-SU WET Zulu Updated 5 months ago",
    "https://docs.truefoundry.com/docs/post-cluster-configurations": "Post cluster configurations Jump to Content Home Documentation API Reference Changelog Blogs Create Account Documentation Blogs Create Account Post cluster configurations All Pages Start typing to search\u2026 Post cluster configurations Once your cluster is configured following steps needs to be done. Creating record for your load balancer Once all the applications are created and the cluster is connected in the control plane, the endpoint where the applications are to be served must resolve to the endpoint of the load balancer's hostname. To get the load balancer's hostname run the following command shell kubectl get svc -n istio-system tfy-istio-ingress -ojsonpath='{.status.loadBalancer.ingress[0].hostname}' Once you get the hostname you can create a CNAME record with the address of your endpoint to the load balancer's address. For .e.g If you want your applications to host at *.apps.example.com , then create a CNAME record in your DNS provider resolving *.apps.example.com to the load balancer's hostname from the above command. You can keep the TTL to 1m or 60 seconds. Once it is done you can use the following command to check if the CNAME record is correct or not dig -t CNAME +short something.apps.example.com Once done you can add the Base domain URL in the Integrations tab by editing the cluster Adding base domain URL Attaching a blob storage S3 Blob Storage is a cloud-based storage service that allows you to store and retrieve vast amounts of data in the form of objects, also known as blobs. It is designed to handle a wide variety of unstructured data, including documents, images, videos, backups, logs, and more. S3 provides durability, high availability, and scalability, making it an ideal solution for companies seeking reliable and cost-effective storage for their blob data. Attaching Blob storage to Truefoundry's control plane To attach a blob storage Create a S3 bucket . Make sure the bucket has lifecycle configuration to abort multipart upload set for 7 days. Make sure CORS is applied on the bucket with the below configuration JSON [ { \"AllowedHeaders\": [ \"*\" ], \"AllowedMethods\": [ \"GET\", \"POST\", \"PUT\" ], \"AllowedOrigins\": [ \"*\" ], \"ExposeHeaders\": [ \"ETag\" ], \"MaxAgeSeconds\": 3000 } ] Create a user with programmatic access or Create IAM role with assume role access who has access to below permissions. Save the credentials somewhere safe. JSON { \"Sid\": \"S3\", \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<YOUR_S3_BUCKET_NAME>\", \"arn:aws:s3:::<YOUR_S3_BUCKET_NAME>/*\" ] } \ud83d\udcd8 Giving full access to s3 Currently there is a bug flow in our system which requires entire s3 access in your cloud. We are working on it and will be resolved soon. For now to attach a blob storage Once this is done, head over to Integrations tab and then click Connect Storage . Fill in the details Connect Storage In the storage root enter s3://<bucket name> Enter the access key and secret key which were generated while creating a user. Attaching Secrets Manager Secrets Manager is a secure and scalable service provided by Amazon Web Services (AWS) that allows you to securely store and manage sensitive information, such as database credentials, API keys, and other secrets used in your applications. It provides a centralized and highly available solution for managing secrets, eliminating the need to hardcode sensitive information in your code or configuration files. Attaching secrets manager to True Foundry's control plane Create a user with programmatic access or Create IAM role with assume role access who has access to below permissions. Save the credentials somewhere safe. The user which was created for blob storage can also be used with the below set of permission additionally attached to it. JSON { \"Sid\": \"SSM\", \"Effect\": \"Allow\", \"Action\": [ \"ssm:GetParameter\", \"ssm:GetParameters\", \"ssm:PutParameter\", \"ssm:DeleteParameter\", \"ssm:DeleteParameters\", \"ssm:GetParameterHistory\" ], \"Resource\": [ \"arn:aws:ssm:AWS_REGION:ACCOUNT_ID:parameter/tfy-secret/*\" ] } Go to Integrations tab in the control plane and click on Connect Secret Store Fill in the AWS access key and AWS secret key with the right region in the form. Connecting Secrets Store Attaching container registry AWS Elastic Container Registry (ECR) is a fully managed Docker container registry provided by Amazon Web Services (AWS). It serves as a secure and scalable storage solution for storing, managing, and deploying container images. With ECR, you can easily store and retrieve container images to facilitate the development, testing, and deployment of containerized applications. Attaching AWS ECR to True Foundry's control plane Create a user with programmatic access or Create IAM role with assume role access who has access to below permissions. Save the credentials somewhere safe. The user which was created for blob storage or Secrets store can also be used with the below set of permission additionally attached to it. JSON [ { \"Sid\": \"ECR\", \"Effect\": \"Allow\", \"Action\": [ \"ecr:GetRegistryPolicy\", \"ecr:DescribeImageScanFindings\", \"ecr:GetLifecyclePolicyPreview\", \"ecr:CreateRepository\", \"ecr:GetDownloadUrlForLayer\", \"ecr:DescribeImageReplicationStatus\", \"ecr:ListTagsForResource\", \"ecr:BatchGetRepositoryScanningConfiguration\", \"ecr:GetRegistryScanningConfiguration\", \"ecr:PutImage\", \"ecr:BatchGetImage\", \"ecr:DescribeRepositories\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetRepositoryPolicy\", \"ecr:GetLifecyclePolicy\", \"ecr:ListImages\", \"ecr:InitiateLayerUpload\", \"ecr:CompleteLayerUpload\", \"ecr:DescribeImages\", \"ecr:DeleteRepository\", \"ecr:UploadLayerPart\" ], \"Resource\": [ \"arn:aws:ecr:AWS_REGION:ACCOUNT_ID:repository/tfy-*\" ] }, { \"Sid\": \"ECR\", \"Effect\": \"Allow\", \"Action\": [ \"ecr:DescribeRegistry\", \"ecr:GetAuthorizationToken\", \"sts:GetServiceBearerToken\" ], \"Resource\": [ \"*\" ] } ] Go to Integrations tab in the control plane and go to the card of Docker registry and click on Connect Secret Store Fill in the AWS access key and AWS secret key with the right registry URL in the form. Click here to check your Registry URL . It will be of form aws_account_id.dkr.ecr.region.amazonaws.com Connecting Registry Create an IAM role with assume role Using IAM access keys creates a security loophole and an overhead of rotating the keys. To avoid this TrueFoundry support IAM role with access to assume role so that access keys can be avoided. This IAM role generates temporary access keys which expire after certain time resulting in a secure key-less architecture. You can create an IAM role for TrueFoundry with the above permissions for blob storage , ECR and SSM The role should have following Trust relationships added (if you are using TrueFoundry's control plane) JSON { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::416964291864:role/tfy-ctl-euwe1-production-truefoundry-deps\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": {} } ] } Updated 5 months ago"
}